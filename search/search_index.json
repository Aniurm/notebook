{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"course_notes/cmu15445/buffer-pool/","title":"BUFFER POOL","text":""},{"location":"course_notes/cmu15445/buffer-pool/#overview","title":"Overview","text":"<p>The buffer pool is responsible for moving physical pages back and forth from main memory to disk. It allows a DBMS to support databases that are larger than the amount of memory available to the system. The buffer pool's operations are transparent to other parts in the system. \uff08\u8fd9\u91cc\u7684transparent\u5e94\u8be5\u7406\u89e3\u4e3a\u201c\u672a\u5bdf\u89c9\u7684\uff0c\u672a\u8ba4\u8bc6\u5230\u7684\u201d\uff09</p> <p>Your implementation will need to be thread-safe.</p>"},{"location":"course_notes/cmu15445/buffer-pool/#task-1-lru-k-replacement-policy","title":"Task #1 - LRU-K Replacement Policy","text":"<p>\u501f\u52a9GPT\uff0c\u6211\u624d\u52c9\u5f3a\u7406\u89e3LRU-K\u3002LRU-K\u7b97\u6cd5\u662f\u4e00\u79cd\u9875\u9762\u7f6e\u6362\u7b97\u6cd5\uff0c\u7528\u4e8e\u8ba1\u7b97\u673a\u7f13\u5b58\u4e2d\u51b3\u5b9a\u54ea\u4e2a\u9875\u9762\u5e94\u8be5\u88ab\u66ff\u6362\u3002\u5728\u8fd9\u4e2a\u7b97\u6cd5\u4e2d\uff0c\"K\"\u4ee3\u8868\u4e00\u4e2a\u6570\u5b57\uff0c\u610f\u5473\u7740\u7b97\u6cd5\u4f1a\u8003\u8651\u5230\u9875\u9762\u88ab\u8bbf\u95ee\u7684\u6700\u540eK\u6b21\u8bb0\u5f55\u3002Backward k-distance\u662f\u4ece\u73b0\u5728\u5f00\u59cb\u5f80\u524d\u6570\uff0c\u7b2ck\u6b21\u8bbf\u95ee\u6b64\u9875\u9762\u70b9\u65f6\u95f4\u4e0e\u73b0\u5728\u7684\u65f6\u95f4\u5dee\u3002</p> <p>\u5f53\u4e00\u4e2a\u9875\u9762\u7684\u8bbf\u95ee\u6b21\u6570\u4e0d\u8db3K\u6b21\u65f6\uff0c\u5b83\u7684\u5411\u540ek\u8ddd\u79bb\u5c31\u88ab\u8ba4\u4e3a\u662f\u65e0\u9650\u5927\uff08+inf\uff09\u3002\u8fd9\u6837\u505a\u662f\u56e0\u4e3a\u7b97\u6cd5\u81f3\u5c11\u9700\u8981K\u6b21\u8bbf\u95ee\u8bb0\u5f55\u6765\u8ba1\u7b97\u4e00\u4e2a\u9875\u9762\u7684\u5411\u540ek\u8ddd\u79bb\uff0c\u4e0d\u8db3K\u6b21\u5c31\u610f\u5473\u7740\u6ca1\u6709\u8db3\u591f\u7684\u4fe1\u606f\u6765\u51b3\u5b9a\u5b83\u7684\u91cd\u8981\u7a0b\u5ea6\u3002</p> <p>\u5f53\u7b97\u6cd5\u9700\u8981\u66ff\u6362\u4e00\u4e2a\u9875\u9762\u65f6\uff0c\u5b83\u4f1a\u9009\u62e9\u5411\u540ek\u8ddd\u79bb\u6700\u5927\u7684\u9875\u9762\u3002\u5982\u679c\u6709\u591a\u4e2a\u9875\u9762\u7684\u5411\u540ek\u8ddd\u79bb\u90fd\u662f\u65e0\u9650\u5927\uff08\u4e5f\u5c31\u662f\u8bf4\uff0c\u8fd9\u4e9b\u9875\u9762\u7684\u8bbf\u95ee\u6b21\u6570\u90fd\u4e0d\u8db3K\u6b21\uff09\uff0c\u90a3\u4e48\u7b97\u6cd5\u4f1a\u9009\u62e9\u6700\u65e9\u88ab\u8bbf\u95ee\u7684\u9875\u9762\uff08\u201c\u6700\u65e9\u88ab\u8bbf\u95ee\u201d\u6307\u7b2c\u4e00\u6b21\u88ab\u8bbf\u95ee\u7684\u65f6\u95f4\u6700\u65e9\uff09\u3002</p> <p>LRU-K Replacement Policy</p> <p>You will need to implement the following methods:</p> <ul> <li><code>Evict(frame_id_t* frame_id)</code> : Evict the frame with largest backward k-distance compared to all other evictable frames being tracked by the Replacer. </li> <li><code>RecordAccess(frame_id_t frame_id)</code> : Record that given frame id is accessed at current timestamp. </li> <li><code>Remove(frame_id_t frame_id)</code> : Clear all access history associated with a frame. </li> <li><code>SetEvictable(frame_id_t frame_id, bool set_evictable)</code> : This method controls whether a frame is evictable or not. It also controls LRUKReplacer's size. </li> <li><code>Size()</code> : This method returns the number of evictable frames that are currently in the LRUKReplacer.</li> </ul> <p>\u6211\u4f7f\u7528\u7684\u6570\u636e\u7ed3\u6784\uff1a\u4e24\u4e2a<code>std::list&lt;frame_id_t&gt;</code>\uff0c\u5206\u522b\u7528\u4e8e\u4fdd\u5b58\u8bbf\u95ee\u6b21\u6570\u5c0f\u4e8e\u3001\u5927\u4e8eK\u7684frame_id\u3002<code>std::unordered_map&lt;frame_id_t, LRUKNode</code>\uff0c\u7528\u4e8e\u4fdd\u5b58frame_id\u5bf9\u5e94\u7684LRUKNode\uff0c\u5176\u4e2dLRUKNode\u5305\u542b\u4e86<code>frame_id</code>\u5bf9\u5e94\u7684<code>history</code>, <code>is_evictable</code>, <code>k</code>. \u4ece\u7b80\u5355\u5230\u590d\u6742\uff0c\u5b9e\u73b0\u51e0\u4e2aMethods\uff1a</p> <ul> <li><code>Size()</code>\uff1a\u8fd4\u56de<code>curr_size_</code></li> <li><code>SetEvictable</code>\uff1a\u4fee\u6539<code>LRUKNode</code>\u7684\u6570\u636e\uff0c\u7ef4\u62a4\u4e24\u4e2a<code>std::list&lt;frame_id_t&gt;</code>\uff0c\u4fee\u6539<code>curr_size_</code>\u7b49\u4fe1\u606f\u3002</li> <li><code>Remove</code>\uff1a\u6211\u76f4\u63a5\u628a<code>LRUKNode</code>\u5220\u6389\u4e86\uff0c\u7ef4\u62a4<code>std::list&lt;frame_id_t&gt;</code>\u7b49\u6570\u636e\u3002\u5f53Buffer Pool Manager\u8c03\u7528<code>DeletePage</code>\u65f6\uff0c\u4f1a\u8c03\u7528<code>Remove</code>\uff0c\u8fd9\u8868\u660eBuffer Pool\u4e0d\u518d\u5173\u5fc3\u8fd9\u4e2aframe\uff0c\u6240\u4ee5\u53ef\u4ee5\u76f4\u63a5\u5220\u6389\u3002</li> <li><code>RecordAccess</code>\uff1a\u4fee\u6539<code>LRUKNode</code>\u4e2d\u7684history\uff0c\u7ef4\u62a4\u4e24\u4e2a<code>std::list&lt;frame_id_t&gt;</code>\u3002</li> <li><code>Evict</code>\uff1a\u5148\u5220\u53bb\u8bbf\u95ee\u6b21\u6570\u5c0f\u4e8eK\u7684list\u7684\u7b2c\u4e00\u4e2a\u5143\u7d20\uff08FIFO\uff09\uff0c\u5982\u679c\u8bbf\u95ee\u6b21\u6570\u90fd\u5927\u4e8eK\uff0c\u90a3\u4e48\u904d\u5386\u8bbf\u95ee\u6b21\u6570\u5927\u4e8eK\u7684list\uff0c\u627e\u5230backward k-distance\u6700\u5927\u7684frame_id\uff0c\u7136\u540e\u5220\u9664\u8fd9\u4e2aframe_id\u3002\u65f6\u95f4\u590d\u6742\u5ea6\\(O(n)\\)\u3002</li> </ul> <p>\u597d\u51e0\u4e2aMethod\u90fd\u9700\u8981\u6839\u636e<code>LRUKNode</code>\u7684\u6570\u636e\uff0c\u8c03\u6574\u4e24\u4e2a<code>std::list&lt;frame_id_t&gt;</code>\uff0c\u6240\u4ee5\u53ef\u4ee5\u628a\u8fd9\u4e2a\u903b\u8f91\u5c01\u88c5\u4e00\u4e0b\u3002</p> <p>\u8bef\u533a</p> <p>\u770b\u4e86\u4e0b\u7f51\u4e0a\u7684\u4e00\u4e9b\u5b9e\u73b0\uff0c\u53d1\u73b0\u6709\u4e9b\u4eba\u4e5f\u7ef4\u62a4\u4e86\u4e24\u4e2a<code>std::list&lt;frame_id_t&gt;</code>\uff0c\u4f46\u662f\u4ed6\u4eec\u5bf9\u8bbf\u95ee\u6b21\u6570\u8d85\u8fc7K\u7684list\u4f7f\u7528\u4e86LRU\u7b97\u6cd5\u3002\u6211\u8ba4\u4e3a\u8fd9\u662f\u4e0d\u80fd\u7b49\u540c\u4e8eLRU-K\u7684\u3002\u4e3e\u4e00\u4e2a\u4f8b\u5b50\uff0c\u5047\u8bbeK=2\u3002\u73b0\u5728\u5bf9\u4e8eframe 1, \u8bbf\u95ee\u65f6\u95f4\u662f1, 9. \u5bf9\u4e8eframe 2, \u8bbf\u95ee\u65f6\u95f4\u662f4, 5. \u73b0\u5728\u7684\u65f6\u95f4\u4e3a10, \u6211\u9700\u8981Evict\u4e00\u4e2aframe\u3002\u5982\u679c\u6309\u7167LRU\u7b97\u6cd5\uff0c\u90a3\u4e48frame 2\u4f1a\u88ab\u66ff\u6362\uff0c\u4f46\u662f\u6309\u7167LRU-K\u7b97\u6cd5\uff0cframe 1\u4f1a\u88ab\u66ff\u6362\uff08\u56e0\u4e3aframe 1\u7684backward k-distance\u662f10-1=9\uff0cframe 2\u7684backward k-distance\u662f10-4=6\uff09\u6240\u4ee5\u8fd9\u4e24\u4e2a\u7b97\u6cd5\u662f\u4e0d\u540c\u7684\u3002\u6211\u8fd9\u8fb9\u8001\u8001\u5b9e\u5b9e\u5730\\(O(N)\\)\u904d\u5386\uff08\u66fe\u7ecf\u60f3\u7528\u5806\uff0c\u4f46\u662f\u7ef4\u62a4\u5806\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u4e5f\u5dee\u4e0d\u591a\uff0c\u5b9e\u73b0\u8d77\u6765\u8fd8\u9ebb\u70e6\uff09</p>"},{"location":"course_notes/cmu15445/buffer-pool/#task-2-disk-scheduler","title":"Task #2 - Disk Scheduler","text":"<p>The disk scheduler can be used by other components to queue disk requests, represented by a <code>DiskRequest</code> struct. The disk scheduler will maintain a background worker thread which is responsible for processing scheduled requests.</p> <p>The disk scheduler will utilize a shared queue to schedule and process the <code>DiskRequest</code>s. One thread will add  requests to the queue, and the disk scheduler's background worker will process requests.</p>"},{"location":"course_notes/cmu15445/buffer-pool/#disk-manager","title":"Disk Manager","text":"<p>The Disk Manager class reads and writes the page data from and to the disk. Your disk scheduler will  use <code>DiskManager::ReadPage()</code> and <code>DiskManager::WritePage()</code> when it is processing a read or write request.</p> <p>Task</p> <ul> <li><code>Schedule(DiskRequest r)</code> : Schedules a request for the <code>DiskManager</code> to execute. The DiskRequest struct specifies whether the request is for a read/write, where the data should be written into/from, and the page ID for the operation. The <code>DiskRequest</code> also includes a <code>std::promise</code> whose value should be set to true once the request is processed.</li> <li><code>StartWorkerThread()</code> : Start method for the background worker thread which processes the scheduled requests. The worker thread is created in the DiskScheduler constructor and calls this method. This method is responsible for getting queued requests and dispatching them to the <code>DiskManager</code>. Remember to set the value on the <code>DiskRequest</code>'s callback to signal to the request issuer that the request has been completed. This should not return until the <code>DiskScheduler</code>'s destructor is called.</li> </ul> <p>\u4e4d\u4e00\u770b\u5f88\u9ad8\u7ea7\uff0c\u5b9e\u9645\u4e0a\u5f88\u7b80\u5355\u3002<code>Schedule</code>\u65b9\u6cd5\u5c31\u662f\u628a<code>DiskRequest</code>\u653e\u5230<code>std::queue&lt;DiskRequest&gt;</code>\u4e2d\uff0c<code>StartWorkerThread</code>\u8fd0\u884c\u5728 background worker thread\u4e2d\uff0c\u6b7b\u5faa\u73af\u4ece<code>std::queue&lt;DiskRequest&gt;</code>\u4e2d\u53d6\u51fa<code>DiskRequest</code>\uff0c\u7136\u540e\u8c03\u7528<code>DiskManager::ReadPage()</code>\u548c<code>DiskManager::WritePage()</code>\uff0c\u6700\u540e\u8bbe\u7f6e<code>DiskRequest</code>\u7684<code>std::promise</code>\u7684\u503c\u3002\u76f4\u5230\u53d6\u51fa<code>std::nullopt</code>\u624d\u9000\u51fa\u3002</p>"},{"location":"course_notes/cmu15445/buffer-pool/#task-3-buffer-pool-manager","title":"Task #3 - Buffer Pool Manager","text":"<p>The <code>BufferPoolManager</code> is responsible for fetching database pages from disk with the <code>DiskScheduler</code> and storing them in memory. The <code>BufferPoolManager</code> can also schedule writes of dirty pages out to disk when it is either explicitly instructed to do so or when it needs to evict a page to make space for a new page.</p> <p>All in-memory pages in the system are represented by <code>Page</code> objects. It is important to understand that <code>Page</code> objects are just containers for memory in the buffer pool and thus are not specific to a unique page. The <code>Page</code> object's identifier (<code>page_id</code>) keeps track of what physical page it contains. Each <code>Page</code> object also maintains a counter for the number of threads that have \"pinned\" that page.</p> <p>Task</p> <ul> <li><code>FetchPage(page_id_t page_id)</code></li> <li><code>UnpinPage(page_id_t page_id, bool is_dirty)</code>, the <code>is_dirty</code> parameter keeps track of whether a page was modified while it was pinned.</li> <li><code>FlushPage(page_id_t page_id)</code>: flush a page regardless of its pin status.</li> <li><code>NewPage(page_id_t* page_id)</code>: should call <code>AllocatePage</code></li> <li><code>DeletePage(page_id_t page_id)</code>: should call <code>DeallocatePage</code></li> </ul> <p>\u4ece<code>BufferPoolManager</code>\u7684\u4ee3\u7801\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u51fa\u4e00\u4e9b\u7aef\u502a\uff1a buffer_pool_manager.h<pre><code> private:\n  /** Number of pages in the buffer pool. */\n  const size_t pool_size_;\n  /** The next page id to be allocated  */\n  std::atomic&lt;page_id_t&gt; next_page_id_ = 0;\n\n  /** Array of buffer pool pages. */\n  Page *pages_;\n  /** Pointer to the disk scheduler. */\n  std::unique_ptr&lt;DiskScheduler&gt; disk_scheduler_ __attribute__((__unused__));\n  /** Pointer to the log manager. Please ignore this for P1. */\n  LogManager *log_manager_ __attribute__((__unused__));\n  /** Page table for keeping track of buffer pool pages. */\n  std::unordered_map&lt;page_id_t, frame_id_t&gt; page_table_;\n  /** Replacer to find unpinned pages for replacement. */\n  std::unique_ptr&lt;LRUKReplacer&gt; replacer_;\n  /** List of free frames that don't have any pages on them. */\n  std::list&lt;frame_id_t&gt; free_list_;\n  /** This latch protects shared data structures. We recommend updating this comment to describe what it protects. */\n  std::mutex latch_;\n</code></pre></p> <p>Buffer Pool\u7684\u672c\u8d28\u5c31\u662f<code>Page *pages_</code>, \u4e00\u4e2a<code>Page</code>\u6570\u7ec4\u3002\u5728\u8fd9\u4e2a\u6570\u7ec4\u91cc\u9762\u7684\u6bcf\u4e00\u4e2a<code>Page</code> object \u90fd\u662f\u4e00\u4e2acontainer\uff0c\u7528\u4e8e\u5b58\u50a8\u4ece disk\u8bfb\u53d6\u7684page\u3002<code>frame_id_t</code>\u5b9e\u9645\u4e0a\u53ef\u4ee5\u76f4\u63a5\u7406\u89e3\u4e3a\u6570\u7ec4\u7684\u4e0b\u6807\u3002</p> <p>\u8fd9\u4ece<code>BufferPoolManager</code>\u7684constructor\u4e2d\u53ef\u4ee5\u770b\u51fa\u6765\uff1a \u7528\u5faa\u73af\u628a<code>pages_</code>\u7684\u4e0b\u6807\u653e\u5230<code>free_list_</code>\u4e2d\u3002 buffer_pool_manager.cpp<pre><code>BufferPoolManager::BufferPoolManager(size_t pool_size, DiskManager *disk_manager, size_t replacer_k,\n                                     LogManager *log_manager)\n    : pool_size_(pool_size), disk_scheduler_(std::make_unique&lt;DiskScheduler&gt;(disk_manager)), log_manager_(log_manager) {\n  // we allocate a consecutive memory space for the buffer pool\n  pages_ = new Page[pool_size_];\n  replacer_ = std::make_unique&lt;LRUKReplacer&gt;(pool_size, replacer_k);\n\n  // Initially, every page is in the free list.\n  for (size_t i = 0; i &lt; pool_size_; ++i) {\n    free_list_.emplace_back(static_cast&lt;int&gt;(i));\n  }\n}\n</code></pre></p> <p>\u63a5\u4e0b\u6765\u5c31\u53ef\u4ee5\u5b8f\u89c2\u5730\u7406\u89e3Buffer Pool Manager\u3002\u9996\u5148\uff0c\u5b83\u4e3a\u4ec0\u4e48\u8981\u5b58\u5728\uff1f\u5728\u6211\u770b\u6765\uff0c Buffer Pool\u628aDisk\u4e2d\u7684page\u8bfb\u53d6\u5230\u5185\u5b58\u4e2d\uff0c\u8fd9\u6837\u5c31\u53ef\u4ee5\u52a0\u5feb\u8bbf\u95ee\u901f\u5ea6\uff0c\u51cf\u5c11Disk IO\u3002\u5b83\u62bd\u8c61\u4e86\u5185\u5b58\u548cDisk\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u8ba9\u5176\u4ed6\u6a21\u5757\u4e0d\u9700\u8981\u5173\u5fc3\u5185\u5b58\u548cDisk\u7684\u4ea4\u4e92\u7ec6\u8282\u3002</p> <p>\u5982\u4f55\u5b9e\u73b0Buffer Pool Manager\u5462\uff1f\u524d\u9762\u5df2\u7ecf\u5b8c\u6210\u4e86\u57fa\u672c\u7684\u7ec4\u4ef6\uff0c\u73b0\u5728\u53ea\u9700\u8981\u628a\u5b83\u4eec\u7ec4\u5408\u8d77\u6765\u5c31\u53ef\u4ee5\u4e86\u3002\u5f53Buffer Pool Manager \u9700\u8981\u4e0eDisk\u4ea4\u4e92\u65f6\uff0c\u5b83\u4f1a\u628a<code>DiskRequest</code>\u53d1\u9001\u7ed9<code>DiskScheduler</code>\uff0c\u7b49<code>DiskScheduler</code>\u5904\u7406\u5b8c\u6bd5\u540e\uff0c<code>DiskScheduler</code>\u901a\u77e5 <code>BufferPoolManager</code>\u3002 Buffer Pool Manager\u8c03\u7528<code>LRUKReplacer</code>\u7684\u63a5\u53e3\uff0c\u8bb0\u5f55page\u7684\u8bbf\u95ee\u60c5\u51b5\uff0c\u6839\u636e\u4e0d\u540c\u60c5\u51b5\u914d\u7f6e<code>SetEvictable</code>\u3002 \u5f53\u5b83\u9700\u8981\u66ff\u6362\u4e00\u4e2apage\u65f6\uff0c\u5b83\u4f1a\u8c03\u7528<code>LRUKReplacer</code>\u7684<code>Evict</code>\u65b9\u6cd5\uff0c<code>LRUKReplacer</code>\u4f1a\u8fd4\u56de\u4e00\u4e2a<code>frame_id_t</code>\uff0c\u7136\u540eBuffer Pool Manager\u5c31\u53ef\u4ee5\u628a\u8fd9\u4e2a<code>frame_id_t</code>\u5bf9\u5e94\u7684<code>Page</code>\u66ff\u6362\u6389\u3002</p> <p>\u5b8f\u89c2\u4e0a\u7406\u89e3\u4e86Buffer Pool Manager\uff0c\u63a5\u4e0b\u6765\u7684\u7ec6\u8282\u5c31\u4e0d\u5fc5\u591a\u8bf4\u4e86\uff0cGPT\u548cCopilot\u5728\u5b9e\u73b0\u3001\u6d4b\u8bd5\u7684\u8fc7\u7a0b\u4e2d\u53ef\u4ee5\u8d77\u5230\u5f88\u5927\u7684\u5e2e\u52a9\u3002</p> <p>\u6700\u559c\u6b22\u7684\u4e00\u96c6\uff1a</p> <pre><code>Autograder Score\n100.0 / 100.0\nPassed Tests\nBuild.Prepare (0/0)\nBuild (0/0)\nBuild.ClangFormat (0/0)\nBuild.ClangTidy (0/0)\nBuild.RelWithDebInfo (0/0)\nBufferPoolManagerTest.SchedulerCheck (0/0)\nLRUKReplacerTest.SampleTest (6/6)\nLRUKReplacerTest.Evict (6/6)\nLRUKReplacerTest.Size (6/6)\nLRUKReplacerTest.ConcurrencyTest (7/7)\nDiskSchedulerTest.ScheduleWriteReadPageTest (7/7)\nDiskSchedulerTest.ScheduleManyWrites (9/9)\nBufferPoolManagerTest.SampleTest (2/2)\nBufferPoolManagerTest.BinaryDataTest (2/2)\nBufferPoolManagerTest.NewPage (2/2)\nBufferPoolManagerTest.UnpinPage (2/2)\nBufferPoolManagerTest.FetchPage (2/2)\nBufferPoolManagerTest.DeletePage (2/2)\nBufferPoolManagerTest.IsDirty (2/2)\nBufferPoolManagerTest.ConcurrencyTest (3/3)\nBufferPoolManagerTest.IntegratedTest (3/3)\nBufferPoolManagerTest.HardTest_1 (6/6)\nBufferPoolManagerTest.HardTest_2 (6/6)\nBufferPoolManagerTest.HardTest_3 (6/6)\nBufferPoolManagerTest.HardTest_4 (6/6)\nLeaderboard.QPS.1 (5/5)\nLeaderboard.QPS.2 (5/5)\nLeaderboard.QPS.3 (5/5)\nLeaderboard.Summary (0/0)\n</code></pre>"},{"location":"course_notes/cmu15445/hash-index/","title":"EXTENDIBLE HASH INDEX","text":""},{"location":"course_notes/cmu15445/hash-index/#overview","title":"Overview","text":"<p>In this programming project you will implement disk-backed hash index in your database system. You will be using a variant of extendible hashing as the hashing scheme. We added a non-resizable header page on top of the directory pages so that the hash table can hold more values and potentially achieve better multi-thread performance.</p> <p></p> <p>Your implementation should support thread-safe search, insertion, and deletion (including growing/shrinking the directory and splitting/merging buckets).</p>"},{"location":"course_notes/cmu15445/hash-index/#task-1-readwrite-page-guards","title":"Task #1 - Read/Write Page Guards","text":"<p>In the Buffer Pool Manager, <code>FetchPage</code> and <code>NewPage</code> functions return pointers to pages that are already pinned. To indicate that the page is no longer needed in memory, the programmer has to manually call <code>UnpinPage</code>. If the programmer forgets to call <code>UnpinPage</code>, the page will never be evicted out of the buffer pool. Not only the performance takes a hit, the bug is also difficult to be detected.</p> <p>Task</p> <ul> <li>You will  implement <code>BasicPageGuard</code> which store the pointers to <code>BufferPoolManager</code> and <code>Page</code> objects. A page guard ensures that <code>UnpinPage</code> is called on the corresponding <code>Page</code> object as soon as it goes out of scope.</li> <li>You will also implement <code>ReadPageGuard</code> and <code>WritePageGuard</code> which automatically unlatch the pages as soon as they go out of scope.</li> </ul>"},{"location":"course_notes/cmu15445/hash-index/#task-2-extendible-hash-table-pages","title":"Task #2 - Extendible Hash Table Pages","text":""},{"location":"course_notes/cmu15445/hash-index/#hash-table-header-page","title":"Hash Table Header Page","text":"<p>The header page sits the at the first level of our disk-based extendible hash table,  and there is only one header page for a hash table. It **stores the logical child pointers  to the directory pages. ** The header page has the following fields:</p> Variable Name Description <code>directory_page_ids_</code> An array of directory page ids <code>max_depth_</code> The maximum depth the header page could handle"},{"location":"course_notes/cmu15445/hash-index/#hash-table-directory-page","title":"Hash Table Directory Page","text":"<p>Directory pages sit at the second level of our disk-based extendible hash table.  Each of them stores the logical child pointers to the bucket pages,  as well as metadata for handling bucket mapping and dynamic directory growing and  shrinking. The directory page has the following fields:</p> Variable Name Description <code>max_depth_</code> The maximum depth the header page could handle <code>global_depth_</code> The current directory global depth <code>local_depths_</code> An array of bucket page local depths <code>bucket_page_ids_</code> An array of bucket page ids"},{"location":"course_notes/cmu15445/hash-index/#hash-table-bucket-page","title":"Hash Table Bucket Page","text":"<p>Bucket pages sit at the third level of our disk-based extendible hash table.  They are the ones that are actually **storing the key-value pairs. ** The bucket page has the following fields:</p> Variable Name Description <code>size</code> The number of key-value pairs the bucket is holding <code>max_size</code> The maximum number of key-value pairs the bucket can handle <code>array_</code> An array of bucket page local depths"},{"location":"course_notes/cmu15445/hash-index/#task-3-extendible-hashing-implementation","title":"Task #3 - Extendible Hashing Implementation","text":"<p>Your implementation needs to support insertions, point search and deletions.  Your only strict API requirement is adhering to <code>Insert</code>, <code>GetValue</code>, and <code>Remove</code>.</p> <p>Note</p> <p>You should use the page classes you implemented in Task #2 to store the key-value pairs as well as the metadata to maintain the hash table (page ids, global/local depths).</p> <p>The extendible hash table is parameterized on arbitrary key, value, and key comparator types.</p> <p>This project requires you to implement:</p> <ul> <li>Bucket splitting and merging</li> <li>Directory growing and shrinking</li> </ul>"},{"location":"course_notes/cmu15445/primer/","title":"C++ PRIMER","text":""},{"location":"course_notes/cmu15445/primer/#project-specification","title":"Project Specification","text":"<p>In this project, you will implement a key-value store backed by a copy-on-write trie.</p> <p>The key-value store you will implement can store string keys mapped to values of any type. The value of a key is stored in the node representing the last character of that key (aka terminal node).</p> <p></p>"},{"location":"course_notes/cmu15445/primer/#task-1-copy-on-write-trie","title":"Task #1 - Copy-On-Write Trie","text":"<p>Copy-On-Write Trie</p> <p>Modify <code>trie.h</code> and <code>trie.cpp</code> to implement a copy-on-write trie. Your trie must support three operations:</p> <ul> <li><code>Get(key)</code>: Get the value corresponding to the key.</li> <li><code>Put(key, value)</code>: Set the corresponding value to the key. Overwrite existing value if the key already exists. Note that the type of the value might be non-copyable (i.e., <code>std::unique_ptr&lt;int&gt;</code>). This method returns a new trie.</li> <li><code>Delete(key)</code>: Delete the value for the key. This method returns a new trie.</li> </ul> <p>\u5bf9\u4e8e\u4e09\u4e2a\u64cd\u4f5c\u6765\u8bf4\uff0c\u6574\u4f53\u601d\u8def\u90fd\u662f\u6839\u636e<code>key</code>\u4e00\u6b65\u6b65\u5f80\u4e0b\u8d70\uff0c\u8fdb\u884c\u76f8\u5e94\u7684\u64cd\u4f5c\u3002<code>Get</code>\u6700\u7b80\u5355\uff0c\u641c\u4e0d\u5230\u67d0\u4e2a\u5b57\u7b26\u7684\u8bdd\u8fd4\u56de<code>nullptr</code>\u5c31\u884c\u4e86\uff0c<code>Put</code>\u641c\u4e0d\u5230\u5b57\u7b26\u65f6\u5e94\u8be5\u65b0\u5efa\u4e00\u4e2a\u8282\u70b9\u3002<code>Delete</code>\u9700\u8981\u9012\u5f52\uff0c\u4eceTrie\u7684\u5e95\u90e8\u4e00\u76f4\u5f80\u4e0a\u5220\u3002</p> <p>Copy-On-Write\u6700\u91cd\u8981\u7684\u4e00\u70b9\u662f\u4e0d\u80fd\u4fee\u6539\u539f\u6765\u7684Trie\uff0c\u5982\u679c\u8981\u505a\u4fee\u6539\uff0c\u5c31\u8981\u590d\u5236\u4e00\u4efd\u65b0\u7684Trie\uff0c\u7136\u540e\u4fee\u6539\u65b0\u7684Trie\u3002</p>"},{"location":"course_notes/cmu15445/primer/#task-2-concurrent-key-value-store","title":"Task #2 - Concurrent Key-Value Store","text":"<p>Concurrent Key-Value Store</p> <ul> <li>Modify <code>trie_store.h</code> and <code>trie_store.cpp</code> to implement a concurrent key-value store for a multithreaded environment.</li> <li>Also, if we get a reference to a value from the trie, we should be able to access it no matter how we modify the trie.</li> </ul> <p>\u672c\u6765\u6211\u4ee5\u4e3a\u8981\u91cd\u65b0\u5b9e\u73b0<code>Get</code>, <code>Put</code>, <code>Delete</code>\uff0c\u4f46\u662f\u6211\u53d1\u73b0\u8c03\u7528Task #1\u4e2d\u5df2\u7ecf\u5b9e\u73b0\u7684\u5c31\u597d\u4e86\u3002 <code>Trie_Store</code>\u8fd9\u4e2aclass\u4f3c\u4e4e\u53ea\u662f\u7ed9\u4e4b\u524d\u7684<code>Trie</code>\u52a0\u4e86\u4e00\u4e9b<code>mutex</code>\u6765\u4fdd\u8bc1\u7ebf\u7a0b\u5b89\u5168\u3002</p>"},{"location":"course_notes/cmu15445/primer/#task-3-debugging","title":"Task #3 - Debugging","text":"<p>\u5173\u4e8eTrie\u7684\u5c0f\u95ee\u9898\uff0c\u5f88\u7b80\u5355\u3002</p>"},{"location":"course_notes/cmu15445/primer/#task-4-sql-string-functions","title":"Task #4 - SQL String Functions","text":"<p>SQL String Functions</p> <p>Implement upper and lower SQL functions. This can be done in 2 steps:</p> <ol> <li>implement the function logic in <code>string_expression.h</code>.</li> <li>register the function in BusTub, so that the SQL framework can call your function when the user executes a SQL, in <code>plan_func_call.cpp</code>.</li> </ol> <p>Well\uff0c\u8fd8\u662f\u5f88\u7b80\u5355\u3002\u95ee\u9898\u662f\uff0c\u73b0\u5728\u505a\u7684\u662f\u5c40\u90e8\u7684\u66f4\u6539\uff0c\u800c\u6211\u4e0d\u77e5\u9053<code>plan_func_call.cpp</code>\u662f\u600e\u4e48\u88ab\u8c03\u7528\u7684\uff0c\u4e3a\u4ec0\u4e48\u7528\u5b83\u6765\u6ce8\u518c\u51fd\u6570\u3002\u3002\u3002\u3002\u3002\u7b97\u4e86\u5148\u7ee7\u7eed\u5f80\u4e0b\u5b66\u5427\u3002</p> <p>\u8fd0\u884cAutograder :) <pre><code>Autograder Score\n100.0 / 100.0\nPassed Tests\nBuild.Prepare (0/0)\nBuild.Configure (0/0)\nBuild.ClangFormat (0/0)\nBuild.ClangTidy (0/0)\nBuild (0/0)\nBuild.TSAN (0/0)\nBuild.Release (0/0)\nTrieTest.ConstructorTest (1/1)\nTrieTest.BasicPutTest (2/2)\nTrieTest.BasicPutGetTest (2/2)\nTrieTest.PutGetOnePath (2/2)\nTrieTest.BasicRemoveTest1 (2/2)\nTrieTest.BasicRemoveTest2 (2/2)\nTrieTest.MismatchTypeTest (2/2)\nTrieTest.CopyOnWriteTest1 (3/3)\nTrieTest.CopyOnWriteTest2 (3/3)\nTrieTest.CopyOnWriteTest3 (3/3)\nTrieTest.PointerStability (3/3)\nTrieDebugger.TestCase (5/5)\nTrieTest.NonCopyableTest (3/3)\nTrieTest.TrieStructureCheck (2/2)\nTrieTest.RemoveFreeTest (3/3)\nTrieTest.MixedTest (8/8)\nTrieStoreTest.BasicTest (3/3)\nTrieStoreTest.GuardTest (3/3)\nTrieStoreTest.ReadWriteTest (8/8)\nTrieStoreTest.NonCopyableTest (8/8)\nTrieStoreTest.MixedTest (7/7)\nTrieStoreTest.MixedConcurrentTest (5/5)\nTrieTest.MixedTest.Rel (3/3)\nTrieStoreTest.MixedConcurrentTest.Rel (3/3)\nSQLLogicTest.p0.01-lower-upper (3/3)\nSQLLogicTest.p0.02-function-error (3/3)\nSQLLogicTest.p0.03-string-scan (5/5)\nTrieStoreTest.MixedConcurrentTest.TSAN (3/3)\n</code></pre></p>"},{"location":"course_notes/cs144/lab0/","title":"Lab 0: networking warmup","text":""},{"location":"course_notes/cs144/lab0/#networking-by-hand","title":"Networking by hand","text":""},{"location":"course_notes/cs144/lab0/#fetch-a-web-page-send-yourself-an-email","title":"Fetch a Web page &amp; Send yourself an email","text":"<p>\u4f7f\u7528 <code>telnet</code> \u547d\u4ee4\uff0c\u624b\u52a8\u53d1\u9001 HTTP\u3001SMTP \u8bf7\u6c42\u3002</p>"},{"location":"course_notes/cs144/lab0/#listening-and-connecting","title":"Listening and connecting","text":"<p>\u7528 <code>netcat</code> \u547d\u4ee4\u76d1\u542c\u7aef\u53e3\uff0c\u7136\u540e\u7528 <code>telnet</code> \u547d\u4ee4\u8fde\u63a5\u3002</p>"},{"location":"course_notes/cs144/lab0/#writing-a-network-program-using-an-os-stream-socket","title":"Writing a network program using an OS stream socket","text":"<p>\u6709\u8da3\u7684\u4e1c\u897f</p> <ul> <li>You will make use of a feature provided by the Linux kernel, and by most other operating systems: the ability to create a reliable bidirectional byte stream between two programs, one running on your computer, and the other on a different computer across the Internet<ul> <li>This feature is known as a stream socket.</li> </ul> </li> <li>The only thing the Internet really does is to give its \u201cbest effort\u201d to deliver short pieces of data, called Internet datagrams, to their destination.</li> <li>It\u2019s normally the job of the operating systems on either end of the connection to turn \u201cbest-effort datagrams\u201d (the abstraction the Internet provides) into \u201creliable byte streams\u201d (the abstraction that applications usually want).</li> </ul>"},{"location":"course_notes/cs144/lab0/#webget","title":"webget","text":"<p>A program to fetch Web pages over the Internet using the operating system\u2019s TCP support and stream-socket abstraction - \u8ddf\u4e4b\u524d\u7528 <code>telnet</code> \u53d1\u9001 HTTP \u8bf7\u6c42\u7c7b\u4f3c\u3002</p> <p>Starter code \u4e2d\u5df2\u7ecf\u5c01\u88c5\u597d\u4e86socket\u3001file descriptor\u7b49\u76f8\u5173API\uff0c\u6309\u7167\u903b\u8f91\u8c03\u7528\u5373\u53ef\u3002</p> <ul> <li>Create a TCP socket</li> <li>Connect to the host</li> <li>Send the HTTP GET request</li> <li>Read the response and write it to the standard output</li> </ul> <p><pre><code>\u276f ./apps/webget cs144.keithw.org /hello\nHTTP/1.1 200 OK\nDate: Mon, 01 Jul 2024 07:05:42 GMT\nServer: Apache\nLast-Modified: Thu, 13 Dec 2018 15:45:29 GMT\nETag: \"e-57ce93446cb64\"\nAccept-Ranges: bytes\nContent-Length: 14\nConnection: close\nContent-Type: text/plain\n</code></pre> <pre><code>\u276f cmake --build build --target check_webget\nTest project /home/ubuntu/CS144-minnow/build\n    Start 1: compile with bug-checkers\n1/2 Test #1: compile with bug-checkers ........   Passed   16.12 sec\n    Start 2: t_webget\n2/2 Test #2: t_webget .........................   Passed    1.06 sec\n\n100% tests passed, 0 tests failed out of 2\n\nTotal Test time (real) =  17.21 sec\nBuilt target check_webget\n</code></pre></p>"},{"location":"course_notes/cs144/lab0/#an-in-memory-reliable-byte-stream","title":"An in-memory reliable byte stream","text":"<p>\u901a\u8fc7\u4e0a\u9762\u7684\u64cd\u4f5c\uff0c\u6211\u4eec\u80fd\u611f\u53d7\u5230 \"reliable byte stream\" \u7684\u62bd\u8c61\u5728\u7f51\u7edc\u4e2d\u7684\u91cd\u8981\u6027\u3002 \u6240\u4ee5\u63a5\u4e0b\u6765\u6211\u4eec\u4f1a\u5b9e\u73b0\u4e00\u4e2a\u7c7b\u4f3c\u7684\u62bd\u8c61\uff0c\u4f46\u662f\u5728\u5185\u5b58\u4e2d\u8fdb\u884c\u3002</p> <p>\u6211\u4eec\u53ef\u4ee5\u4ece\u63a5\u53e3\u7684\u89d2\u5ea6\u6765\u7406\u89e3\u5b83\uff0c\u8fd8\u662f\u6bd4\u8f83\u76f4\u89c2\u7684\uff1a</p>"},{"location":"course_notes/cs144/lab0/#interface","title":"Interface","text":""},{"location":"course_notes/cs144/lab0/#writer","title":"Writer","text":"<pre><code>void push( std::string data ); // Push data to stream, but only as much as available capacity allows.\nvoid close();                  // Signal that the stream has reached its ending. Nothing more will be written.\n\nbool is_closed() const;              // Has the stream been closed?\nuint64_t available_capacity() const; // How many bytes can be pushed to the stream right now?\nuint64_t bytes_pushed() const;       // Total number of bytes cumulatively pushed to the stream\n</code></pre>"},{"location":"course_notes/cs144/lab0/#reader","title":"Reader","text":"<pre><code>std::string_view peek() const; // Peek at the next bytes in the buffer\nvoid pop( uint64_t len );      // Remove `len` bytes from the buffer\n\nbool is_finished() const;        // Is the stream finished (closed and fully popped)?\nuint64_t bytes_buffered() const; // Number of bytes currently buffered (pushed and not popped)\nuint64_t bytes_popped() const;   // Total number of bytes cumulatively popped from stream\n</code></pre>"},{"location":"course_notes/cs144/lab0/#implementation","title":"Implementation","text":"<pre><code>class ByteStream\n{\n  // other stuff...\n\n  uint64_t capacity_;\n  uint64_t bytes_pushed_;\n  uint64_t bytes_popped_;\n  bool closed_;\n  std::string buffer_;\n  bool error_ {};\n};\n</code></pre> <p>\u6700\u6838\u5fc3\u7684\u95ee\u9898\uff1a\u5e94\u8be5\u7528\u4ec0\u4e48\u6570\u636e\u7ed3\u6784\u6765\u5b9e\u73b0\u8fd9\u4e2a buffer\uff1f</p> <p>\u6211\u8fd9\u91cc\u4f7f\u7528\u4e86 <code>std::string</code>\uff0c\u5728<code>push</code>, <code>pop</code>\u7b49\u64cd\u4f5c\u65f6\uff0c\u8c03\u7528\u5b57\u7b26\u4e32\u7684\u76f8\u5173\u65b9\u6cd5\u5373\u53ef\u3002</p>"},{"location":"course_notes/cs144/lab0/#example","title":"Example","text":"<pre><code>void Writer::push( string data )\n{\n  if ( closed_ || error_ )\n    return;\n  uint64_t available = available_capacity();\n  uint64_t to_write = min( available, static_cast&lt;uint64_t&gt;( data.size() ) );\n  buffer_.append( data, 0, to_write );\n  bytes_pushed_ += to_write;\n}\n</code></pre> <p>Failure</p> <p>\u6211\u4e4b\u524d\u4f7f\u7528\u4e86<code>std::deque</code>\uff0c\u4f46\u5b83\u7684\u5185\u5b58\u4e0d\u662f\u8fde\u7eed\u7684\uff0c\u6240\u4ee5\u65e0\u6cd5\u76f4\u63a5\u4ece\u4e2d\u521b\u5efa <code>std::string_view</code>\u3002</p> <p>\u6d4b\u8bd5\u65f6\u4f1a\u62a5\u9519\uff1a</p> <pre><code>=================================================================\n==21534==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x615000000f00 at pc 0x7facec0240ad bp 0x7ffd51365bc0 sp 0x7ffd51365368\nREAD of size 17 at 0x615000000f00 thread T0\n    #0 0x7facec0240ac in MemcmpInterceptorCommon(void*, int (*)(void const*, void const*, unsigned long), void const*, void const*, unsigned long) ../../../../src/libsanitizer/sanitizer_common/sanitizer_common_interceptors.inc:932\n    #1 0x7facec024556 in __interceptor_memcmp ../../../../src/libsanitizer/sanitizer_common/sanitizer_common_interceptors.inc:964\n    #2 0x7facec024556 in __interceptor_memcmp ../../../../src/libsanitizer/sanitizer_common/sanitizer_common_interceptors.inc:959\n    #3 0x5647548b4425 in std::char_traits&lt;char&gt;::compare(char const*, char const*, unsigned long) (/home/ubuntu/CS144-minnow/build/tests/byte_stream_stress_test_sanitized+0x7c425) (BuildId: c6d7996a388c87d75222a8e99a442fa23629da7b)\n    #4 0x5647548d19da in std::basic_string_view&lt;char, std::char_traits&lt;char&gt; &gt;::compare(std::basic_string_view&lt;char, std::char_traits&lt;char&gt; &gt;) const (/home/ubuntu/CS144-minnow/build/tests/byte_stream_stress_test_sanitized+0x999da) (BuildId: c6d7996a388c87d75222a8e99a442fa23629da7b)\n    #5 0x5647548c772f in bool std::operator==&lt;char, std::char_traits&lt;char&gt; &gt;(std::basic_string_view&lt;char, std::char_traits&lt;char&gt; &gt;, std::__type_identity&lt;std::basic_string_view&lt;char, std::char_traits&lt;char&gt; &gt; &gt;::type) (/home/ubuntu/CS144-minnow/build/tests/byte_stream_stress_test_sanitized+0x8f72f) (BuildId: c6d7996a388c87d75222a8e99a442fa23629da7b)\n    #6 0x5647548bb6d1 in PeekOnce::execute(ByteStream&amp;) const (/home/ubuntu/CS144-minnow/build/tests/byte_stream_stress_test_sanitized+0x836d1) (BuildId: c6d7996a388c87d75222a8e99a442fa23629da7b)\n    #7 0x5647548c9135 in TestHarness&lt;ByteStream&gt;::execute(TestStep&lt;ByteStream&gt; const&amp;) (/home/ubuntu/CS144-minnow/build/tests/byte_stream_stress_test_sanitized+0x91135) (BuildId: c6d7996a388c87d75222a8e99a442fa23629da7b)\n    #8 0x5647548b2590 in stress_test(unsigned long, unsigned long, unsigned long) /home/ubuntu/CS144-minnow/tests/byte_stream_stress_test.cc:58\n    #9 0x5647548b3c8e in program_body() /home/ubuntu/CS144-minnow/tests/byte_stream_stress_test.cc:77\n    #10 0x5647548b3cb7 in main /home/ubuntu/CS144-minnow/tests/byte_stream_stress_test.cc:84\n    #11 0x7faceb3d0d8f in __libc_start_call_main ../sysdeps/nptl/libc_start_call_main.h:58\n    #12 0x7faceb3d0e3f in __libc_start_main_impl ../csu/libc-start.c:392\n    #13 0x5647548b0a04 in _start (/home/ubuntu/CS144-minnow/build/tests/byte_stream_stress_test_sanitized+0x78a04) (BuildId: c6d7996a388c87d75222a8e99a442fa23629da7b)\n\n0x615000000f00 is located 0 bytes after 512-byte region [0x615000000d00,0x615000000f00)\nallocated by thread T0 here:\n    #0 0x7facec039ba8 in operator new(unsigned long) ../../../../src/libsanitizer/asan/asan_new_delete.cpp:95\n    #1 0x5647548ea4c8 in std::__new_allocator&lt;char&gt;::allocate(unsigned long, void const*) /usr/include/c++/13/bits/new_allocator.h:147\n    #2 0x5647548e4453 in std::allocator&lt;char&gt;::allocate(unsigned long) /usr/include/c++/13/bits/allocator.h:198\n    #3 0x5647548e4453 in std::allocator_traits&lt;std::allocator&lt;char&gt; &gt;::allocate(std::allocator&lt;char&gt;&amp;, unsigned long) /usr/include/c++/13/bits/alloc_traits.h:482\n    #4 0x5647548e4453 in std::_Deque_base&lt;char, std::allocator&lt;char&gt; &gt;::_M_allocate_node() /usr/include/c++/13/bits/stl_deque.h:583\n    #5 0x5647548df1ff in std::_Deque_base&lt;char, std::allocator&lt;char&gt; &gt;::_M_create_nodes(char**, char**) /usr/include/c++/13/bits/stl_deque.h:684\n    #6 0x5647548d50ee in std::_Deque_base&lt;char, std::allocator&lt;char&gt; &gt;::_M_initialize_map(unsigned long) /usr/include/c++/13/bits/stl_deque.h:658\n    #7 0x5647548f8067 in std::_Deque_base&lt;char, std::allocator&lt;char&gt; &gt;::_Deque_base() /usr/include/c++/13/bits/stl_deque.h:460\n    #8 0x5647548f7fe6 in std::deque&lt;char, std::allocator&lt;char&gt; &gt;::deque() /usr/include/c++/13/bits/stl_deque.h:855\n    #9 0x5647548f6a9c in ByteStream::ByteStream(unsigned long) /home/ubuntu/CS144-minnow/src/byte_stream.cc:6\n    #10 0x5647548b63d0 in ByteStreamTestHarness::ByteStreamTestHarness(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, unsigned long) (/home/ubuntu/CS144-minnow/build/tests/byte_stream_stress_test_sanitized+0x7e3d0) (BuildId: c6d7996a388c87d75222a8e99a442fa23629da7b)\n    #11 0x5647548b14f8 in stress_test(unsigned long, unsigned long, unsigned long) /home/ubuntu/CS144-minnow/tests/byte_stream_stress_test.cc:24\n    #12 0x5647548b3c8e in program_body() /home/ubuntu/CS144-minnow/tests/byte_stream_stress_test.cc:77\n    #13 0x5647548b3cb7 in main /home/ubuntu/CS144-minnow/tests/byte_stream_stress_test.cc:84\n    #14 0x7faceb3d0d8f in __libc_start_call_main ../sysdeps/nptl/libc_start_call_main.h:58\n\nSUMMARY: AddressSanitizer: heap-buffer-overflow ../../../../src/libsanitizer/sanitizer_common/sanitizer_common_interceptors.inc:932 in MemcmpInterceptorCommon(void*, int (*)(void const*, void const*, unsigned long), void const*, void const*, unsigned long)\nShadow bytes around the buggy address:\n0x615000000c80: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n0x615000000d00: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n0x615000000d80: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n0x615000000e00: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n0x615000000e80: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n=&gt;0x615000000f00:[fa]fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n0x615000000f80: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd\n0x615000001000: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd\n0x615000001080: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd\n0x615000001100: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd\n0x615000001180: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\nShadow byte legend (one shadow byte represents 8 application bytes):\nAddressable:           00\nPartially addressable: 01 02 03 04 05 06 07 \nHeap left redzone:       fa\nFreed heap region:       fd\nStack left redzone:      f1\nStack mid redzone:       f2\nStack right redzone:     f3\nStack after return:      f5\nStack use after scope:   f8\nGlobal redzone:          f9\nGlobal init order:       f6\nPoisoned by user:        f7\nContainer overflow:      fc\nArray cookie:            ac\nIntra object redzone:    bb\nASan internal:           fe\nLeft alloca redzone:     ca\nRight alloca redzone:    cb\n==21534==ABORTING\n</code></pre>"},{"location":"course_notes/cs144/lab0/#summary","title":"Summary","text":"<p>Lab0\u4ee5\u7ec3\u624b\u4e3a\u4e3b\uff0c\u8f83\u7b80\u5355\u3002</p>"},{"location":"course_notes/cs144/lab1/","title":"Lab 1: stitching substrings into a byte stream","text":"<p>Over the coming weeks, you\u2019ll implement TCP yourself, to provide the byte-stream abstraction between a pair of computers separated by an unreliable datagram network.</p> <ul> <li>TCP receiver: receives datagrams and turns them into a reliable byte stream to be read from the socket by the application.</li> <li>TCP sender: divides its byte stream into short segments so that each segment fits into a single datagram, and sends the segments to the receiver.</li> </ul> <p>The network might reorder these datagrams, or drop them, or deliver them more than once. The receiver must reassemble the segments into the contiguous stream of bytes that they started out as.</p> <p>In this lab you\u2019ll write the data structure that will be responsible for this reassembly: a Reassembler.</p> <p>Why am I doing this?</p> <p>Providing a service or an abstraction on top of a different less-reliable service accounts for many of the interesting problems in networking.</p>"},{"location":"course_notes/cs144/lab1/#putting-substrings-in-sequence","title":"Putting substrings in sequence","text":""},{"location":"course_notes/cs144/lab1/#interface","title":"Interface","text":"<pre><code>// Insert a new substring to be reassembled into a ByteStream.\nvoid insert( uint64_t first_index, std::string data, bool is_last_substring );\n\n// How many bytes are stored in the Reassembler itself?\nuint64_t bytes_pending() const;\n\n// Access output stream reader\nReader&amp; reader() { return output_.reader(); }\n</code></pre> <p>Why am I doing this?</p> <p>TCP robustness against reordering and duplication comes from its ability to stitch arbitrary excerpts of the byte stream back into the original stream. Implementing this in a discrete testable module will make handling incoming segments easier.</p>"},{"location":"course_notes/cs144/lab1/#what-should-the-reassembler-store-internally","title":"What should the Reassembler store internally?","text":"<p>In principle, the <code>Reassembler</code> will have to handle three categories of knowledge:</p> <ol> <li>Bytes that are the next bytes in the stream. The <code>Reassembler</code> should push these to the stream (<code>output_.writer()</code>) as soon as they are known.</li> <li>Bytes that fit within the stream's available capacity but can't yet be written, because earlier bytes remain unknown. These should be stored internally in the <code>Reassembler</code>.</li> <li>Bytes that lie beyond the stream's available capacity. These should be discarded. The <code>Reassembler</code>'s will not store any bytes that can't be pushed to the ByteStream either immediately, or as soon as earlier bytes become known.</li> </ol> <p>The goal of this behavior is to limit the amount of memory used by the <code>Reassembler</code> and  the <code>ByteStream</code>, no matter how the incoming substrings arrive.</p> <p></p>"},{"location":"course_notes/cs144/lab1/#implementation","title":"Implementation","text":""},{"location":"course_notes/cs144/lab1/#data-structure","title":"Data structure","text":"<pre><code>class Reassembler\n{\n// ...\n\nprivate:\n  ByteStream output_; // the Reassembler writes to this ByteStream\n  uint64_t next_index_; // next byte index to write\n  uint64_t unassembled_bytes_; // total number of unassembled bytes\n  uint64_t eof_index_; // index that represents the end of the stream\n  std::map&lt;uint64_t, std::string&gt; substrings_; // store substrings\n};\n</code></pre> <p>\u53ef\u4ee5\u7ed3\u5408\u4e0a\u9762\u7684\u793a\u610f\u56fe\u7406\u89e3\u4ee3\u7801\u3002</p> <p>\u6211\u7528<code>std::map&lt;uint64_t, std::string&gt; substrings_</code>\u6765\u5b58\u50a8\u90a3\u4e9b\u8fd8\u4e0d\u80fd\u5199\u5165<code>ByteStream output_</code> \u7684\u3001\u5904\u4e8epending\u72b6\u6001\u7684substring\uff0c\u5176\u4e2d<code>uint64_t</code>\u662fsubstring\u7684\u8d77\u59cbindex\uff0c<code>std::string</code>\u662fsubstring\u7684\u5185\u5bb9\u3002 <code>unassembled_bytes_</code>\u5c31\u662f\u8fd9\u4e9bpending substring\u7684\u603b\u5b57\u8282\u6570\u3002<code>substrings_</code>\u5c31\u662f\u793a\u610f\u56fe\u4e2d \u7ea2\u8272\u7684\u90e8\u5206\uff1a\u201cbytes(substrings) that in the <code>Reassembler</code>'s internal storage\u201d\u3002</p> <p><code>next_index_</code>\u662f\u4e0b\u4e00\u4e2a\u53ef\u4ee5\u5199\u5165<code>ByteStream output_</code>\u7684index\uff0c\u4e5f\u5c31\u662f\u793a\u610f\u56fe\u4e2d\u7684\u7eff\u8272\u90e8\u5206(bytes buffered in the <code>ByteStream</code>)\u7684\u672b\u5c3eindex\u3002</p> <p>\u4ee5\u4e0a\u5c31\u662f\u672c\u6b21\u5b9e\u73b0\u7684data structure.</p>"},{"location":"course_notes/cs144/lab1/#insert","title":"<code>insert</code>","text":"<p>\u8f93\u5165\uff1asubstring\u4ee5\u53ca\u4e00\u4e9bmetadata\u3002\u601d\u8def\u5982\u4e0b\uff1a</p> <ol> <li>\u8fd9\u4e2asubstring\u6709\u6ca1\u6709\u8d85\u8fc7capacity\uff08\u8d85\u8fc7\u7684\u8bdd\u88c5\u4e0d\u4e0b\uff09\u8fd9\u4e2asubstring\u7684\u4fe1\u606f\u662f\u4e0d\u662f\u4e4b\u524d\u5df2\u7ecf\u5199\u5165\u4e86\uff08\u91cd\u590d\u7684substring\u6ca1\u7528\uff09</li> <li>IMPORTANT\uff1a\u68c0\u67e5\u8fd9\u4e2asubstring\u662f\u5426\u4e0e\u4e4b\u524d\u7684\u5b58\u5728<code>substrings_</code>\u4e2d\u7684substring\u6709overlap\uff0c\u5982\u679c\u6709overlap\uff0c\u5c31\u8981\u8fdb\u884c\u76f8\u5e94\u7684\u5904\u7406\u3002<ol> <li>\u975e\u5e38\u91cd\u8981\uff01\u5fc5\u987b\u4fdd\u8bc1<code>substrings_</code>\u4e2d\u7684substring\u662f\u4e92\u4e0doverlap\u7684\uff0c\u5426\u5219\u4f1a\u5bfc\u81f4<code>unassembled_bytes_</code>\u7b49\u6570\u636e\u8ba1\u7b97\u9519\u8bef\uff01</li> </ol> </li> <li>\u66f4\u65b0<code>substrings_</code>\uff0c<code>unassembled_bytes_</code></li> <li>\u904d\u5386<code>substrings_</code>\uff0c\u770b\u770b\u65b0\u52a0\u5165\u7684substring\u662f\u5426\u80fd\u591f\u5c06\u4e4b\u524d\u5206\u6563\u7684substring\u8fde\u63a5\u8d77\u6765\uff0c\u5e76\u80fd\u591f\u5199\u5165<code>ByteStream</code>.</li> </ol> <p>\u8fd8\u6709\u4e00\u4e9b\u7ec6\u8282\uff0c\u6bd4\u5982index\u7684\u66f4\u65b0\u3001EOF\u7684\u5224\u5b9a\u7b49\u7b49\uff0c\u4e0d\u518d\u8d58\u8ff0\u3002</p> <pre><code>\u276f cmake --build build --target check1\nTest project /home/ubuntu/CS144-minnow/build\n      Start  1: compile with bug-checkers\n 1/17 Test  #1: compile with bug-checkers ........   Passed    0.29 sec\n      Start  3: byte_stream_basics\n 2/17 Test  #3: byte_stream_basics ...............   Passed    0.01 sec\n      Start  4: byte_stream_capacity\n 3/17 Test  #4: byte_stream_capacity .............   Passed    0.01 sec\n      Start  5: byte_stream_one_write\n 4/17 Test  #5: byte_stream_one_write ............   Passed    0.01 sec\n      Start  6: byte_stream_two_writes\n 5/17 Test  #6: byte_stream_two_writes ...........   Passed    0.01 sec\n      Start  7: byte_stream_many_writes\n 6/17 Test  #7: byte_stream_many_writes ..........   Passed    0.06 sec\n      Start  8: byte_stream_stress_test\n 7/17 Test  #8: byte_stream_stress_test ..........   Passed    0.03 sec\n      Start  9: reassembler_single\n 8/17 Test  #9: reassembler_single ...............   Passed    0.01 sec\n      Start 10: reassembler_cap\n 9/17 Test #10: reassembler_cap ..................   Passed    0.01 sec\n      Start 11: reassembler_seq\n10/17 Test #11: reassembler_seq ..................   Passed    0.02 sec\n      Start 12: reassembler_dup\n11/17 Test #12: reassembler_dup ..................   Passed    0.04 sec\n      Start 13: reassembler_holes\n12/17 Test #13: reassembler_holes ................   Passed    0.01 sec\n      Start 14: reassembler_overlapping\n13/17 Test #14: reassembler_overlapping ..........   Passed    0.02 sec\n      Start 15: reassembler_win\n14/17 Test #15: reassembler_win ..................   Passed    0.32 sec\n      Start 37: compile with optimization\n15/17 Test #37: compile with optimization ........   Passed    0.07 sec\n      Start 38: byte_stream_speed_test\n             ByteStream throughput: 3.08 Gbit/s\n16/17 Test #38: byte_stream_speed_test ...........   Passed    0.25 sec\n      Start 39: reassembler_speed_test\n             Reassembler throughput: 6.91 Gbit/s\n17/17 Test #39: reassembler_speed_test ...........   Passed    0.40 sec\n\n100% tests passed, 0 tests failed out of 17\n\nTotal Test time (real) =   1.59 sec\nBuilt target check1\n</code></pre>"},{"location":"course_notes/cs144/lab2/","title":"Lab 2: the TCP receiver","text":""},{"location":"course_notes/cs144/lab2/#overview","title":"Overview","text":"<p>In this lab, you will implement the <code>TCPReceiver</code>, the part of a TCP implementation that handles the incoming byte stream.</p> <p>The <code>TCPReceiver</code> receives messages from the peer's sender (via the <code>receive()</code> method) and turns them into calls to a <code>Reassembler</code>, which eventually writes to the incoming <code>ByteStream</code>. Applications read from this <code>ByteStream</code>.</p> <p>Meanwhile, the <code>TCPReceiver</code> also generates messages that go back to the peer\u2019s sender, via the <code>send()</code> method. These \u201creceiver messages\u201d are responsible for telling the sender:</p> <ol> <li>the index of the \"first unassembled\" byte, which is called the \"acknowledgment number\" or \"ackno\". This is the first byte that the receiver needs from the sender.</li> <li>the available capacity in the output ByteStream -  the \"window size\".</li> </ol> <p>Together, the ackno and window size describe describes the receiver\u2019s window: a range of indexes that the TCP sender is allowed to send.</p>"},{"location":"course_notes/cs144/lab2/#the-tcp-receiver","title":"The TCP Receiver","text":"<p>In TCP,  acknowledgment means, \"What's the index of the next byte that the receiver needs so it can reassemble more of the <code>ByteStream</code>?\" Flow control means,  \"What range of indices is the receiver interested and willing to receive?\" (a function of its available capacity). This tells the sender how much it's allowed to send.</p>"},{"location":"course_notes/cs144/lab2/#translating-between-64-bit-indexes-and-32-bit-seqnos","title":"Translating between 64-bit indexes and 32-bit seqnos","text":"<p>In <code>Reassembler</code>, each individual byte has a 64-bit index. </p> <p>IMPORTANT</p> <p>In the TCP headers, however, space is precious, and each byte's index in the stream is represented not with a 64-bit index but with a 32-bit \"sequence number\" or \"seqno\". This adds three complexities:</p> <ol> <li>Your implementation needs to plan for 32-bit integers to wrap around. Streams in TCP can be arbitrarily long \u2014 there\u2019s no limit to the length of a ByteStream that can be sent over TCP.</li> <li>TCP sequence numbers start at a random value: The first sequence number for a stream is a random 32-bit number called the Initial Sequence Number (ISN). \u4f7f\u7528\u968f\u673a\u6570\u7684\u76ee\u7684\u662fimprove robustness and avoid getting confused by old segments belonging to earlier connections.</li> <li>The logical beginning and ending each occupy one sequence number: The SYN  (beginning-of-stream) and FIN (end-of-stream) control flags each occupy one sequence number.</li> </ol> <p>Sequence numbers (seqnos) are transmitted in the header of each TCP segment.</p> <p>Some terminology:</p> <ul> <li>absolute sequence number: always starts at zero and doesn\u2019t wrap.</li> <li>stream index: an index for each byte in the stream, starting at zero.</li> </ul> <p>Consider the byte stream <code>cat</code>:</p> element SYN c a t FIN seqno \\(2^{32}-2\\) \\(2^{32}-1\\) 0 1 2 absolute seqno 0 1 2 3 4 stream index 0 1 2 <p>\u5728\u8fd9\u4e9b\u4e0d\u540c\u7684index\u4e4b\u95f4\u8f6c\u6362\u6bd4\u8f83tricky\uff0c\u6240\u4ee5\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684type <code>Wrap32</code>\uff0c\u7528\u6765\u8868\u793asequence  number\uff0c\u540c\u65f6\u63d0\u4f9b\u4e00\u4e9b\u8f6c\u6362\u51fd\u6570\u3002</p> <ol> <li><code>static Wrap32 Wrap32::wrap( uint64 t n, Wrap32 zero point )</code><ol> <li>\u7ed9\u5b9aabsolute sequence number\u548c\u201c\u96f6\u70b9\u201d\uff0c\u8fd4\u56de\u4e00\u4e2aWrap32 object(\u8868\u793asequence number)\u3002</li> <li>\u5c06absolute sequence number\u4e0e\u96f6\u70b9\u76f8\u52a0\u540e\u53d6\u6a21\u5c31\u884c\u3002</li> </ol> </li> <li><code>uint64 t unwrap( Wrap32 zero point, uint64 t checkpoint ) const</code><ol> <li>\u7ed9\u5b9a\u4e00\u4e2a\u201c\u96f6\u70b9\u201d\u548c\u4e00\u4e2acheckpoint\uff0c\u5c06sequence number\u8f6c\u5316\u4e3aabsolute sequence number\u3002</li> <li>\u56e0\u4e3asequence number\u662f\u201c\u53d6\u6a21\u201d\u540e\u7684\u7ed3\u679c\uff0c\u6240\u4ee5\u6709\u51e0\u4e2a\u53ef\u80fd\u7684\u503c\uff0c\u9009\u62e9\u4e00\u4e2a\u6700\u63a5\u8fd1checkpoint\u7684\u503c\u3002</li> </ol> </li> </ol>"},{"location":"course_notes/cs144/lab2/#implementing-the-tcpreceiver","title":"Implementing the TCPReceiver","text":"<p>TCPReceiver\u8981\u5e72\u561b\uff1f</p> <ol> <li> <p>receive messages from its peer\u2019s sender and reassemble the <code>ByteStream</code> using a <code>Reassembler</code>.</p> <ol> <li> <p>\u5b83\u6536\u5230\u7684message be like:</p> <pre><code>struct TCPSenderMessage\n{\n    Wrap32 seqno { 0 };\n\n    bool SYN {};\n    std::string payload {};\n    bool FIN {};\n\n    bool RST {};\n\n    // How many sequence numbers does this segment use?\n    size_t sequence_length() const { return SYN + payload.size() + FIN; }\n};\n</code></pre> </li> </ol> </li> <li> <p>send messages back to the peer\u2019s sender that contain the acknowledgment number (ackno) and window size.</p> <ol> <li> <p>\u5b83\u53d1\u9001\u56de\u53bb\u7684message be like:</p> <pre><code>struct TCPReceiverMessage\n{\n    std::optional&lt;Wrap32&gt; ackno {};\n    uint16_t window_size {};\n    bool RST {};\n};\n</code></pre> </li> </ol> </li> </ol> <p>\u90a3\u4e48\u6211\u4eec\u8981implement\u7684\u5b9e\u9645\u4e0a\u662f<code>TCPReceiver</code>\u7684<code>receive</code>\u548c<code>send</code>\u65b9\u6cd5\u3002</p> <ul> <li><code>receive()</code><ul> <li>\u63a5\u53d7<code>TCPSenderMessage</code>\uff0c\u5c06sequence number\u8f6c\u5316\u4e3astream index\uff0c\u7136\u540e\u8c03\u7528<code>Reassembler</code>\u7684<code>insert</code>\u65b9\u6cd5\u3002</li> </ul> </li> <li><code>send()</code><ul> <li>\u751f\u6210<code>TCPReceiverMessage</code>\uff0c\u5305\u542backno\u548cwindow size\u3002</li> </ul> </li> </ul> <p>\u5f53\u7136\u8fd8\u6709\u4e00\u4e9bedge cases\uff0c\u6bd4\u5982\uff1a\u6536\u5230<code>RST</code>\uff0c\u6536\u5230<code>FIN</code>\u7b49\u7b49\uff0c\u4f1a\u6d89\u53ca\u4e00\u4e9bindex\u7684tricky\u7ec6\u8282\uff0c\u4e0d\u518d\u8d58\u8ff0\u3002</p> <p>\u56e0\u4e3a\u201c\u91cd\u7ec4\u4e71\u5e8f\u7684\u5b57\u8282\u6d41\u201d\u8fd9\u4e2a\u95ee\u9898\u5728Reassembler\u4e2d\u5df2\u7ecf\u89e3\u51b3\u4e86\uff0c\u6240\u4ee5\u5728TCPReceiver\u89e3\u51b3\u7684\u95ee\u9898 \u4e3b\u8981\u662f\u201c\u5404\u79cdindex\u4e4b\u95f4\u7684\u8f6c\u6362\u201d\uff0c\u96be\u5ea6\u5e76\u4e0d\u5927\uff0c\u4f46\u662fedge cases\u6bd4\u8f83\u591a\u3002</p> <p>\u4e00\u4e9b\u6211\u89c9\u5f97\u6709\u8da3\u7684\u5c0f\u7ec6\u8282\uff1a\u5728\u672c\u6b21\u5b9e\u9a8c\u4e2d\uff0c\u6bcf\u4e00\u6b21<code>receive()</code>\u4e4b\u540e\u90fd\u4f1a\u8c03\u7528<code>send()</code>\uff0c\u5c06ackno\u548cwindow size\u53d1\u9001\u56de\u53bb\u3002 \u4f46\u662f\u5728\u5b9e\u9645\u7684TCP\u534f\u8bae\u4e2d\uff0c\u4f1a\u6709\u4e00\u4e9b\u4f18\u5316\uff0c\u7528\u4e8e\u51cf\u5c11\u7f51\u7edc\u4e2d\u7684 ACK \u5305\u6570\u91cf\uff0c\u63d0\u9ad8\u5e26\u5bbd\u5229\u7528\u7387\uff0c\u5982\uff1a Delayed ACK\uff08\u4f7f\u7528\u4e00\u4e2a ACK \u6765\u786e\u8ba4\u591a\u4e2a\u6570\u636e\u6bb5\uff09\u3001Piggybacking\uff08\u5c06 ACK \u653e\u5728\u6570\u636e\u5305\u4e2d\uff09\u7b49\u7b49\u3002</p>"},{"location":"course_notes/cs144/lab3/","title":"Lab3: the TCP sender","text":""},{"location":"course_notes/cs144/lab3/#overview","title":"Overview","text":"<p>The <code>TCPSender</code> is a tool that translates from an outbound byte stream to segments that will become the payloads of unreliable datagrams.</p>"},{"location":"course_notes/cs144/lab3/#the-tcp-sender","title":"The TCP Sender","text":"<p>It will be your <code>TCPSender</code>\u2019s responsibility to:</p> <ul> <li>Keep track of the receiver\u2019s window.</li> <li>Fill the window when possible, by reading from the <code>ByteStream</code>, creating new TCP segments (including SYN and FIN flags if needed), and sending them.</li> <li>Keep track of which segments have been sent but not yet acknowledged by the receiver -- we call these \u201coutstanding\u201d segments.</li> <li>Re-send outstanding segments if enough time passes since they were sent, and they haven\u2019t been acknowledged yet.</li> </ul> <p>Why am I doing this?</p> <p>The basic principle is to send whatever the receiver will allow us to send (filling the window), and keep retransmitting until the receiver acknowledges each segment. This is called \u201cautomatic repeat request\u201d (ARQ).</p>"},{"location":"course_notes/cs144/lab3/#how-does-the-tcpsender-know-if-a-segment-was-lost","title":"How does the <code>TCPSender</code> know if a segment was lost?","text":"<p>\u5728\u8fd9\u4e2a\u90e8\u5206\uff0cLab\u6587\u6863\u8bb2\u4e86\u5f88\u591a\uff0c\u5305\u62ec\u4e00\u4e9b\u7ec6\u679d\u672b\u8282\u7684\u4e1c\u897f\uff0c\u6211\u603b\u7ed3\u4e00\u4e0b\u6838\u5fc3\u7684idea\uff1a</p> <ul> <li>In addition to sending those segments, the <code>TCPSender</code> also has to keep track of its outstanding segments until the sequence numbers they occupy have been fully acknowledged.<ul> <li>If the <code>TCPSender</code> found that the oldest-sent segment has been outstanding for too long without acknowledgment, it will retransmit this segment.</li> </ul> </li> <li>What does it mean for \"waiting too long\"? <ul> <li>\u4ee5retransmission timeout (RTO)\u4e3a\u6807\u51c6\uff0c\u5982\u679c\u4e00\u4e2asegment\u5728RTO\u65f6\u95f4\u5185\u6ca1\u6709\u88abacknowledged\uff0c\u90a3\u5c31\u662f\u201cwaiting too long\u201d\u3002</li> </ul> </li> </ul> <p>\u6765\u5230\u5177\u4f53\u7684\u5b9e\u73b0\uff0c<code>TCPSender</code>\u5c06\u4f1a\u6709\u4e00\u4e2aretransmission timer: an alarm that can be started at a certain time, and the alarm goes off (or \u201cexpires\u201d) once the RTO has elapsed.</p> <ul> <li>\u6bcf\u4e00\u6b21\u53d1\u9001segment\u7684\u65f6\u5019\uff0c\u5982\u679ctimer\u6ca1\u6253\u5f00\uff0c\u5c31\u6253\u5f00\u4e00\u4e2a\u65b0\u7684timer\uff0c\u8ba1\u65f6RTO\u3002</li> <li>\u5982\u679c\u6240\u6709\u53d1\u51fa\u7684segments\u90fd\u88abacknowledged\uff0c\u5c31\u5173\u95edtimer\u3002</li> <li>\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\uff0c\u5982\u679ctimer\u8fc7\u671f\uff1a<ul> <li>\u91cd\u53d1\u6700\u8001\u7684outstanding segment\u3002</li> <li>\u5982\u679cwindow size is nonzero\uff08\u8bf4\u660ereceiver\u8fd8\u6709\u7a7a\u95f4\u63a5\u53d7\u65b0\u7684segment\uff09\uff1a<ul> <li>\u8bb0\u5f55the number of consecutive retransmissions. \u82e5\u91cd\u4f20\u6b21\u6570\u8fc7\u591a\uff0c\u5c31\u8ba4\u4e3a\u7f51\u7edc\u8fde\u63a5\u5931\u6548</li> <li>\u91cd\u8bbeRTO\u4e3a\u4e4b\u524d\u7684\u4e24\u500d -- \u201cexponential backoff\u201d\uff0c\u76ee\u7684\u4e3a\u51cf\u5c11\u91cd\u4f20\u6b21\u6570\uff0c\u907f\u514d\u7f51\u7edc\u62e5\u585e\u3002</li> </ul> </li> <li>Restart the timer.</li> </ul> </li> <li>When the receiver gives the sender an ackno that acknowledges the successful receipt of new data:<ul> <li>Set the RTO back to its \u201cinitial value.\u201d</li> <li>Reset the count of \u201cconsecutive retransmissions\u201d back to zero.</li> <li>\u5982\u679c\u4ecd\u6709outstanding segments\uff0crestart the timer.</li> </ul> </li> </ul>"},{"location":"course_notes/cs144/lab3/#implementing-the-tcpsender","title":"Implementing the <code>TCPSender</code>","text":""},{"location":"course_notes/cs144/lab3/#void-push-const-transmitfunction-transmit","title":"<code>void push( const TransmitFunction&amp; transmit );</code>","text":"<p>\u8fd9\u4e2a\u51fd\u6570\u5c06TCP segments\u51c6\u5907\u597d\u4e4b\u540e\uff0c\u901a\u8fc7<code>transmit</code> function\u53d1\u9001\u51fa\u53bb\u3002\u5b83\u8ba9\u6bcf\u4e2asegment\u5c3d\u53ef\u80fd\u5927\uff0c\u4f46\u4e0d\u80fd\u8d85\u8fc7receiver\u7684window size\u548cMAX PAYLOAD SIZE (1452 bytes). \u53d1\u9001segment\u4e4b\u540e\uff0c\u5c06\u5176\u8bb0\u5f55\u5230<code>outstanding_segments</code>\u4e2d\uff0c \u5982\u679c\u662f\u7b2c\u4e00\u6b21\u53d1\u9001\uff0c\u5c31\u6253\u5f00timer\u3002</p>"},{"location":"course_notes/cs144/lab3/#void-receive-const-tcpreceivermessage-msg","title":"<code>void receive( const TCPReceiverMessage&amp; msg );</code>","text":"<p>\u8fd9\u4e2a\u51fd\u6570\u5904\u7406\u6765\u81eareceiver\u7684message\uff0c\u5305\u62ecacknowledgment number\u548cwindow size\u3002\u5982\u679c\u6709\u65b0\u7684acknowledgment number\uff0c \u5c31\u628a\u5bf9\u5e94\u7684segment\u4ece<code>outstanding_segments</code>\u4e2d\u5220\u9664\uff0c\u5982\u679c\u6240\u6709\u7684segments\u90fd\u88abacknowledged\uff0c\u5c31\u5173\u95edtimer\uff0c \u5982\u679c\u4ecd\u6709outstanding segments\uff0c\u5c31restart timer\u3002</p>"},{"location":"course_notes/cs144/lab3/#void-tick-uint64-t-ms_since_last_tick-const-transmitfunction-transmit","title":"<code>void tick( uint64 t ms_since_last_tick, const TransmitFunction&amp; transmit );</code>","text":"<p>\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\uff0c\u8fd9\u4e2a\u51fd\u6570\u4f1a\u88ab\u8c03\u7528\u3002\u6211\u4eec\u5b9e\u73b0\u7684\u65f6\u5019\u4ec5\u80fd\u53c2\u8003<code>ms_since_last_tick</code>\uff0c\u800c\u4e0d\u80fd\u76f4\u63a5\u4f7f\u7528 \u771f\u5b9e\u7684\u65f6\u95f4\uff08\u65b9\u4fbfLab\u6784\u9020\u6d4b\u8bd5\uff09\u3002\u8fd9\u4e2a\u51fd\u6570\u4e5f\u4f1a\u8c03\u7528timer\u7684<code>tick</code>\u51fd\u6570\uff0c\u5982\u679ctimer\u8fc7\u671f\uff0c\u5c31\u6267\u884c\u4e0a\u8ff0 \u7684retransmission\u903b\u8f91\u3002</p>"},{"location":"course_notes/cs188/homework/hw1/","title":"HW1","text":""},{"location":"course_notes/cs188/homework/hw1/#q7-hive-minds-swarm-movement","title":"Q7 Hive Minds: Swarm Movement","text":"<p>\u5148\u56de\u987e\u4e00\u4e0badmissibility:</p> <p>Defining \\(h^*(n)\\) as the true optimal forward cost to reach a goal state a given node \\(n\\),  we can formulate the adimissibility constraint as:</p> \\[ \\forall n, \\quad 0 \\leq h(n) \\leq h^*(n) \\] <p>\u4ee5\u9009\u98794\u4e3a\u4f8b\uff0c\\(h(n)\\)\u662fMax of costs of optimal paths for each insect to its goal if it were acting alone in the environment, unobstructed by the other insects. \u771f\u5b9e\u7684cost \\(h^*(n)\\)\u80af\u5b9a\u4f1a\u5927\u4e8e\u7b49\u4e8e\\(h(n)\\)\uff0c\u56e0\u4e3ainsect\u4e4b\u95f4\u4f1a\u4e92\u76f8\u5e72\u6270\uff0c\u6240\u4ee5\\(h(n)\\)\u662fadmissible\u7684\u3002</p>"},{"location":"course_notes/cs188/homework/hw1/#q10-hive-minds-lost-at-night","title":"Q10 Hive Minds: Lost at Night","text":"<p>It is night and you control a single insect. You know the maze, but you do not know what square the insect will start in. You must pose a search problem whose solution is an all-purpose sequence of actions such that, after executing those actions, the insect is guaranteed to be on the exit square, regardless of initial position. The insect executes the actions mindlessly and does not know whether its moves succeed: if it uses an action which would move it in a blocked direction, it will stay where it is. For example, in the maze below, moving right twice guarantees that the insect will be at the exit regardless of its starting position.</p> <p></p>"},{"location":"course_notes/cs188/homework/hw1/#q101","title":"Q10.1","text":"<p>Which of the following state representations could be used to solve this problem?</p> <p>Answer: A list of boolean variables, one for each position in the maze, indicating whether the insect could be in that position.</p> <p>\u6211\u53bb\u3002\u3002\u3002\u7b2c\u4e00\u6b21\u78b0\u5230\u8fd9\u79cd\u95ee\u9898\uff0c\u60f3\u4e86\u534a\u5929\u624d\u641e\u61c2\u3002state\u662f\u4e00\u4e2aboolean list\uff0c\u8868\u793ainsect\u53ef\u80fd\u5728\u54ea\u4e2a\u4f4d\u7f6e\u3002\u6211\u4eec\u5e76\u4e0d\u77e5\u9053insect\u7684\u521d\u59cb\u4f4d\u7f6e\uff0c\u6240\u4ee5\u4e00\u5f00\u59cb\u6240\u6709\u4f4d\u7f6e\u90fd\u662f\u53ef\u80fd\u7684\uff0c\u8fd9\u4e2alist\u91cc\u9762\u5168\u662fTrue\u3002\u7136\u540e\u6211\u4eec\u5f00\u59cb\u641c\u7d22\uff0c\u6bcf\u6b21\u641c\u7d22\u90fd\u4f1a\u6839\u636einsect\u7684\u79fb\u52a8\u65b9\u5411\u66f4\u65b0\u8fd9\u4e2alist\u3002</p> <p>Goal test: \\(list[x_{goal}][y_{goal}] = True\\), \u5176\u4ed6\u4f4d\u7f6e\u90fd\u662f\\(False\\)\u3002</p> <p>Successor: Actions available to us are {NORTH,SOUTH,EAST,WEST}\u3002\u4ee5WEST\u4e3a\u4f8b\uff1a</p> <pre><code>for x in range(width):\n\n    for y in range(height):\n\n        bool_next[x][y] = bool[x+1][y] or (bool[x][y] and is_wall((x-1,y)))\n</code></pre>"},{"location":"course_notes/cs188/homework/hw1/#q103","title":"Q10.3","text":"<p>\u53ea\u8981\u6ee1\u8db3\\(\\forall n, \\quad 0 \\leq h(n) \\leq h^*(n)\\)\uff0c\u90fd\u80fd\u7b97admissible.</p>"},{"location":"course_notes/cs188/homework/hw1/#summary","title":"Summary","text":"<p>HW1\u9898\u91cf\u5927\uff0c\u5e76\u4e14\u9700\u8981\u4e00\u5b9a\u7684\u601d\u8003\uff0c\u7ed9\u6211\u7684\u5468\u672b\u4e0a\u4e86\u5f88\u5927\u7684\u5f3a\u5ea6(\u6211\u611f\u89c9\u5728\u8fd9\u6837997\u4e0b\u53bb\u6211\u5c31\u8981\u7d2f\u6b7b\u4e86)\u3002</p> <p>HW1\u5927\u6982\u5305\u62ec\u4e86\u4ee5\u4e0b\u5185\u5bb9\uff1aA* Search\u3001DFS\u3001BFS\u3001UCS\u7684\u7406\u89e3\u4ee5\u53ca\u7ec6\u8282\u6df1\u6316\uff1bstate space\u7684\u6784\u5efa\u3001admissible heuristic\u548cconsistent heuristic\u7684\u7406\u89e3\u3002</p> <p></p>"},{"location":"course_notes/cs188/homework/hw2/","title":"HW2","text":""},{"location":"course_notes/cs188/homework/hw2/#q1-campus-layout","title":"Q1 Campus Layout","text":"<p>\u6b64\u9898\u7ed3\u5408\u4e00\u4e2a\u5b9e\u9645\u4f8b\u5b50\u8003\u5bdfconstraint, arc consistency\u7b49\u95ee\u9898\u3002 \u4e0b\u9762\u662f\u6d89\u53ca\u5230\u7684\u77e5\u8bc6\u70b9\uff1a</p> <p>Unary constraints are constraints that involve a single variable.</p> <p>After an arc is enforced, every value in the domain of the tail has at least one possible value in the domain of the head such that the two values satisfy all constraints with each other. If a value in the tail's domain does not satisfy this, that value can be removed.</p> <p>When a domain changes after enforcing an arc, all arcs that end at that variable must be re-added to the queue.</p>"},{"location":"course_notes/cs188/homework/hw2/#q2-csp-properties","title":"Q2 CSP Properties","text":""},{"location":"course_notes/cs188/homework/hw2/#q21","title":"Q2.1","text":"<ul> <li>Even when using arc consistency, backtracking might be needed to solve a CSP.</li> <li>Arc consistency is often not sufficient on its own to solve a CSP.</li> <li>Consider a CSP with three variables, where the only constraint involves all three variables. Because it is not a binary constraint, arc consistency will not reduce any of the domains, and normal backtracking search is necessary.</li> <li>Even when using forward checking, backtracking might be needed to solve a CSP.</li> </ul> <p>\u53ef\u4ee5\u4e3e\u4e00\u4e2a\u4f8b\u5b50\u8bf4\u660e:</p> <p>Explicitly, consider a CSP with variables \\(A \\in\\{1,2\\}, B \\in\\{1,2\\}, C \\in\\{3,4\\}\\). The only constraint is that \\(A+B&gt;C\\). Using MRV and LCV, and breaking ties by choosing the lowest value/variable, the first assignment would be \\(A=1\\), while the only solution is \\(A=2, B=\\) \\(2, C=3\\).</p>"},{"location":"course_notes/cs188/homework/hw2/#q22","title":"Q2.2","text":"<p>For a CSP with binary constraints that has no solution, some initial values may still pass arc consistency before any variable is assigned.</p> <p>Example: Three variables, \\(A, B, C\\), each with domains \\(\\{1,2\\}\\). The only constraint is that no two variables can have the same value. Enforcing arc consistency will not eliminate any value from any domain, because arc consistency only considers two variables at a time.</p>"},{"location":"course_notes/cs188/homework/hw2/#q3-4-queens","title":"Q3 4-Queens","text":"<p>Solve the 4-queens problem using the min-conflicts algorithm.</p>"},{"location":"course_notes/cs188/homework/hw2/#q4-arc-consistency","title":"Q4 Arc Consistency","text":"<p>\u9898\u5982\u5176\u540d\uff0c\u975e\u5e38\u7b80\u5355\u7684\u63a8\u5bfc\u3002</p>"},{"location":"course_notes/cs188/homework/hw2/#q5-arc-consistency-properties","title":"Q5 Arc Consistency Properties","text":"<p>In general, to determine whether the CSP has a solution, enforcing arc consistency alone is not sufficient; backtracking may be required.</p>"},{"location":"course_notes/cs188/homework/hw2/#q6-backtracking-arc-consistency","title":"Q6 Backtracking Arc Consistency","text":""},{"location":"course_notes/cs188/lecture/BN-Independence/","title":"BN: Independence","text":""},{"location":"course_notes/cs188/lecture/BN-Independence/#d-separation","title":"D-Separation","text":"<p>A node is conditionally independent of all its ancestor nodes in the graph given all of its parents.</p>"},{"location":"course_notes/cs188/lecture/BN-Independence/#causal-chains","title":"Causal Chains","text":"<p>Figure 1 is a configuration of three nodes known as a causal chain.</p> \\[ P(x, y, z)=P(z \\mid y) P(y \\mid x) P(x) \\] <p>X and Z are not guaranteed to be independent.</p> <p>However, we can make the statement that \\(X \\newcommand{\\indep}{\\perp \\!\\!\\! \\perp} \\indep Z \\mid Y\\).</p> \\[ \\begin{aligned} P(X \\mid Z, y) &amp; =\\frac{P(X, Z, y)}{P(Z, y)}=\\frac{P(Z \\mid y) P(y \\mid X) P(X)}{\\sum_x P(X, y, Z)}=\\frac{P(Z \\mid y) P(y \\mid X) P(X)}{P(Z \\mid y) \\sum_x P(y \\mid x) P(x)} \\\\ &amp; =\\frac{P(y \\mid X) P(X)}{\\sum_x P(y \\mid x) P(x)}=\\frac{P(y \\mid X) P(X)}{P(y)}=P(X \\mid y) \\end{aligned} \\]"},{"location":"course_notes/cs188/lecture/BN-Independence/#_1","title":"\u4f8b\u5b50","text":"<p>\u672a\u89c2\u5bdf\u4e2d\u95f4\u8282\u70b9\uff1a</p> <ul> <li>\u4f60\u6709\u4e00\u4e2a\u670b\u53cb \\(Y\\), \u4ed6\u53d7\u4f60\u7684\u60c5\u7eea \\(X\\) \u5f71\u54cd\uff08\u5982\u679c\u4f60\u5f00\u5fc3, \u4ed6\u4e5f\u5f00\u5fc3\uff09, \u540c\u65f6\u4ed6\u4e5f\u4f1a\u5f71\u54cd\u4ed6\u5bb6\u91cc\u7684\u5ba0\u7269 \\(Z\\) (\u4ed6\u5f00\u5fc3\u65f6\u5ba0\u7269\u4e5f\u9ad8\u5174)\u3002</li> <li>\u5982\u679c\u4f60\u4e0d\u77e5\u9053\u670b\u53cb \\(Y\\) \u7684\u60c5\u7eea, \u4f60\u53ea\u80fd\u731c\u6d4b\u4f60\u81ea\u5df1\u7684\u60c5\u7eea\u53ef\u80fd\u901a\u8fc7\u670b\u53cb\u5f71\u54cd\u5230\u4e86\u5ba0\u7269\u3002\u56e0\u6b64, \\(X\\) \u548c \\(Z\\) \u4e4b\u95f4\u6709\u4e00\u79cd\u5173\u8054\u3002</li> </ul> <p>\u89c2\u5bdf\u4e2d\u95f4\u8282\u70b9\uff1a</p> <ul> <li>\u73b0\u5728, \u4f60\u77e5\u9053\u4e86\u670b\u53cb \\(Y\\) \u7684\u60c5\u7eea\u3002\u65e0\u8bba\u4f60\u5982\u4f55\u60c5\u7eea, \u4f60\u53ea\u9700\u8981\u770b\u670b\u53cb\u7684\u60c5\u7eea\u6765\u5224\u65ad\u5ba0\u7269\u7684\u72b6\u6001\u3002\u56e0\u6b64, \u77e5\u9053\u4e86 \\(Y\\) \u4e4b\u540e, \\(X\\) \u548c \\(Z\\) \u4e4b\u95f4\u5c31\u6ca1\u6709\u76f4\u63a5\u5173\u7cfb\u4e86\u3002</li> </ul>"},{"location":"course_notes/cs188/lecture/BN-Independence/#common-cause","title":"Common Cause","text":"\\[ P(x, y, z)=P(x \\mid y) P(z \\mid y) P(y) \\] <p>X is not guaranteed to be independent of Z.</p> <p>\\(X \\indep Z \\mid Y\\): X and Z are independent if Y is observed.</p> \\[ P(X \\mid Z, y)=\\frac{P(X, Z, y)}{P(Z, y)}=\\frac{P(X \\mid y) P(Z \\mid y) P(y)}{P(Z \\mid y) P(y)}=P(X \\mid y) \\]"},{"location":"course_notes/cs188/lecture/BN-Independence/#_2","title":"\u4f8b\u5b50","text":"<ul> <li>\u5047\u8bbe \\(Y\\) \u662f\u5929\u6c14, \\(X\\) \u662f\u4eba\u4eec\u662f\u5426\u5e26\u4f1e, \\(Z\\) \u662f\u5730\u9762\u662f\u5426\u6e7f\u6ed1\u3002\u5929\u6c14\u5f71\u54cd\u4eba\u4eec\u662f\u5426\u5e26\u4f1e\uff08\u5982\u679c\u5929\u6c14\u9884\u62a5\u4e0b\u96e8, \u4eba\u4eec\u4f1a\u5e26\u4f1e), \u4e5f\u5f71\u54cd\u5730\u9762\u662f\u5426\u6e7f\u6ed1\uff08\u5982\u679c\u4e0b\u96e8, \u5730\u9762\u4f1a\u6e7f\u6ed1)\u3002</li> <li>\u5982\u679c\u4f60\u4e0d\u77e5\u9053\u5929\u6c14\uff08\u672a\u89c2\u5bdf \\(Y\\) ), \u4f60\u53ef\u80fd\u4f1a\u53d1\u73b0\u5e26\u4f1e\u548c\u5730\u9762\u6e7f\u6ed1\u4e4b\u95f4\u6709\u67d0\u79cd\u5173\u8054\uff08\u56e0\u4e3a\u5b83\u4eec\u90fd\u53d7\u5929\u6c14\u5f71\u54cd)\u3002</li> <li>\u4f46\u5982\u679c\u4f60\u77e5\u9053\u5929\u6c14\u60c5\u51b5 (\u89c2\u5bdf \\(Y\\) ), \u6bd4\u5982\u77e5\u9053\u4eca\u5929\u4e0b\u96e8, \u90a3\u4e48\u5e26\u4f1e\u548c\u5730\u9762\u6e7f\u6ed1\u7684\u5173\u7cfb\u5c31\u53d8\u5f97\u72ec\u7acb\u4e86 (\u77e5\u9053\u4e86\u5929\u6c14, \u4f60\u4e0d\u9700\u8981\u901a\u8fc7\u770b\u5730\u9762\u6765\u5224\u65ad\u662f\u5426\u9700\u8981\u5e26\u4f1e)\u3002</li> </ul>"},{"location":"course_notes/cs188/lecture/BN-Independence/#common-effect","title":"Common Effect","text":"\\[ P(x, y, z)=P(y \\mid x, z) P(x) P(z) \\] <p>In the configuration shown in Figure 5, X and Z are independent: \\(X \\indep Z\\)</p> <p>However, they are not necessarily independent when conditioned on Y.</p> <p>Example:</p> \\[ \\begin{aligned} &amp; P(X=\\text { true })=P(X=\\text { false })=0.5 \\\\ &amp; P(Z=\\text { true })=P(Z=\\text { false })=0.5 \\end{aligned} \\] <p>and \\(Y\\) is determined by whether \\(X\\) and \\(Z\\) have the same value:</p> \\[ P(Y \\mid X, Z)= \\begin{cases}1 &amp; \\text { if } X=Z \\text { and } Y=\\text { true } \\\\ 1 &amp; \\text { if } X \\neq Z \\text { and } Y=\\text { false } \\\\ 0 &amp; \\text { else }\\end{cases} \\] <p>Then X and Z are independent if Y is unobserved. But if Y is observed, then knowing X tells you about Z. So X and Z are not conditionally independent given Y.</p> <p>This same logic applies when conditioning on descendants of Y in the graph. If one of Y\u2019s descendant nodes is observed, as in Figure 7, X and Z are not guaranteed to be independent.</p> <p></p>"},{"location":"course_notes/cs188/lecture/BN-Independence/#general-case-and-d-separation","title":"General Case, and D-Separation","text":"<p>We formulate the problem as follows:</p> <p>Problem</p> <p>Given a Bayes Net \\(G\\), two nodes \\(X\\) and \\(Y\\), and a (possibly empty) set of observed nodes \\(\\left \\{ Z_1, \\dots, Z_k \\right \\}\\), must the following statement be true: \\(X \\indep Y \\mid \\left \\{ Z_1, \\dots, Z_k \\right \\}\\)?</p> <p>D-Separation(Directed Separation): If a set of variables \\(Z_1, \\dots, Z_k\\) d-separates \\(X\\) and \\(Y\\), then \\(X \\indep Y \\mid \\left \\{ Z_1, \\dots, Z_k \\right \\}\\). in all possible distributions that can be encoded by the Bayes Net.</p> <p>D-Separation Algorithm</p> <ol> <li>Shade all observed nodes \\(\\left\\{Z_1, \\ldots, Z_k\\right\\}\\) in the graph.</li> <li>Enumerate all undirected paths from \\(X\\) to \\(Y\\).</li> <li>For each path:<ol> <li>Decompose the path into triples (segments of 3 nodes).</li> <li>If all triples are active, this path is active and d-connects \\(X\\) to \\(Y\\).</li> </ol> </li> <li>If no path d-connects \\(X\\) and \\(Y\\), then \\(X\\) and \\(Y\\) are d-separated, so they are conditionally independent given \\(\\left\\{Z_1, \\ldots, Z_k\\right\\}\\)</li> </ol> <p>Any path in a graph from X to Y can be decomposed into a set of 3 consecutive nodes and 2 edges - each of which is called a triple.</p> <p>A triple is active or inactive depending on whether or not the middle node is observed.</p> <p></p>"},{"location":"course_notes/cs188/lecture/BN-Independence/#examples","title":"Examples","text":""},{"location":"course_notes/cs188/lecture/BN-Inference/","title":"BN: Inference","text":""},{"location":"course_notes/cs188/lecture/BN-Inference/#exact-inference-in-bayes-nets","title":"Exact Inference in Bayes Nets","text":"<p>Inference is the problem of finding the value of some probability distribution \\(P(Q_1 \\ldots Q_m \\mid e_1 \\ldots e_n)\\).</p> <p>Given a Bayes Net, we can solve this problem naively by forming the joint PDF and using Inference by Enumeration.  This requires the creation of and iteration over an exponentially large table.</p>"},{"location":"course_notes/cs188/lecture/BN-Inference/#variable-elimination","title":"Variable Elimination","text":"<p>An alternate approach is to eliminate hidden variables one by one.</p> <p>To eliminate a variable \\(X\\):</p> <ol> <li>Join (multiply together) all factors involving \\(X\\).</li> <li>Sum out (marginalize) \\(X\\).</li> </ol> <p>A factor is defined simply as an unnormalized probability.</p> <p>Variable elimination algorithm for inference in Bayes networks</p> <p></p>"},{"location":"course_notes/cs188/lecture/BN-Inference/#example","title":"Example","text":"<p>Suppose we have a model as shown below, where T , C, S, and E can take on binary values, as shown below. Here, T represents the chance that an adventurer takes a treasure, C represents the chance that a cage falls on the adventurer given that he takes the treasure, S represents the chance that snakes are released if an adventurer takes the treasure, and E represents the chance that the adventurer escapes given information about the status of the cage and snakes.</p> <p></p> <p>In this case, we have the factors \\(P(T), P(C \\mid T), P(S \\mid T)\\), and \\(P(E \\mid C, S)\\). Suppose we want to calculate \\(P(T \\mid+e)\\). The inference by enumeration approach would be to form the 16 row joint \\(\\operatorname{PDF} P(T, C, S, E)\\), select only the rows corresponding to +e, then summing out \\(C\\) and \\(S\\) and finally normalizing.</p> <p>The alternate approach is to eliminate \\(C\\), then \\(S\\), one variable at a time. We'd proceed as follows:</p> <ul> <li>Join (multiply) all the factors involving \\(C\\), forming \\(f_1(C,+e, T, S)=P(C \\mid T) \\cdot P(+e \\mid C, S)\\). Sometimes this is written as \\(P(C,+e \\mid T, S)\\).</li> <li>Sum out \\(C\\) from this new factor, leaving us with a new factor \\(f_2(+e, T, S)\\), sometimes written as \\(P(+e \\mid T, S)\\).</li> <li>Join all factors involving \\(S\\), forming \\(f_3(+e, S, T)=P(S \\mid T) \\cdot f_2(+e, T, S)\\), sometimes written as \\(P(+e, S \\mid T)\\).</li> <li>Sum out \\(S\\), yielding \\(f_4(+e, T)\\), sometimes written as \\(P(+e \\mid T)\\).</li> <li>Join the remaining factors, which gives \\(f_5(+e, T)=f_4(+e, T) \\cdot P(T)\\).</li> </ul> <p>Once we have \\(f_5(+e, T)\\), we can easily compute \\(P(T \\mid+e)\\) by normalizing.</p> <p>An alternate way of looking at the problem is to observe that the calculation of \\(P(T \\mid+e)\\) can either be done through inference by enumeration as follows:</p> \\[ \\alpha \\sum_s \\sum_c P(T) P(s \\mid T) P(c \\mid T) P(+e \\mid c, s) \\] <p>or by Variable elimination as follows:</p> \\[ \\alpha P(T) \\sum_s P(s \\mid T) \\sum_c P(c \\mid T) P(+e \\mid c, s) \\] <p>As a final note on variable elimination, it\u2019s important to observe that it only improves on inference by enumeration if we are able to limit the size of the largest factor to a reasonable value.</p>"},{"location":"course_notes/cs188/lecture/BN-Sampling/","title":"BN: Sampling","text":""},{"location":"course_notes/cs188/lecture/BN-Sampling/#approximate-inference-in-bayes-nets-sampling","title":"Approximate Inference in Bayes Nets: Sampling","text":"<p>IBE or Variable Elimination can be computationally expensive when the Bayes Net is large or when the query is complex. </p> <p>An alternate approach for probabilistic reasoning is to implicitly calculate the probabilities for our query by simply counting samples.</p> <p>This will not yield the exact solution, but this approximate inference is often good enough, especially when taking into account massive savings in computation.</p>"},{"location":"course_notes/cs188/lecture/BN-Sampling/#prior-sampling","title":"Prior Sampling","text":"<pre><code>import random\n\ndef get_t():\n    if random.random() &lt; 0.99:\n        return True\n\n    return False\n\ndef get_c(t):\n    if t and random.random() &lt; 0.95:\n        return True\n\n    return False\n\ndef get_sample():\n    t = get_t()\n    c = get_c(t)\n    return [t, c]\n</code></pre> <p>We call this simple approach prior sampling. </p> <p>The downside of this approach is that it may require the generation of a very large number of samples in order to perform analysis of unlikely scenarios.  If we wanted to compute \\(P(C \\mid-t)\\), we'd have to throw away \\(99 \\%\\) of our samples.</p>"},{"location":"course_notes/cs188/lecture/BN-Sampling/#rejection-sampling","title":"Rejection Sampling","text":"<p>One way to mitigate the previously stated problem is to modify our procedure to early reject any sample inconsistent with our evidence.  We call this approach rejection sampling.</p> <p>These two approaches work for the same reason: any valid sample occurs with the same probability as specified in the joint PDF.</p>"},{"location":"course_notes/cs188/lecture/BN-Sampling/#likelihood-weighting","title":"Likelihood Weighting","text":"<p>A more exotic approach is likelihood weighting.</p> <p>We manually set the evidence variables to their observed values and then sample the remaining variables.</p> <p>The problem here is that this may yield samples that are inconsistent with the correct distribution.</p> <p>Likelihood weighting solves this issue by using a weight for each sample, which is the probability of the evidence given the sample.</p> <p>To do this, we iterate through each variable in Bayes Net, sampling a value if the value is not an evidence variable, or changing the weight for the sample if the variable is evidence.</p> <p></p>"},{"location":"course_notes/cs188/lecture/BN-Sampling/#gibbs-sampling","title":"Gibbs Sampling","text":"<p>Note</p> <p>Gibbs Sampling is a fourth approach for sampling. In this approach, we first set all variables to some totally random values. </p> <p>We then repeatedly pick one variable at a time, clear its value, and resample it given the values currently assigned to other variables.</p> <p>If we repeat this process enough times, our later samples will eventually converge to the correct distribution even though we may start from a low-probability assignment of values.</p> <p></p>"},{"location":"course_notes/cs188/lecture/BN-Sampling/#conclusion","title":"Conclusion","text":"<p>To summarize, Bayesian Networks is a powerful representation of joint probability distributions. Its topological structure encodes independence and conditional independence relationships, and we can use it to model arbitrary distributions to perform inference and sampling.</p> <p>We covered two approaches to probabilistic inference: exact inference and probabilistic inference (sampling).</p> <p>In exact inference, we are guaranteed the exact correct probability, but the amount of computation may be prohibitive:</p> <ul> <li>Inference By Enumeration</li> <li>Variable Elimination</li> </ul> <p>We can turn to sampling to approximate solutions while using less computation:</p> <ul> <li>Prior Sampling</li> <li>Rejection Sampling</li> <li>Likelihood Weighting</li> <li>Gibbs Sampling</li> </ul>"},{"location":"course_notes/cs188/lecture/BN-representation/","title":"BN: Representation","text":"<p>\u53ea\u770bnote\u7684\u8bdd\u592a\u96be\u61c2\u4e86\u3002\u3002\u3002\u3002\u4e8e\u662f\u770b\u4e86Lecture</p> <p>Fall 2018\u7684\u8001\u5e08\u8bb2\u5f97\u4e0d\u9519\uff0c\u53ef\u4ee5\u591a\u542c\u542c</p> <p>Lecture\u4e00\u5f00\u59cb\u6ca1\u6709\u76f4\u63a5\u8bb2BN\uff0c\u800c\u662f\u5148\u94fa\u57ab\u4e00\u4e9b\u6982\u7387\u8bba</p>"},{"location":"course_notes/cs188/lecture/BN-representation/#conditional-independence","title":"Conditional Independence","text":"<p>\u8fd9\u91cc\u4e3e\u4e86\u7259\u75db\u7684\u4f8b\u5b50\uff0c\u6709\u4e09\u4e2a\u53d8\u91cf\uff1aTootheache, Cavity, Catch.</p> <p>\u5982\u679c\u6211\u4eec\u77e5\u9053\u4e86Cavity\uff0c\u90a3\u4e48Tootheache\u548cCatch\u5c31\u662f\u6761\u4ef6\u72ec\u7acb\u7684\u3002</p> <ul> <li>X is conditionally independent of Y given Z: \\(X \\newcommand{\\indep}{\\perp \\!\\!\\! \\perp} \\indep Y \\mid Z\\)</li> </ul> <p>if and only if:</p> \\[ \\forall x, y, z: P(x, y \\mid z)=P(x \\mid z) P(y \\mid z) \\] <p>or, equivalently, if and only if</p> \\[ \\forall x, y, z: P(x \\mid z, y)=P(x \\mid z) \\] <p>\u603b\u4e4b\uff0cunconditional (absolute) independence is very rare.</p> <p>Conditional independence is our most basic and robust form of knowledge about uncertain environments.</p>"},{"location":"course_notes/cs188/lecture/BN-representation/#bayesian-network-representation","title":"Bayesian Network Representation","text":"<p>Representing an entire joint distribution in the memory of a computer is impractical for real problems</p> <p>If we have \\(n\\) variables, each of which can take on \\(k\\) values, then we need to store \\(k^n\\) numbers - Impractical to store and manipulate.</p> <p>Bayes nets avoid this issue by taking advantage of the idea of conditional probability.</p> <p>We formally define a Bayes Net as consisting of:</p> <ul> <li>A directed acyclic graph of nodes, one per variable \\(X\\).</li> <li>A conditional distribution for each node \\(P(X \\mid A_1 \\dots A_n)\\), where \\(A_i\\) is the \\(i^{th}\\) parent of \\(X\\),    stored as a conditional probability table.</li> </ul> <p>It's important to remember that edges between Bayes Net nodes do not mean there is specifically a causal relationship between those nodes. It just means that there may be some relationship between the nodes.</p> <p>Bayes Nets are only a type of model - with good modeling choices they can still be good enough approximations that they are useful for solving real-world problems.</p>"},{"location":"course_notes/cs188/lecture/BN-representation/#structure-of-bayes-nets","title":"Structure of Bayes Nets","text":"<p>Two rules for Bayes Net independences:</p> <ul> <li>Each node is conditionally independent of all its ancestor nodes in the graph, given all of its parents.</li> <li>Each node is conditionally independent of all other variables given its Markov blanket. A variable's Markov blanket consists of its parents, children, and children's other parents.</li> </ul>"},{"location":"course_notes/cs188/lecture/CSPs/","title":"Constraint Satisfaction Problems","text":"<p>CSPs are a type of identification problem, problems in which we must simply identify whether a state is a goal state or not, with no regard to how we arrive at that goal.</p> <p>CSPs are defined by three factors:</p> <ol> <li>Variables: \\(X_1, X_2, \\ldots, X_n\\) </li> <li>Domains: A set of \\(\\{x_1, x_2, \\ldots, x_d\\}\\) representing all possible values      that a CSP variable can take on.</li> <li>Constraints: Defines restrictions on the values of variables.</li> </ol> <p>Constraints satisfaction problems are NP-Hard.</p> <p>We can often get around this by formulating CSPs as search problems, defining states as partial assignments.</p>"},{"location":"course_notes/cs188/lecture/CSPs/#constraint-graphs","title":"Constraint Graphs","text":"<p>Another CSP example: map coloring.</p> <p>Color a map such that no two adjacent regions have the same color.</p> <p></p> <p>Constraint satisfaction problems are often represented as constraint graphs, where nodes represent variables and edges represent constraints between them.</p> <ul> <li>Unary constraints: involve a single variable in the CSP.</li> <li>Binary constraints: involve two variables in the CSP.</li> <li>Higher-order constraints: involve more than two variables.</li> </ul> <p>The value of constraint graphs is that we can use them to extract valuable information about the structure of the CSPs we are solving.</p>"},{"location":"course_notes/cs188/lecture/CSPs/#solving-constraint-satisfaction-problems","title":"Solving Constraint Satisfaction Problems","text":"<p>Backtracking search, an optimization on depth-first search, is used specifically for the problem of constraint satisfaction, with improvements coming from two main principles:</p> <ol> <li>Fix an ordering for variables, and select values for variables in this order.</li> <li>When selecting values for a variable, only select values that     don't conflict with any previously assigned variables.</li> </ol> <p></p> <p>Though backtracking search is a vast improvement over the brute-forcing of depth first search, we can get more gains in speed still with further improvements through filtering, variable/value ordering, and structural explotation.</p>"},{"location":"course_notes/cs188/lecture/CSPs/#filtering","title":"Filtering","text":"<p>Checks if we can prune the domains of unassigned variables ahead of time by removing values we know will result in backtracking.</p>"},{"location":"course_notes/cs188/lecture/CSPs/#naive-approach-forward-checking","title":"Naive approach: Forward Checking","text":"<p>Whenever a value is assigned to a variable \\(X_i\\), prunes the domains of unassigned variables that share a constraint with \\(X_i\\) that would violate the constraint if assigned.</p> <p>The idea of forward checking can be generalized to arc consistency.</p>"},{"location":"course_notes/cs188/lecture/CSPs/#arc-consistency","title":"Arc Consistency","text":"<p>Info</p> <p>For arc consistency, we interpret each undirected edge of the constraint graph for a CSP as two directed edges pointing to opposite directions. Each of these directed edges is called an arc.</p> <p>Arc consistency\u5173\u6ce8\u7684\u662f\u4e8c\u5143\u7ea6\u675f\uff0c\u4e5f\u5c31\u662f\u6d89\u53ca\u4e24\u4e2a\u53d8\u91cf\u7684\u7ea6\u675f\u3002\u5bf9\u4e8e\u4e00\u4e2aarc\uff0c \u5728tail\u7684domain\u4e2d\u4efb\u610f\u4e00\u4e2a\u503c\u90fd\u80fd\u627e\u5230\u4e00\u4e2ahead\u7684\u503c\uff0c\u4f7f\u5f97\u8fd9\u4e24\u4e2a\u503c\u6ee1\u8db3\u7ea6\u675f\uff0c\u90a3\u4e48\u8fd9\u4e2aarc\u5c31\u662fconsistent\u7684\u3002 \u5bf9\u4e8e\u4e00\u4e2aCSP\uff0c\u5982\u679c\u6240\u6709\u7684arc\u90fd\u662fconsistent\u7684\uff0c\u90a3\u4e48\u8fd9\u4e2aCSP\u5c31\u662farc consistent\u7684\u3002</p> <p>Takeaway</p> <ul> <li>If X loses a value, neighbors of X need to be rechecked.</li> <li>Arc consistency detects failure earlier than forward checking.</li> <li>Can be run as a preprocessor or after each assignment.</li> <li>Remember: delete from the tail!</li> </ul> <p>The arc consistency algorithm is as follows:</p> <ul> <li>Begin by storing all arcs in the constraint graph for the CSP in a queue \\(Q\\).</li> <li>Iteratively remove arcs from \\(Q\\) and enforce the condition that in each removed arc     \\(X_{i} \\longrightarrow X_{j}\\), for every remaining value \\(v\\) for the tail variable     \\(X_i\\), there is at least one remaining value \\(w\\) for the head variable \\(X_j\\) such that     the assignment \\(X_i = v\\) and \\(X_j = w\\) does not violate any constrains.     If some value \\(v\\) for \\(X_i\\) would not work with any of the remaining values for \\(X_j\\),     we remove \\(v\\) from the set of possible values for \\(X_i\\).</li> <li>If at least one value is removed for \\(X_i\\) when enforcing arc consistency for an arc     \\(X_i \\longrightarrow X_j\\), add arcs of the form \\(X_k \\longrightarrow X_i\\) to \\(Q\\),     for all unassigned variables \\(X_k\\).</li> <li>Continue until \\(Q\\) is empty, or the domain of some variables is empty and      triggers a backtrack.</li> </ul> <p>Arc consistency is typically implemented with the AC-3 algorithm:</p> <p></p>"},{"location":"course_notes/cs188/lecture/CSPs/#ordering","title":"Ordering","text":"<p>It\u2019s often much more effective to compute the next variable and corresponding value \"on the fly\" with two broad principles, minimum remaining values and least constraining value.</p> <ul> <li>Minimum Remaining Values (MRV) - When selecting which variable to assign next, using an MRV policy chooses whichever unassigned variable has the fewest valid remaining values (the most constrained variable).<ul> <li>Fail-fast ordering: \u4f60\u603b\u8981\u8d4b\u503c\u7ed9\u6240\u6709\u7684\u53d8\u91cf\uff0c\u90a3\u4e48\u73b0\u5728\u6709\u4e00\u4e9b\u53d8\u91cf\u6709\u5f88\u591aremaining values(\u6613\u5904\u7406)\uff0c\u6709\u4e00\u4e9b\u53d8\u91cf\u53ea\u6709\u5f88\u5c11\u7684remaining values(\u96be\u5904\u7406)\uff0c\u90a3\u4e48\u4f60\u5c31\u5e94\u8be5\u5148\u5904\u7406\u90a3\u4e9b\u96be\u641e\u7684\u53d8\u91cf\uff0c\u8fd9\u6837\u5982\u679c\u6709\u95ee\u9898\uff0c\u4f60\u5c31\u80fd\u65e9\u70b9\u53d1\u73b0\uff0c\u65e9\u70b9backtrack\uff0c\u800c\u4e0d\u662f\u6d6a\u8d39\u65f6\u95f4\u5728\u641eeasy stuff\uff0c\u6700\u540e\u624d\u53d1\u73b0\u6709\u95ee\u9898\u3002     </li> </ul> </li> <li>Least Constraining Value (LCV) - Similarly, when selecting which value to assign next, a good policy to implement is to select the value that prunes the fewest values from the domains of the remaining unassigned values.<ul> <li>\u8ddf\u9009\u62e9variable\u7684\u65f6\u5019\u4e0d\u540c\uff0c\u4f60\u5e76\u4e0d\u9700\u8981\u628a\u6240\u6709\u7684value\u90fd\u8bd5\u4e00\u904d\uff0c\u6240\u4ee5\u4f60\u53ef\u4ee5\u5148\u5c1d\u8bd5\u90a3\u4e9bprune\u6700\u5c11\u7684value\uff0c\u8fd9\u6837\u4f60\u7ed9\u5176\u4ed6variable\u7559\u4e0b\u7684domain\u5c31\u4f1a\u66f4\u5927\uff0c\u66f4\u6709\u53ef\u80fd\u627e\u5230\u4e00\u4e2asolution\u3002</li> </ul> </li> </ul>"},{"location":"course_notes/cs188/lecture/CSPs/#structure","title":"Structure","text":"<p>A final class of improvements to solving constraint satisfaction problems are those that exploit their structure.</p>"},{"location":"course_notes/cs188/lecture/CSPs/#tree-structured-csps","title":"Tree-Structured CSPs","text":"<p>We can solve tree-structured CSPs (one that has no loops in its constraint graph) in \\(O(n \\cdot d^2)\\) time, using the tree-structured CSP algorithm:</p> <ul> <li>Pick an arbitrary node in the constraint graph for the CSP to serve as the root of the tree.</li> <li>Convert all undirected edges in the tree to directed edges that point away from the root. Then linearize (or topologically sort) the resulting directed acyclic graph.<ul> <li>\u7b80\u5355\u6765\u8bf4\u5c31\u662f\u628atree\u8f6c\u5316\u6210\u4e00\u4e2a\u6709\u5411\u56fe\uff0c\u7136\u540e\u628anode\u6392\u5e8f\uff0c\u8ba9\u6bcf\u4e00\u6761edge\u90fd\u662f\u4ece\u5de6\u5f80\u53f3\u6307\u7684\u3002</li> </ul> </li> </ul> <p></p> <ul> <li>Perform a backwards pass of arc consistency.<ul> <li>\u4ece\u53f3\u5f80\u5de6\u904d\u5386\uff0cenforce arc consistency for all arcs \\(Parent(X_i) \\longrightarrow X_i\\).</li> <li>\u8fd9\u53ef\u80fd\u4f1aprune\u6389\u4e00\u4e9bvalue\u3002</li> </ul> </li> </ul> <p></p> <ul> <li>Finally, perform a forwards assignment.<ul> <li>Starting from \\(X_1\\) and going to \\(X_n\\) (\u4ece\u5de6\u5f80\u53f3), assign each \\(X_i\\) a value consistent with that of its parents.</li> <li>Because we\u2019ve enforced arc consistency on all of these arcs, this iterative assignment guarantees a correct solution.</li> </ul> </li> </ul>"},{"location":"course_notes/cs188/lecture/CSPs/#cutset-conditioning","title":"Cutset Conditioning","text":"<p>\u6709\u4e9bCSP\u7684constraint graph\u4e0d\u662ftree-structured\u7684\uff0c\u4f46\u662f\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7cutset conditioning\u6765\u628a\u5b83\u53d8\u6210tree-structured\u7684\uff0c\u7136\u540e\u518d\u7528tree-structured CSP algorithm\u6765\u89e3\u51b3\u3002</p> <p></p> <ul> <li>First finding the smallest subset of variables in a constraint graph such that their removal results in a tree (such a subset is known as a cutset for the graph).</li> <li>Once the smallest cutset is found, we assign all variables in it and prune the domains of all neighboring nodes.</li> <li>What\u2019s left is a tree-structured CSP, upon which we can solve with the tree-structured CSP algorithm.</li> </ul>"},{"location":"course_notes/cs188/lecture/CSPs/#local-search","title":"Local Search","text":"<p>Backtracking search isn't the only algorithm for solving constraint satisfaction problems. Another widely used algorithm is local search.</p> <p>Idea</p> <p>Iterative improvement - start with some random assignment to values then iteratively select a random conflicted variable and reassign its value to the one that violates the fewest constraints until no more constraint violations exist (a policy known as the min-conflicts heuristic).</p> <p>Local search is both incomplete and suboptimal, there is a critical ratio around which using local search becomes extremely expensive:</p> \\[ R = \\frac{number \\ \\ of \\ \\ constraints}{number \\ \\ of \\ \\ variables} \\] <p></p> <p>We wish to find the state that corresponds to the highest objective value.</p> <p></p> <p>The basic idea of local search algorithms is that from each state they locally move towards states that have a higher objective value until a maximum (hopefully the global) is reached.</p>"},{"location":"course_notes/cs188/lecture/CSPs/#hill-climbing-search","title":"Hill-Climbing Search","text":"<ul> <li>Moves to better states but can get trapped at local maxima or plateaus.</li> <li>Variants like stochastic hill-climbing improve results at the cost of more iterations.</li> </ul>"},{"location":"course_notes/cs188/lecture/CSPs/#simulated-annealing-search","title":"Simulated Annealing Search","text":"<ul> <li>Aims to combine random walk (randomly moves to nearby states) and hill-climbing to obtain a complete and efficient search algorithm.</li> </ul>"},{"location":"course_notes/cs188/lecture/CSPs/#genetic-algorithms","title":"Genetic Algorithms","text":""},{"location":"course_notes/cs188/lecture/CSPs/#summary","title":"Summary","text":"<p>Constraint satisfaction problems in general don't have an efficient algorithm which solves them in polynomial time.</p> <p>However, we can often find solutions in an acceptable amount of time:</p> <ul> <li>Filtering - Filtering handles pruning the domains of unassigned variables ahead of time to prevent unnecessary backtracking. The two important filtering techniques we\u2019ve covered are forward checking and arc consistency.</li> <li>Ordering - Ordering handles selection of which variable or value to assign next to make backtracking as unlikely as possible. For variable selection, we learned about a MRV policy and for value selection we learned about a LCV policy.</li> <li>Structure - If a CSP is tree-structured or close to tree-structured, we can run the tree-structured CSP algorithm on it to derive a solution in linear time. Similarly, if a CSP is close to tree structured, we can use cutset conditioning to transform the CSP into one or more independent tree-structured CSPs and solve each of these separately.</li> </ul>"},{"location":"course_notes/cs188/lecture/Decision%20Networks%20%26%20VPI/","title":"Decision Networks &amp; VPI","text":""},{"location":"course_notes/cs188/lecture/Decision%20Networks%20%26%20VPI/#utility","title":"Utility","text":""},{"location":"course_notes/cs188/lecture/Decision%20Networks%20%26%20VPI/#the-mathematical-language-of-preferences","title":"The mathematical language of preferences","text":"<ul> <li>If an agent prefers receiving a prize A to receiving a prize B, this is written \\(A \\succ B\\).</li> <li>If an agent is indifferent between receiving A or B, this is written as \\(A \\sim B\\).</li> <li>A lottery is a situation with different prizes resulting with different probabilities.</li> </ul> \\[ L=[p, A ;(1-p), B] \\] <p>In order for a set of preferences to be rational, they must follow the five Axioms of Rationality:</p>"},{"location":"course_notes/cs188/lecture/Decision%20Networks%20%26%20VPI/#five-axioms-of-rationality","title":"Five Axioms of Rationality","text":"<ul> <li> <p>Orderability: \\((A \\succ B) \\vee(B \\succ A) \\vee(A \\sim B)\\)</p> <p>A rational agent must either prefer one of A or B, or be indifferent between the two.</p> </li> <li> <p>Transitivity: \\((A \\succ B) \\wedge(B \\succ C) \\Rightarrow(A \\succ C)\\)</p> </li> <li> <p>Continuity: \\(A \\succ B \\succ C \\Rightarrow \\exists p \\ [p, A ;(1-p), C] \\sim B\\)</p> <p>If a rational agent prefers \\(A\\) to \\(B\\) but \\(B\\) to \\(C\\), then it's possible to construct a lottery \\(L\\) between \\(A\\) and \\(C\\) such that the agent is indifferent between \\(L\\) and \\(B\\) with appropriate selection of \\(p\\).</p> </li> <li> <p>Substitutability: \\(A \\sim B \\Rightarrow[p, A ;(1-p), C] \\sim[p, B ;(1-p), C]\\)</p> </li> <li> <p>Monotonicity: \\(A \\succ B \\Rightarrow(p \\geq q) \\Leftrightarrow[p, A ;(1-p), B] \\succeq[q, A ;(1-q), B]\\)</p> <p>If a rational agent prefers \\(A\\) over \\(B\\), then given a choice between lotteries involving only \\(A\\) and \\(B\\), the agent prefers the lottery assigning the highest probability to \\(A\\).</p> </li> </ul> <p>If all five axioms are satisfied by an agent, then it\u2019s guaranteed that the agent\u2019s behavior is describable as a maximization of expected utility.</p> \\[ \\begin{aligned} U(A) \\geq U(B) &amp; \\Leftrightarrow A \\succeq B \\\\ U\\left(\\left[p_1, S_1 ; \\ldots ; p_n, S_n\\right]\\right) &amp; =\\sum_i p_i U\\left(S_i\\right) \\end{aligned} \\]"},{"location":"course_notes/cs188/lecture/Decision%20Networks%20%26%20VPI/#example","title":"Example","text":"<p>Consider the following lottery:</p> \\[ L=[0.5, \\$ 0 ; 0.5, \\$ 1000] \\] <p>This represents a lottery where you receive \\(\\$ 1000\\) with probability 0.5 and \\(\\$ 0\\) with probability 0.5 . </p> <p>Now consider three agents \\(A_1, A_2\\), and \\(A_3\\) which have utility functions \\(U_1(\\$ x)=x, U_2(\\$ x)=\\sqrt{x}\\), and \\(U_3(\\$ x)=x^2\\) respectively. If each of the three agents were faced with a choice between participating in the lottery and receiving a flat payment of \\(\\$ 500\\), which would they choose? </p> <p>The respective utilities for each agent of participating in the lottery and accepting the flat payment are listed in the following table:</p> Agent Lottery Flat Payment 1 500 500 2 15.81 22.36 3 500000 250000 <ul> <li>Agent \\(A_1\\) is indifferent between participating in the lottery and receiving the flat payment (the utilities for both cases are identical). Such an agent is known as risk-neutral. </li> <li>\\(A_2\\) prefers the flat payment to the lottery and is risk-averse.</li> <li>\\(A_3\\) prefers the lottery to the flat payment and is risk-seeking.</li> </ul>"},{"location":"course_notes/cs188/lecture/Decision%20Networks%20%26%20VPI/#decision-networks","title":"Decision Networks","text":"<p>Note</p> <p>Now we\u2019ll discuss a combination of both Bayes\u2019 nets and expectimax known as a decision network that we can use to model the effect of various actions on utilities based on an overarching graphical probabilistic model.</p> <ul> <li>Chance nodes: Each outcome in a chance node has an associated probability, which can be determined by running inference on the underlying Bayes\u2019 net it belongs to.</li> <li>Action nodes: Representing a choice between any of a number of actions which we have the power to choose from.</li> <li>Utility nodes: Output a utility based on the values taken on by their parents.</li> </ul> <p>Our goal with decision networks is again to select the action which yields the maximum expected utility (MEU):</p> <ul> <li>Start by instantiating all evidence that\u2019s known, and run inference to calculate the posterior probabilities.</li> <li>Go through each possible action and compute the expected utility of taking that action given the posterior probabilities computed in the previous step.     $$     E U(a \\mid e)=\\sum_{x_1, \\ldots, x_n} P\\left(x_1, \\ldots, x_n \\mid e\\right) U\\left(a, x_1, \\ldots, x_n\\right)     $$</li> <li>Select the action which yielded the highest utility to get the MEU.</li> </ul>"},{"location":"course_notes/cs188/lecture/Decision%20Networks%20%26%20VPI/#example_1","title":"Example","text":"\\[ \\begin{aligned} E U(\\text { leave } \\mid \\text { bad }) &amp; =\\sum_w P(w \\mid \\text { bad }) U(\\text { leave }, w) \\\\ &amp; =0.34 \\cdot 100+0.66 \\cdot 0=34 \\\\ E U(\\text { take } \\mid \\mathrm{bad}) &amp; =\\sum_w P(w \\mid \\text { bad }) U(\\text { take }, w) \\\\ &amp; =0.34 \\cdot 20+0.66 \\cdot 70=53 \\end{aligned} \\] <p>All that's left to do is take the maximum over these computed utilities to determine the MEU: $$ \\operatorname{MEU}(F=\\mathrm{bad})=\\max _a E U(a \\mid \\mathrm{bad})=53 $$</p> <p>The action that yields the maximum expected utility is take.</p>"},{"location":"course_notes/cs188/lecture/Decision%20Networks%20%26%20VPI/#outcome-trees","title":"Outcome Trees","text":""},{"location":"course_notes/cs188/lecture/Decision%20Networks%20%26%20VPI/#the-value-of-perfect-information","title":"The Value of Perfect Information","text":"<p>Tip</p> <p>One of the most important parts of decision making is knowing whether or not it\u2019s worth gathering more evidence to help decide which action to take. </p> <p>The Value of Perfect Information (VPI)</p> <p>Mathematically quantifies the amount of an agent's maximum expected utility is  expected to increase if it observes some new evidence.</p>"},{"location":"course_notes/cs188/lecture/Decision%20Networks%20%26%20VPI/#general-formula","title":"General Formula","text":"<p>We know our current maximum utility given our current evidence \\(e\\):</p> \\[ \\operatorname{MEU}(e)=\\max _a \\sum_s P(s \\mid e) U(s, a) \\] <p>Additionally, we know that if we observed some new evidence \\(e^{\\prime}\\) before acting, the MEU of our action at that point would become</p> \\[ \\operatorname{MEU}\\left(e, e^{\\prime}\\right)=\\max _a \\sum_s P\\left(s \\mid e, e^{\\prime}\\right) U(s, a) \\] <p>However, note that we don't know what new evidence we'll get. For example, if we didn't know the weather forecast beforehand and chose to observe it, the forecast we observe might be either good or bad. Because we don't know what what new evidence \\(e^{\\prime}\\) we'll get, we must represent it as a random variable \\(E^{\\prime}\\). How do we represent the new MEU we'll get if we choose to observe a new variable if we don't know what the evidence gained from observation will tell us? The answer is to compute the expected value of the maximum expected utility which, while being a mouthful, is the natural way to go:</p> \\[ \\operatorname{MEU}\\left(e, E^{\\prime}\\right)=\\sum_{e^{\\prime}} P\\left(e^{\\prime} \\mid e\\right) M E U\\left(e, e^{\\prime}\\right) \\] <p>Returning to our definition for VPI, we want to find the amount our MEU is expected to increase if we choose to observe the new evidence \\(E^{\\prime}\\):</p> \\[ V P I(E^{\\prime} \\mid e)= \\operatorname{MEU}(e, E^{\\prime})-\\operatorname{MEU}(e) \\]"},{"location":"course_notes/cs188/lecture/Decision%20Networks%20%26%20VPI/#properties-of-vpi","title":"Properties of VPI","text":"<ul> <li>Nonnegativity: \\(\\forall E^{\\prime}, e \\quad V P I(E^{\\prime} \\mid e) \\geq 0\\)</li> <li>Nonadditivity: \\(V P I\\left(E_j, E_k \\mid e\\right) \\neq V P I\\left(E_j \\mid e\\right)+V P I\\left(E_k \\mid e\\right)\\)     Generally  observing some new evidence \\(E_j\\) might change how much we care about     \\(E_k\\); therefore we can't simply add</li> <li>Order-independence: \\(V P I\\left(E_j, E_k \\mid e\\right)=V P I\\left(E_j \\mid e\\right)+V P I\\left(E_k \\mid e, E_j\\right)=V P I\\left(E_k \\mid e\\right)+V P I\\left(E_j \\mid e, E_k\\right)\\)     Observing multiple new evidences yields the same gain in maximum expected utility regardless of the order of observation.</li> </ul>"},{"location":"course_notes/cs188/lecture/Hidden%20Markov%20Models/","title":"Hidden Markov Models","text":""},{"location":"course_notes/cs188/lecture/Hidden%20Markov%20Models/#markov-models","title":"Markov Models","text":"<p>Markov model can be thought of as analogous to a chain-like, infinite-length Bayes' net.</p> <p>Our weather model will be time-dependent (as are Markov models in general)</p> <p></p> <p>To track how our quantity under consideration changes over time, we need to know the following:</p> <ul> <li>Initial Distribution: The probability of the quantity at time 0.</li> <li>Transition Model: The probability of moving from one state to another between time steps.</li> </ul> <p>The weather at time \\(t = i + 1\\) satisfies the Markov Property or  memoryless property, and is independent of the weather at all other  timesteps besides \\(t = i\\).</p> <p>Markov models make the following assumptions:</p> \\[ W_{i+1} \\newcommand{\\indep}{\\perp \\!\\!\\! \\perp} \\indep \\left\\{W_0, \\ldots, W_{i-1}\\right\\} \\mid W_i \\] <p>This allows us to reconstruct the joint distribution:</p> \\[ P\\left(W_0, W_1, \\ldots, W_n\\right)=P\\left(W_0\\right) P\\left(W_1 \\mid W_0\\right) P\\left(W_2 \\mid W_1\\right) \\ldots P\\left(W_n \\mid W_{n-1}\\right)=P\\left(W_0\\right) \\prod_{i=0}^{n-1} P\\left(W_{i+1} \\mid W_i\\right) \\] <p>A final assumption is that the transition model is stationary, meaning that for all values of \\(i\\) (all time steps): \\(P\\left(W_{i+1} \\mid W_i\\right)\\) is identical.</p>"},{"location":"course_notes/cs188/lecture/Hidden%20Markov%20Models/#the-mini-forward-algorithm","title":"The Mini-Forward Algorithm","text":"<p>By properties of marginalization, we know that</p> \\[ P\\left(W_{i+1}\\right)=\\sum_{w_i} P\\left(w_i, W_{i+1}\\right) \\] <p>By the chain rule we can re-express this as follows:</p> \\[ P\\left(W_{i+1}\\right)=\\sum_{w_i} P\\left(W_{i+1} \\mid w_i\\right) P\\left(w_i\\right) \\] <p>With this equation, we can iteratively compute the distribution of the weather at any time step by starting with the initial distribution \\(P\\left(W_0\\right)\\) and using it to compute \\(P\\left(W_1\\right)\\), and so on.</p>"},{"location":"course_notes/cs188/lecture/Hidden%20Markov%20Models/#stationary-distribution","title":"Stationary Distribution","text":"<p>Question</p> <p>Does the probability of being in a state at a given timestep ever converge?</p> <p>To solve the problem above, we must compute the stationary distribution of the Markov model. </p> \\[ P\\left(W_{t+1}\\right)=P\\left(W_t\\right)=\\sum_{w_t} P\\left(W_{t+1} \\mid w_t\\right) P\\left(w_t\\right) \\]"},{"location":"course_notes/cs188/lecture/Hidden%20Markov%20Models/#hidden-markov-models_1","title":"Hidden Markov Models","text":"<p>Hidden Markov Models allows us to observe some evidence at each time step, which can potentially affect the belief distribution at each of the states.</p> <p></p> <p>Unlike vanilla Markov models, we now have two different types of nodes:</p> <ul> <li>\\(W_i\\): state variable.</li> <li>\\(F_i\\): evidence variable.</li> </ul> \\[ \\begin{array}{lll} F_1 &amp; \\indep W_0 \\mid W_1 &amp; \\\\ \\forall i &amp; =2, \\ldots, n ; &amp; W_i \\indep \\left\\{W_0, \\ldots, W_{i-2}, F_1, \\ldots, F_{i-1}\\right\\} \\mid W_{i-1} \\\\ \\forall i &amp; =2, \\ldots, n ; &amp; F_i \\indep \\left\\{W_0, \\ldots, W_{i-1}, F_1, \\ldots, F_{i-1}\\right\\} \\mid W_i \\end{array} \\] <p>Hidden Markov Models make the following assumptions:</p> <ul> <li>The transition model \\(P\\left(W_{i+1} \\mid W_i\\right)\\) is stationary.</li> <li>The sensor model \\(P\\left(F_i \\mid W_i\\right)\\) is also stationary.</li> </ul>"},{"location":"course_notes/cs188/lecture/Hidden%20Markov%20Models/#believe-distribution","title":"Believe Distribution","text":"<p>All evidence \\(F_1, \\ldots, F_i\\) is observed up to date:</p> \\[ B\\left(W_i\\right)=P\\left(W_i \\mid f_1, \\ldots, f_i\\right) \\] <p>Evidence \\(f_1, \\ldots, f_{i-1}\\) is observed:</p> \\[ B'\\left(W_i\\right)=P\\left(W_i \\mid f_1, \\ldots, f_{i-1}\\right) \\] <p>Defining \\(e_i\\) as evidence observed at timestep \\(i\\), you might see the aggregated evidence from timesteps \\(1 \\leq i \\leq t\\) reexpressed in the form:</p> \\[ e_{1: t}=e_1, \\ldots, e_t \\]"},{"location":"course_notes/cs188/lecture/Hidden%20Markov%20Models/#the-forward-algorithm","title":"The Forward Algorithm","text":"<p>Relationship between \\(B\\left(W_i\\right)\\) and \\(B'\\left(W_{i + 1}\\right)\\):</p> \\[ B^{\\prime}\\left(W_{i+1}\\right)=\\sum_{w_i} P\\left(W_{i+1} \\mid w_i\\right) B\\left(w_i\\right) \\] <p>Relationship between \\(B'\\left(W_{i+1}\\right)\\) and \\(B\\left(W_{i+1}\\right)\\):</p> \\[ B\\left(W_{i+1}\\right) \\propto P\\left(f_{i+1} \\mid W_{i+1}\\right) B^{\\prime}\\left(W_{i+1}\\right) \\] <p>Combining the two relationships we've just derived yields an iterative algorithm known as the forward algorithm, the Hidden Markov Model analog of the mini-forward algorithm from earlier:</p> \\[ B\\left(W_{i+1}\\right) \\propto P\\left(f_{i+1} \\mid W_{i+1}\\right) \\sum_{w_i} P\\left(W_{i+1} \\mid w_i\\right) B\\left(w_i\\right) \\] <p>The forward algorithm can be thought of as consisting of two distinctive steps:</p> <ol> <li>Time elapse update: determining \\(B'\\left(W_{i+1}\\right)\\) from \\(B\\left(W_i\\right)\\).</li> <li>Observation update: determining \\(B\\left(W_{i+1}\\right)\\) from \\(B'\\left(W_{i+1}\\right)\\).</li> </ol>"},{"location":"course_notes/cs188/lecture/Hidden%20Markov%20Models/#viterbi-algorithm","title":"Viterbi Algorithm","text":"<p>\\(\\arg \\max _{x_{1: N}} P\\left(x_{1: N} \\mid e_{1: N}\\right)\\)</p> <p>What is the most likely sequence of hidden states the system followed given the observed evidence variables so far?</p>"},{"location":"course_notes/cs188/lecture/Informed-Search/","title":"Informed Search","text":"<p>Tip</p> <p>If we have some notion of the direction in which we should focus our search, we can significantly improve performance and \"hone in\" on a goal much more quickly. This is the exactly the focus of informed search.</p>"},{"location":"course_notes/cs188/lecture/Informed-Search/#heuristics","title":"Heuristics","text":"<p>Heuristics are the driving force that allow estimation of distance to goal states - they're functions that take in a state as input and output a corresponding estimate.</p>"},{"location":"course_notes/cs188/lecture/Informed-Search/#greedy-search","title":"Greedy Search","text":"<ul> <li>Description: Always selects the frontier node with the   lowest heuristic value for expansion, which corresponds   to the state it believes is nearest to the goal.</li> <li>Frontier Representation: priority queue</li> <li>Completeness and Optimality: Greedy search is neither   complete nor optimal.</li> </ul>"},{"location":"course_notes/cs188/lecture/Informed-Search/#a-search","title":"A* Search","text":"<ul> <li>Description: Always selects the frontier node with the   lowest estimated total cost for expansion<ul> <li>A* combines the total backward cost (sum of edge   weights in the path to the state) used by UCS   with the estimated forward cost (heuristic value)   used by greedy search by summing them together,   effectively yielding an \\(estimated \\ \\ total \\ \\ cost\\)   from start to goal.</li> </ul> </li> <li>Frontier Representation: priority queue</li> <li>Completeness and Optimality: A* is both complete and   optimal given an appropriate heuristic.</li> </ul>"},{"location":"course_notes/cs188/lecture/Informed-Search/#admissibility-and-consistency","title":"Admissibility and Consistency","text":"<p>What makes a heuristic good?</p> <ul> <li>\\(g(n)\\) - The function representing total backwards cost   computed by UCS.</li> <li>\\(h(n)\\) - The heuristic value function, or estimated forward   cost, used by greedy search.</li> <li>\\(f(n) = g(n) + h(n)\\) - The function representing the   estimated total cost used by A*.</li> </ul> <p>The condition required for optimality when using A* tree search is known as admissibility. Defining \\(h^*(n)\\) as the true optimal forward cost to reach a goal state a given node \\(n\\),  we can formulate the adimissibility constraint as:</p> \\[ \\forall n, \\quad 0 \\leq h(n) \\leq h^*(n) \\] <p>Theorem</p> <p>For a given search problem, if the admissibility constraint is satisfied by a heuristic function \\(h\\), using A* tree search with \\(h\\) on that  search problem will yield an optimal solution.</p> <p>An additional caveat of graph search is that it tends to ruin the optimality of A*, even under admissible heuristics. \u8bf7\u770b\u4e0b\u9762\u7684\u4f8b\u5b50\uff1a</p> <p></p> <p>\u53ef\u4ee5\u770b\u5230\uff0c\u5de6\u8fb9\u7684\u56fe\u7247\u7684\\(h(n)\\)\u662fadmissible\u7684\uff0c\u4f46\u662f\u53f3\u56fe\u8bf4\u660e\uff0c\u5728graph search\u4e2d\u4f7f\u7528\u8fd9\u4e2aheuristic\u4f1a\u5bfc\u81f4A*\u6ca1\u6709\u627e\u5230\u6700\u4f18\u89e3\u3002</p> <p>\u660e\u663e\u7684\u95ee\u9898\u662f\uff0c\u6211\u4eec\u7684heuristic\u5e76\u6ca1\u6709\u5bf9edge\u7684cost\u4f5c\u51fa\u5408\u7406\u7684\u4f30\u8ba1(\u5728\u5de6\u56fe\u4e2d\uff0c\\(h(A) - h(S) = 2 \\gt cost(S, A) = 1\\))</p> <p>So here comes the concept of consistency. Consistency is a stronger condition that encompasses  admissibility and adds additional requirements. </p> <p>Consistency: We enforce not only that a heuristic underestimates the total distance to a goal from any given state, but also the cost/weight of each edge in the graph. The consistency constraint is formulated as:</p> \\[ \\forall A,\\ C \\quad h(A) - h(C) \\leq cost(A, C) \\] <p>Theorem</p> <p>For a given search problem, if the consistency constraint is satisfied by a heuristic function \\(h\\), using A* graph search with \\(h\\) on that  search problem will yield an optimal solution.</p> <p>Important Note: </p> <ul> <li>For heuristic that are either admissible or consistent to be valid,   it must by definition be the case that \\(\\forall goal, \\ \\ h(goal) = 0\\).</li> <li>Consistency implies admissibility: if no edge costs are overestimated   (as guaranteed by consistency), the total estimated cost from any node   to the goal will also be underestimated.</li> </ul>"},{"location":"course_notes/cs188/lecture/Informed-Search/#dominance","title":"Dominance","text":"<p>The standard metric for comparing heuristics is the concept of dominance.</p> <p>If heuristic a is dominant over heuristic b, then the estimated goal distance for a is greater than the estimated goal distance for b for every node in the  state space graph.</p> \\[ \\forall n, \\quad h_a(n) \\geq h_b(n) \\] <p>The trivial heuristic is defined as \\(h(n) = 0\\) for all \\(n\\), reducing A* to UCS.</p> <p></p> <p>Common Practice</p> <p>Generate multiple admisible/consistent heuristics for any given search problem and compute the max over the values output by them to generate a heuristic that dominates (and hence is better than) all of them.</p>"},{"location":"course_notes/cs188/lecture/Informed-Search/#search-summary","title":"Search: Summary","text":"<ul> <li>Search problem. Their components: state space, actions, transition function,   action cost, start state, goal state.</li> <li>Rationality: The agent seeks to maximize their expected utility.</li> <li>Use PEAS to define the task environment.</li> <li>Uninformed search: BFS, DFS, UCS.</li> <li>Informed search: Greedy search, A* search.</li> </ul>"},{"location":"course_notes/cs188/lecture/Intro-to-AI/","title":"Intro to AI","text":""},{"location":"course_notes/cs188/lecture/Intro-to-AI/#agents","title":"Agents","text":"<p>In AI, the central problem at hand is that of the creation of a rational agent, an entity that has goals or preferences and tries to perform a series of actions that yield the best/optimal expected outcome given these goals.</p> <ul> <li>Reflex agent: doesn\u2019t think about the consequences of its actions</li> <li>Planning agent: maintain a model of the world and use this model to simulate performing various actions. Then, the agent can determine hypothesized consequences of the actions and can select the best one.</li> </ul>"},{"location":"course_notes/cs188/lecture/Intro-to-AI/#environment","title":"Environment","text":"<p>The design of an agent heavily depends on the type of environment the agents acts upon</p> <p>To define the task environment we use the PEAS (Performance Measure, Environment, Actuators, Sensors) description.</p> <ul> <li>Partially observable environment: the agent does not have full information about the state and thus the agent must have an internal estimate of the state of the world.</li> <li>contrast to fully observable environment: the agent has full access to the state of the world.</li> <li>Stochastic environment: have uncertainty in the transition model, i.e. taking an action in a specific state may have multiple possible outcomes with different probabilities.</li> <li>contrast to deterministic environment</li> <li>multi-agent environment: the agent might need to randomize its actions in order to avoid being \u201cpredictable\" by other agents.</li> <li>If the environment does not change as the agent acts on it, then this environment is called static. This is contrast to dynamic environments that change as the agent interacts with it.</li> <li>known physics, unknown physics.</li> </ul>"},{"location":"course_notes/cs188/lecture/MDPs/","title":"Markov Decision Processes","text":""},{"location":"course_notes/cs188/lecture/MDPs/#non-deterministic-search","title":"Non-Deterministic Search","text":"<p>In the first note, we talked about traditional search problems and how to solve them; then, in the third note, we changed our model to account for adversaries and other agents in the world that influenced our path to goal states.</p> <p>Now, we\u2019ll change our model again to account for another influencing factor \u2013 the dynamics of world itself. The environment in which an agent is placed may subject the agent\u2019s actions to being nondeterministic, which means that there are multiple possible successor states that can result from an action taken in some state.</p> <p>Such problems where the world poses a degree of uncertainty are known as nondeterministic search problems, and can be solved with models known as Markov decision processes, or MDPs.</p>"},{"location":"course_notes/cs188/lecture/MDPs/#markov-decision-processes_1","title":"Markov Decision Processes","text":"<p>A Markov Decision Process is defined by several properties:</p> <ul> <li>A set of states \\(S\\). States in MDPs are represented in the same way as states in traditional search problems.</li> <li>A set of actions \\(A\\). Actions in MDPs are also represented in the same way as in traditional search problems.</li> <li>A start state.</li> <li>Possibly one or more terminal states.</li> <li>Possibly a discount factor \\(\\gamma\\). </li> <li>A transition function \\(T\\left(s, a, s^{\\prime}\\right)\\). Since we have introduced the possibility of nondeterministic actions, we need a way to delineate the likelihood of the possible outcomes after taking any given action from any given state. The transition function for a MDP does exactly this - it's a probability function which represents the probability that an agent taking an action \\(a \\in A\\) from a state \\(s \\in S\\) ends up in a state \\(s^{\\prime} \\in S\\).</li> <li>A reward function \\(R\\left(s, a, s^{\\prime}\\right)\\). Typically, MDPs are modeled with small \"living\" rewards at each step to reward an agent's survival, along with large rewards for arriving at a terminal state. Rewards may be positive or negative depending on whether or not they benefit the agent in question, and the agent's objective is naturally to acquire the maximum reward possible before arriving at some terminal state.</li> </ul> <p>We represent the movement of an agent through different MDP states over time with discrete timesteps, defining \\(s_t \\in S\\) and \\(a_t \\in A\\) as the state in which an agent exists and the action which an agent takes at timestep \\(t\\) respectively. An agent starts in state \\(s_0\\) at timestep 0 , and takes an action at every timestep. The movement of an agent through a MDP can thus be modeled as follows:</p> \\[ s_0 \\xrightarrow{a_0} s_1 \\xrightarrow{a_1} s_2 \\xrightarrow{a_2} s_3 \\xrightarrow{a_3} \\ldots \\] <p>Additionally, knowing that an agent\u2019s goal is to maximize it\u2019s reward across all timesteps, we can correspondingly express this mathematically as a maximization of the following utility function:</p> \\[ U\\left(\\left[s_0, a_0, s_1, a_1, s_2, \\ldots\\right]\\right)=R\\left(s_0, a_0, s_1\\right)+R\\left(s_1, a_1, s_2\\right)+R\\left(s_2, a_2, s_3\\right)+\\ldots \\] <p>Markov decision processes, like state-space graphs, can be unraveled into search trees. Uncertainty is modeled in these search trees with Q-states, also known as action states. The Qstate represented by having taken action \\(a\\) from state \\(s\\) is notated as the tuple \\((s, a)\\).</p> <p></p> <p>The green nodes represent Q-states, where an action has been taken from a state but has yet to be resolved into a successor state.</p>"},{"location":"course_notes/cs188/lecture/MDPs/#finite-horizons-and-discounting","title":"Finite Horizons and Discounting","text":"<p>An MDP enforcing a finite horizon is simple - it essentially defines a \"lifetime\" for agents, which gives them some set number of timesteps \\(n\\) to accrue as much reward as they can before being automatically terminated.</p> <p>Discount factors are introduced to model an exponential decay in the value of rewards over time. Concretely, with a discount factor of \\(\\gamma\\), taking action \\(a_t\\) from state \\(s_t\\) at timestep \\(t\\) and ending up in state \\(s_{t+1}\\) results in a reward of \\(\\gamma^t R\\left(s_t, a_t, s_{t+1}\\right)\\) instead of just \\(R\\left(s_t, a_t, s_{t+1}\\right)\\). Now, instead of maximizing the additive utility</p> \\[ U\\left(\\left[s_0, a_0, s_1, a_1, s_2, \\ldots\\right]\\right)=R\\left(s_0, a_0, s_1\\right)+R\\left(s_1, a_1, s_2\\right)+R\\left(s_2, a_2, s_3\\right)+\\ldots \\] <p>we attempt to maximize discounted utility</p> \\[ U\\left(\\left[s_0, a_0, s_1, a_1, s_2, \\ldots\\right]\\right)=R\\left(s_0, a_0, s_1\\right)+\\gamma R\\left(s_1, a_1, s_2\\right)+\\gamma^2 R\\left(s_2, a_2, s_3\\right)+\\ldots \\] <p>Noting that the above definition of a discounted utility function looks similar to a geometric series with ratio \\(\\gamma\\), we can prove that it's guaranteed to be finite-valued as long as the constraint \\(|\\gamma|&lt;1\\) is met :</p> \\[ \\begin{aligned} U\\left(\\left[s_0, a_0, s_1, a_1, s_2, \\ldots\\right]\\right) &amp;= R\\left(s_0, a_0, s_1\\right)+\\gamma R\\left(s_1, a_1, s_2\\right)+\\gamma^2 R\\left(s_2, a_2, s_3\\right)+\\ldots \\\\ &amp;= \\sum_{t=0}^{\\infty} \\gamma^t R\\left(s_t, a_t, s_{t+1}\\right) \\leq \\sum_{t=0}^{\\infty} \\gamma^t R_{\\max } = \\frac{R_{\\max}}{1-\\gamma} \\end{aligned} \\]"},{"location":"course_notes/cs188/lecture/MDPs/#markovianess","title":"Markovianess","text":"<p>Markov decision processes are \"markovian\" in the sense that they satisfy the Markov property, or memoryless property, which states that the future and the past are conditionally independent, given the present. Intuitively, this means that, if we know the present state, knowing the past doesn\u2019t give us any more information about the future.</p> \\[ \\begin{align*} P(S_{t+1}=s_{t+1} &amp;\\mid S_t=s_t, A_t=a_t, S_{t-1}=s_{t-1}, A_{t-1}=a_{t-1}, \\ldots, S_0=s_0) \\\\ &amp;= P(S_{t+1}=s_{t+1} \\mid S_t=s_t, A_t=a_t) \\end{align*} \\] <p>which is \"memoryless\" in the sense that the probability of arriving in a state \\(s^{\\prime}\\) at time \\(t+1\\) depends only on the state \\(s\\) and action \\(a\\) taken at time \\(t\\), not on any earlier states or actions. In fact, it is these memoryless probabilities which are encoded by the transition function: \\(T\\left(s, a, s^{\\prime}\\right)=P\\left(s^{\\prime} \\mid s, a\\right)\\).</p>"},{"location":"course_notes/cs188/lecture/MDPs/#solving-markov-decision-processes","title":"Solving Markov Decision Processes","text":"<p>Solving a Markov decision process, on the other hand, means finding an optimal policy \\(\\pi^*: S \\rightarrow A\\), a function mapping each state \\(s \\in S\\) to an action \\(a \\in A\\). An explicit policy \\(\\pi\\) defines a reflex agent - given a state \\(s\\), an agent at \\(s\\) implementing \\(\\pi\\) will select \\(a=\\pi(s)\\) as the appropriate action to make without considering future consequences of its actions. An optimal policy is one that if followed by the implementing agent, will yield the maximum expected total reward or utility.</p>"},{"location":"course_notes/cs188/lecture/MDPs/#the-bellman-equation","title":"The Bellman Equation","text":"<p>We must first introduce two new mathematical quantities:</p> <ul> <li>The optimal value of a state \\(s, \\  U^*(s) \\ \\ or \\ \\  V^*(s)\\) - the optimal value of \\(s\\) is the expected value of the utility an optimally-behaving agent that starts in \\(s\\) will receive, over the rest of the agent's lifetime. </li> <li>The optimal value of a Q-state \\((s, a), \\ Q^*(s, a)\\) - the optimal value of \\((s, a)\\) is the expected value of the utility an agent receives after starting in \\(s\\), taking \\(a\\), and acting optimally henceforth.</li> </ul> <p>Using these two new quantities and the other MDP quantities discussed earlier, the Bellman equation is defined as follows:</p> \\[ U^*(s)=\\max _a \\sum_{s^{\\prime}} T\\left(s, a, s^{\\prime}\\right)\\left[R\\left(s, a, s^{\\prime}\\right)+\\gamma U^*\\left(s^{\\prime}\\right)\\right] \\] <p>Before we begin interpreting what this means, let's also define the equation for the optimal value of a Q-state (commonly known as an optimal Q-value):</p> \\[ Q^*(s, a)=\\sum_{s^{\\prime}} T\\left(s, a, s^{\\prime}\\right)\\left[R\\left(s, a, s^{\\prime}\\right)+\\gamma U^*\\left(s^{\\prime}\\right)\\right] \\] <p>This second definition allows us to reexpress the Bellman equation as</p> \\[ U^*(s)=\\max _a Q^*(s, a) \\] <p>Tip</p> <p>The Bellman equation is an example of a dynamic programming equation, an equation that decomposes a problem into smaller subproblems via an inherent recursive structure.</p> <p>\u4ece\u72b6\u6001\\(s\\)\u51fa\u53d1\uff0c\u91c7\u53d6action \\(a\\)\uff0c\u4f1a\u6709\u591a\u79cd\u53ef\u80fd\u7684successor \\(s'\\),  \\(\\left[R\\left(s, a, s^{\\prime}\\right)+\\gamma U^*\\left(s^{\\prime}\\right)\\right]\\) \u4ee3\u8868\u8be5\u9879\u8868\u793a\u4ee3\u7406\u901a\u8fc7\u9996\u5148\u4ece\\(s\\)\u91c7\u53d6\\(a\\)\u5e76\u5230\u8fbe\\(s'\\)\uff0c\u7136\u540e\u5728\u6b64\u540e\u91c7\u53d6\u6700\u4f73\u884c\u52a8\u800c\u83b7\u5f97\u7684total utility\u3002 \\(Q^*(s, a)\\)\u5bf9\u6240\u6709\u53ef\u80fd\u7684\\(s'\\)\u7684utility\u8fdb\u884c\u52a0\u6743\u6c42\u548c\u3002</p> <p>\u4ece\u72b6\u6001\\(s\\)\u5f00\u59cb\u6709\u591a\u79cd\u53ef\u80fd\u7684action \\(a\\)\uff0c\u6bcf\u4e2aaction \\(a\\)\u90fd\u6709\u5bf9\u5e94\u7684\\(Q^*(s, a)\\)\uff0c\\(U^*(s)\\)\u9009\u62e9\u4e86\u6700\u5927\u7684\u90a3\u4e2a\u3002</p>"},{"location":"course_notes/cs188/lecture/MDPs/#value-iteration","title":"Value Iteration","text":"<p>Value iteration is a dynamic programming algorithm that uses an iteratively longer time limit to compute time-limited values until convergence. It operates as follows:</p> <ol> <li>\\(\\forall s \\in S\\), initialize \\(U_0(s)=0\\).</li> <li>Repeat the following update rule until convergence:</li> </ol> \\[ \\forall s \\in S, U_{k+1}(s) \\leftarrow \\max _a \\sum_{s^{\\prime}} T\\left(s, a, s^{\\prime}\\right)\\left[R\\left(s, a, s^{\\prime}\\right)+\\gamma U_k\\left(s^{\\prime}\\right)\\right] \\] <p>For conciseness, we frequently denote \\(U_{k+1}(s) \\leftarrow \\max _a \\sum_{s^{\\prime}} T\\left(s, a, s^{\\prime}\\right)\\left[R\\left(s, a, s^{\\prime}\\right)+\\gamma U_k\\left(s^{\\prime}\\right)\\right]\\) with the shorthand \\(U_{k+1} \\leftarrow B U_k\\), where \\(B\\) is called the Bellman operator. </p>"},{"location":"course_notes/cs188/lecture/MDPs/#policy-extraction","title":"Policy Extraction","text":"<p>Recall that our ultimate goal in solving a MDP is to determine an optimal policy. This can be done once all optimal values for states are determined using a method called policy extraction. The intuition behind policy extraction is very simple: if you're in a state \\(s\\), you should take the action \\(a\\) which yields the maximum expected utility.</p> \\[ \\forall s \\in S, \\pi^*(s)=\\underset{a}{\\operatorname{argmax}} Q^*(s, a)=\\underset{a}{\\operatorname{argmax}} \\sum_{s^{\\prime}} T\\left(s, a, s^{\\prime}\\right)\\left[R\\left(s, a, s^{\\prime}\\right)+\\gamma U^*\\left(s^{\\prime}\\right)\\right] \\]"},{"location":"course_notes/cs188/lecture/MDPs/#q-value-iteration","title":"Q-Value Iteration","text":"<p>Q-value iteration is a dynamic programming algorithm that computes time-limited Q-values. It is described in the following equation:</p> \\[ Q_{k+1}(s, a) \\leftarrow \\sum_{s^{\\prime}} T\\left(s, a, s^{\\prime}\\right)\\left[R\\left(s, a, s^{\\prime}\\right)+\\gamma \\max _{a^{\\prime}} Q_k\\left(s^{\\prime}, a^{\\prime}\\right)\\right] \\] <p>Once we have the optimal Q-values for each state and action, we can then find the policy for a state by simply choosing the action which has the highest Q-value.</p>"},{"location":"course_notes/cs188/lecture/MDPs/#policy-iteration","title":"Policy Iteration","text":"<p>Policy iteration: an algorithm that maintains the optimality of value iteration while providing significant performance gains. </p> <ol> <li>Define an initial policy. This can be arbitrary, but policy iteration will converge faster the closer the initial policy is to the eventual optimal policy.</li> <li>Repeat the following until convergence:</li> </ol> <ul> <li>Evaluate the current policy with policy evaluation. For a policy \\(\\pi\\), policy evaluation means computing \\(U^\\pi(s)\\) for all states \\(s\\), where \\(U^\\pi(s)\\) is expected utility of starting in state \\(s\\) when following \\(\\pi\\) :</li> </ul> \\[ U^\\pi(s)=\\sum_{s^{\\prime}} T\\left(s, \\pi(s), s^{\\prime}\\right)\\left[R\\left(s, \\pi(s), s^{\\prime}\\right)+\\gamma U^\\pi\\left(s^{\\prime}\\right)\\right] \\] <p>Define the policy at iteration \\(i\\) of policy iteration as \\(\\pi_i\\). Since we are fixing a single action for each state, we no longer need the max operator which effectively leaves us with a system of \\(|S|\\) equations generated by the above rule. Each \\(U^{\\pi_i}(s)\\) can then be computed by simply solving this system. Alternatively, we can also compute \\(U^{\\pi_i}(s)\\) by using the following update rule until convergence, just like in value iteration: $$ U_{k+1}^{\\pi_i}(s) \\leftarrow \\sum_{s^{\\prime}} T\\left(s, \\pi_i(s), s^{\\prime}\\right)\\left[R\\left(s, \\pi_i(s), s^{\\prime}\\right)+\\gamma U_k^{\\pi_i}\\left(s^{\\prime}\\right)\\right] $$</p> <p>However, this second method is typically slower in practice.</p> <ul> <li>Once we've evaluated the current policy, use policy improvement to generate a better policy. Policy improvement uses policy extraction on the values of states generated by policy evaluation to generate this new and improved policy:</li> </ul> \\[ \\pi_{i+1}(s)=\\underset{a}{\\operatorname{argmax}} \\sum_{s^{\\prime}} T\\left(s, a, s^{\\prime}\\right)\\left[R\\left(s, a, s^{\\prime}\\right)+\\gamma U^{\\pi_i}\\left(s^{\\prime}\\right)\\right] \\] <p>If \\(\\pi_{i+1}=\\pi_i\\), the algorithm has converged, </p> <p>and we can conclude that \\(\\pi_{i+1}=\\pi_i=\\pi^*\\).</p>"},{"location":"course_notes/cs188/lecture/MDPs/#summary","title":"Summary","text":"<ul> <li>Value iteration: Used for computing the optimal values of states, by iterative updates until convergence.</li> <li>Policy evaluation: Used for computing the values of states under a specific policy.</li> <li>Policy extraction: Used for determining a policy given some state value function. If the state values are optimal, this policy will be optimal. This method is used after running value iteration, to compute an optimal policy from the optimal state values; or as a subroutine in policy iteration, to compute the best policy for the currently estimated state values.</li> <li>Policy iteration: A technique that encapsulates both policy evaluation and policy extraction and is used for iterative convergence to an optimal policy. It tends to outperform value iteration, by virtue of the fact that policies usually converge much faster than the values of states.</li> </ul>"},{"location":"course_notes/cs188/lecture/ML-Logistic-Regression/","title":"ML: Logistic Regression","text":""},{"location":"course_notes/cs188/lecture/ML-Logistic-Regression/#logistic-regression","title":"Logistic Regression","text":"<p>Logistic regression allows us to turn a linear combination of our input features into a probability using the logistic function:</p> \\[ h_{\\mathbf{w}}(\\mathbf{x})=\\frac{1}{1+e^{-\\mathbf{w}^T \\mathbf{x}}} . \\] <p>Misnomer</p> <p>Logistic regression is used to solve classification problems, not regression problems.</p> <p>The logistic function \\(g(z)=\\frac{1}{1+e^{-z}}\\) is frequently used to model binary outputs. Note that the output of the function is always between 0 and 1 , as seen in the following figure:</p> <p></p> <p>Intuitively, the logistic function models the probability of a data point belonging in class with label 1. The reason for that is that the output of the logistic function is bounded between 0 and 1, and we want our model to capture the probability of a feature having a specific label.</p>"},{"location":"course_notes/cs188/lecture/ML-Logistic-Regression/#multi-class-logistic-regression","title":"Multi-Class Logistic Regression","text":"<p>In multi-class logistic regression, we want to classify data points into K distinct categories. We use the softmax function in place of the logistic function, which models the probability of a new data point with features \\(x\\) having label \\(i\\) as follows:</p> \\[ P(y=i \\mid \\mathbf{f}(\\mathbf{x}) ; \\mathbf{w})=\\frac{e^{\\mathbf{w}_i^T \\mathbf{f}(\\mathbf{x})}}{\\sum_{k=1}^K e^{\\mathbf{w}_k^T \\mathbf{f}(\\mathbf{x})}} \\] <p>We estimate the parameters \\(\\mathbf{w}\\) to maximize the likelihood that we observed the data.</p> \\[ \\ell\\left(\\mathbf{w}_1, \\ldots, \\mathbf{w}_K\\right)=\\prod_{i=1}^n P\\left(y_i \\mid \\mathbf{f}\\left(\\mathbf{x}_i\\right) ; \\mathbf{w}\\right) \\]"},{"location":"course_notes/cs188/lecture/ML-Naive%20Bayes/","title":"ML: Naive Bayes","text":""},{"location":"course_notes/cs188/lecture/ML-Naive%20Bayes/#machine-learning","title":"Machine Learning","text":"<p>Machine Learning</p> <p>A broad field of computer science that deals with constructing and/or learning the parameters of a specified model given some data.</p> <p>Two primary subgroups of machine learning algorithms:</p> <ul> <li>Supervised Learning: Infer a relationship between input data and corresponding     output data in order to predict outputs for new, previously unseen inputs.</li> <li>Unsupervised Learning: Given input data that does not have corresponding output data, deal with recognizing inherent structure between or within datapoints and grouping and/or processing them accordingly.</li> </ul> <p>In this note, the algorithms we\u2019ll discuss will be limited to supervised learning tasks.</p> <p>Once you have a dataset that you\u2019re ready to learn with, the machine learning process usually involves splitting your dataset into three distinct subsets:</p> <ol> <li>Training Set: is used to actually generate a model mapping inputs to outputs.</li> <li>Validation Set: is used to measure your model\u2019s performance on data it hasn\u2019t seen before, and to tune hyperparameters (parameters that are not learned by the model, but are set by the user).</li> <li>Test Set: never seen by your model until the very end of development, and is    equivalent of a \"final exam\" to gauge performance on real-world data.</li> </ol>"},{"location":"course_notes/cs188/lecture/ML-Naive%20Bayes/#naive-bayes","title":"Naive Bayes","text":"<p>Classification Problem</p> <p>Given various data points, our goal is to group them into one of two or more classes. For classification problems, we're given a training set of data points along with their corresponding labels, which are typically one of a few discrete values.</p> <p>In this section we'll describe how to construct a type of model for solving classification problems known as a Naive Bayes Classifier.</p> <p>In order to learn anything useful, we need to extract certain attributes from each of them known as features. </p> <p>The specific features extracted for training are often dependent on the specific problem you\u2019re trying to solve and which features you decide to select can often impact the performance of your model dramatically. Deciding which features to utilize is known as feature engineering and is fundamental to machine learning.</p> <p></p> <ul> <li>Random Variables in this Bayes' net<ul> <li>\\(Y\\): the class label</li> <li>\\(F_1, F_2, \\ldots, F_n\\): the features</li> </ul> </li> <li>Probability tables in this Bayes\u2019 net<ul> <li>\\(P(Y)\\): Probability of each label occurring, given no information about the features - known as the prior.</li> <li>\\(P(F_i|Y)\\): One table per feature. Probability distribution over a feature given a class label.</li> </ul> </li> <li>To perform training:<ul> <li>Use the training dataset to estimate the probability tables.</li> <li>Estimate \\(P(Y)\\): how often does each label occur?</li> <li>Estimate \\(P(F_i|Y)\\): how does the label affect the feature?</li> </ul> </li> <li>To perform classification:<ul> <li>Instantiate all features.</li> <li>Query for \\(P(Y \\mid f_1, f_2, \\ldots, f_n)\\). Probability of each label given the features.</li> </ul> </li> </ul>"},{"location":"course_notes/cs188/lecture/ML-Naive%20Bayes/#example-spam-filtering","title":"Example: Spam Filtering","text":"<p>After noting that our desired probabilities - the probability of each label \\(y_i\\) given our features, \\(P\\left(Y=y_i \\mid F_1=f_1, \\ldots, F_n=f_n\\right)\\) - is proportional to the joint \\(P\\left(Y=y_i, F_1=f_1, \\ldots, F_n=f_n\\right)\\), we can compute:</p> \\[ P\\left(Y, F_1=f_1, \\ldots, F_n=f_n\\right)=\\left[\\begin{array}{c} P\\left(Y=y_1, F_1=f_1, \\ldots, F_n=f_n\\right) \\\\ P\\left(Y=y_2, F_1=f_1, \\ldots, F_n=f_n\\right) \\\\ \\vdots \\\\ P\\left(Y=y_k, F_1=f_1, \\ldots, F_n=f_n\\right) \\end{array}\\right]=\\left[\\begin{array}{c} P\\left(Y=y_1\\right) \\prod_i P\\left(F_i=f_i \\mid Y=y_1\\right) \\\\ P\\left(Y=y_2\\right) \\prod_i P\\left(F_i=f_i \\mid Y=y_2\\right) \\\\ \\vdots \\\\ P\\left(Y=y_k\\right) \\prod_i P\\left(F_i=f_i \\mid Y=y_k\\right) \\end{array}\\right] \\] <p>Our prediction for class label corresponding to the feature vector \\(F\\) is simply the label corresponding to the maximum value in the above computed vector:</p> \\[ \\text { prediction }(F)=\\underset{y_i}{\\operatorname{argmax}} P\\left(Y=y_i\\right) \\prod_j P\\left(F_j=f_j \\mid Y=y_i\\right) \\]"},{"location":"course_notes/cs188/lecture/ML-Naive%20Bayes/#parameter-estimation","title":"Parameter Estimation","text":"<p>Assume you have a set of N sample points or observations, \\(x_1, \\ldots, x_N\\), and you believe that this data was drawn from a distribution parameterized by an unknown value \\(\\theta\\).</p> <p>How can you \"learn\" the most likely value of \\(\\theta\\) given your sample?</p> <p>A frequently used and fundamental method in machine learning known as Maximum Likelihood Estimation (MLE) does exactly this.</p> <p>Maximum likelihood estimation typically makes the following simplifying assumptions:</p> <ul> <li>Each sample is drawn from the same distribution.</li> <li>Each sample \\(x_i\\) is conditionally independent of the others, given the parameters for our distribution. In the coin flipping example, the outcome of one flip does not affect any of the others.</li> <li>All possible values of \\(\\theta\\) are equally likely before observing the data. This is known as a uniform prior.</li> </ul> <p>The first two assumptions are often referred to as independent, identically distributed (i.i.d.).</p> <p>The likelihood \\(\\mathscr{L}(\\theta)\\) of our sample, a function which represents the probability of having drawn our sample from our distribution. For a fixed sample \\(x_1, x_N\\), the likelihood is just a function of \\(\\theta\\) :</p> \\[ \\mathscr{L}(\\theta)=P_\\theta\\left(x_1, \\ldots, x_N\\right) \\] <p>Using our simplifying assumption that the samples \\(x_i\\) are i.i.d., the likelihood function can be re-expressed as follows:</p> \\[ \\mathscr{L}(\\theta)=\\prod_{i=1}^N P_\\theta\\left(x_i\\right) \\] <p>The maximum likelihood estimate for \\(\\theta\\) is a value that satisfies the following:</p> \\[ \\frac{\\partial}{\\partial \\theta} \\mathscr{L}(\\theta)=0 \\]"},{"location":"course_notes/cs188/lecture/ML-Naive%20Bayes/#maximum-likelihood-for-naive-bayes","title":"Maximum Likelihood for Naive Bayes","text":"<p>Let\u2019s now return to the problem of inferring conditional probability tables for our spam classifier.</p> <p>For simplicity, let's specifically consider \\(P\\left(F_i \\mid Y=\\right.\\) ham \\()\\) and try to find the maximum likelihood estimate for a parameter \\(\\theta=P\\left(F_i=1 \\mid Y=h a m\\right)\\) i.e. the probability that the \\(i^{t h}\\) word in our dictionary appears in a ham email. </p> <p>(\\(f_i^{(j)}\\) is 1 if the \\(i^{t h}\\) word in the \\(j^{t h}\\) email is present, and 0 otherwise.)</p> \\[ \\mathscr{L}(\\theta)=\\prod_{j=1}^{N_h} P\\left(F_i=f_i^{(j)} \\mid Y=h a m\\right)=\\prod_{j=1}^{N_h} \\theta^{f_i^{(j)}}(1-\\theta)^{1-f_i^{(j)}} \\] <p>Because \\(\\log\\) is a monotonic function, maximizing the likelihood is equivalent to maximizing the log likelihood:</p> \\[ \\begin{aligned} \\log \\mathscr{L}(\\theta) &amp; =\\log \\left(\\prod_{j=1}^{N_h} \\theta^{f_i^{(j)}}(1-\\theta)^{1-f_i^{(j)}}\\right) \\\\ &amp; =\\log (\\theta) \\sum_{j=1}^{N_h} f_i^{(j)}+\\log (1-\\theta) \\sum_{j=1}^{N_h}\\left(1-f_i^{(j)}\\right) \\end{aligned} \\] \\[ \\frac{\\partial}{\\partial \\theta}\\left(\\log (\\theta) \\sum_{j=1}^{N_h} f_i^{(j)}+\\log (1-\\theta) \\sum_{j=1}^{N_h}\\left(1-f_i^{(j)}\\right)\\right)=0 \\] \\[ \\theta=\\frac{1}{N_h} \\sum_{j=1}^{N_h} f_i^{(j)} \\] <p>According to our formula above, the maximum likelihood estimate for \\(\\theta\\) (which,  remember, is the probability that \\(P\\left(F_i=1 \\mid Y=h a m\\right)\\)) corresponds to counting the number of ham emails in which word i appears and dividing it by the total number of ham emails. </p>"},{"location":"course_notes/cs188/lecture/ML-Naive%20Bayes/#smoothing","title":"Smoothing","text":"<p>Warning</p> <p>Though maximum likelihood estimation is a very powerful method for parameter estimation, bad training data can often lead to unfortunate consequences.</p> <p>For example, if every time the word \u201cminute\u201d appears in an email in our training set, that email is classified as spam, our trained model will learn that</p> \\[ P\\left(F_{\\text {minute}}=1 \\mid Y=\\text {ham}\\right)=0 \\] <p>Hence  your model will never classify any email containing the word minute as ham.</p> <p>This is a classic example of overfitting, or building a model that doesn\u2019t generalize well to previously unseen data.</p> <p>Overfitting with Naive Bayes\u2019 classifiers can be mitigated by Laplace Smoothing. Conceptually, Laplace smoothing with strength \\(k\\) assumes having seen \\(k\\)  extra of each outcome. Hence if for a given sample your maximum likelihood estimate for an outcome \\(x\\) that can take on \\(|X|\\) different values from a sample of size \\(N\\) is:</p> \\[ P_{M L E}(x)=\\frac{\\operatorname{count}(x)}{N} \\] <p>then the Laplace estimate with strength \\(k\\) is</p> \\[ P_{L A P, k}(x)=\\frac{\\operatorname{count}(x)+k}{N+k|X|} \\] <p>There are two particularly notable cases for Laplace smoothing. When \\(k = 0\\), Laplace smoothing is equivalent to maximum likelihood estimation. When \\(k = \\infty\\):</p> \\[ P_{L A P, \\infty}(x)=\\frac{1}{|X|} \\] <p>The specific value of \\(k\\) that\u2019s appropriate to use in your model is typically determined by trial-and-error. \\(k\\) is a hyperparameter in your model, which means that you can set it to whatever you want and see which value yields the best prediction accuracy/performance on your validation set.</p>"},{"location":"course_notes/cs188/lecture/ML-Neural-Networks/","title":"ML: Neural Networks","text":""},{"location":"course_notes/cs188/lecture/ML-Neural-Networks/#neural-networks-motivation","title":"Neural Networks: Motivation","text":""},{"location":"course_notes/cs188/lecture/ML-Neural-Networks/#non-linear-separators","title":"Non-linear Separators","text":"<p>Many practical problems involve the need for decision boundaries that are nonlinear in nature, and our linear perceptron model isn\u2019t expressive enough to capture this relationship.</p> <p>Consider the following set of data:</p> <p></p> <p>We would like to separate the two colors, and clearly there is no way this can be done in a single dimension (a single dimensional decision boundary would be a point, separating the axis into two regions). </p> <p>Tip</p> <p>To fix this problem, we can add additional (potentially nonlinear) features to construct a decision boundary from. </p> <p>Consider the same dataset with the addition of \\(x^2\\) as a feature:</p> <p></p> <p>Now we were able to fix the problem by mapping our data to a higher dimensional space by manually adding useful features to data points.</p> <p>A natural desire is to learn these featurization or transformation functions as well, perhaps using a nonlinear function class that is capable of representing a wider variety of functions.</p>"},{"location":"course_notes/cs188/lecture/ML-Neural-Networks/#multi-layer-perceptrons","title":"Multi-layer Perceptrons","text":"<p>In fact, a multi-layer perceptron is a universal function approximator and can represent any real function, leaving us only with the problem of  selecting the best set of weights to parameterize our network.</p> <p>Theorem - Universal Function Approximators</p> <p>A two-layer neural network with a sufficient number of neurons can approximate any continuous function to any desired accuracy.</p>"},{"location":"course_notes/cs188/lecture/ML-Neural-Networks/#measuring-accuracy","title":"Measuring Accuracy","text":"<p>The accuracy of the binary perceptron after making \\(n\\) predictions can be expressed as:</p> \\[ l^{a c c}(\\boldsymbol{w})=\\frac{1}{n} \\sum_{i=1}^n\\left(\\operatorname{sgn}\\left(\\boldsymbol{w} \\cdot \\mathbf{f}\\left(\\mathbf{x}_i\\right)\\right)==y_i\\right) \\] <p>In this context, \\(\\operatorname{sgn}(x)\\) represents an indicator function, which returns \\(1\\) if \\(x\\) is positive and \\(-1\\) otherwise. We can note that our accuracy function above is equivalent to dividing the total number of correct predictions by the raw total number of predictions.</p> <p>Sometimes, we want an output that is more expressive than a binary label. It then becomes useful to produce a probability for each of the N classes we want to classify into, which reflects our a degree of certainty that the data point belongs to each of the possible classes.</p> <p>We transition from storing a single weight vector to storing a weight vector for each class j, and estimate probabilities with the softmax function. The softmax function defines the probability of classifying \\(x^{(i)}\\) to class \\(j\\) as:</p> \\[ \\sigma\\left(\\mathbf{x}_i\\right)_j=\\frac{e^{\\mathbf{f}\\left(\\mathbf{x}_i\\right)^T \\mathbf{w}_j}}{\\sum_{\\ell=1}^N e^{\\mathbf{f}\\left(\\mathbf{x}_i\\right)^T \\mathbf{w}_{\\ell}}}=P\\left(y_i=j \\mid \\mathbf{f}\\left(\\mathbf{x}_i\\right) ; \\mathbf{w}\\right) \\] <p>Given a vector that is output by our function \\(f\\), softmax performs normalization to output a probability distribution. To come up with a general loss function for our models, we can use this probability distribution to generate an expression for the likelihood of a set of weights:</p> \\[ \\ell(\\boldsymbol{w})=\\prod_{i=1}^n P\\left(y_i \\mid \\mathbf{f}\\left(\\mathbf{x}_i\\right) ; \\mathbf{w}\\right) \\] <p>This expression denotes the likelihood of a particular set of weights explaining the observed labels and data points. We would like to find the set of weights that maximizes this quantity.</p>"},{"location":"course_notes/cs188/lecture/ML-Neural-Networks/#multi-layer-feedforward-neural-networks","title":"Multi-layer Feedforward Neural Networks","text":"<p>Note</p> <p>This is much like the multi-layer perceptron, however, we choose a different non-linearity to apply after the individual perceptron nodes. Note that it is these added non-linearities that makes the network as a whole non-linear and more expressive.</p>"},{"location":"course_notes/cs188/lecture/ML-Neural-Networks/#sigmoid-function","title":"Sigmoid Function","text":"\\[ \\sigma(x)=\\frac{1}{1+e^{-x}} \\]"},{"location":"course_notes/cs188/lecture/ML-Neural-Networks/#rectified-linear-unit-relu","title":"Rectified Linear Unit (ReLU)","text":"\\[ f(x)= \\begin{cases}0 &amp; \\text { if } x&lt;0 \\\\ x &amp; \\text { if } x \\geq 0\\end{cases} \\] <p>The choice of nonlinearity is a design choice that typically requires some experimentation to select a good one for each individual use case.</p> \\[ \\nabla_w \\ell(\\mathbf{w})=\\left[\\frac{\\partial \\ell \\ell(\\mathbf{w})}{\\partial \\mathbf{w}_1}, \\ldots, \\frac{\\partial \\ell \\ell(\\mathbf{w})}{\\partial \\mathbf{w}_n}\\right] \\]"},{"location":"course_notes/cs188/lecture/ML-Neural-Networks/#loss-functions-and-multivariate-optimization","title":"Loss Functions and Multivariate Optimization","text":"\\[ \\nabla_w \\ell(\\mathbf{w})=\\left[\\frac{\\partial \\ell \\ell(\\mathbf{w})}{\\partial \\mathbf{w}_1}, \\ldots, \\frac{\\partial \\ell \\ell(\\mathbf{w})}{\\partial \\mathbf{w}_n}\\right] \\] <p>We can find the optimal values of the parameters using the gradient ascent method described earlier.</p>"},{"location":"course_notes/cs188/lecture/ML-Neural-Networks/#neural-networks-backpropagation","title":"Neural Networks: Backpropagation","text":""},{"location":"course_notes/cs188/lecture/ML-Neural-Networks/#the-chain-rule","title":"The Chain Rule","text":""},{"location":"course_notes/cs188/lecture/ML-Neural-Networks/#the-backpropagation-algorithm","title":"The Backpropagation Algorithm","text":""},{"location":"course_notes/cs188/lecture/ML-Neural-Networks/#summary","title":"Summary","text":""},{"location":"course_notes/cs188/lecture/ML-Optimization/","title":"ML: Optimization","text":""},{"location":"course_notes/cs188/lecture/ML-Optimization/#optimization","title":"Optimization","text":"<p>In general though, a closed form solution may not exist for a given objective function. In cases like that we have to use gradient-based methods to find the optimal weights.</p> <p>Note</p> <p>The idea behind this is that the gradient points towards the direction of steepest increase of the objective. We maximize a function by moving towards the steepest ascent, and we minimize a function by moving towards the steepest descent direction.</p> <p>Gradient Ascent is used if the objective is a function which we want to maximize.</p> <p></p> <p>Gradient Descent is used if the objective is a loss function that we are trying to minimize.</p> <p></p> <p>At the beginning, we initialize the weights randomly. </p> <p>We denote the learning rate, which captures the size of the steps we make towards the gradient direction, with \\(\\alpha\\).</p> <ul> <li>Learning rate decay: Start gradient descent with a relatively large learning rate and reduce the learning rate as the number of iterations increases. </li> </ul> <p>If our dataset has a large number of \\(n\\) data points then computing the gradient as above in each iteration of the gradient descent algorithm might be too computationally intensive.</p> <p>As such, approaches like stochastic and batch gradient descent have been proposed.</p>"},{"location":"course_notes/cs188/lecture/ML-Perceptrons/","title":"ML: Perceptrons","text":""},{"location":"course_notes/cs188/lecture/ML-Perceptrons/#linear-regression","title":"Linear Regression","text":"<p>Regression problems are a form of machine learning problem in which the output is a continuous variable (denoted with \\(y\\) ). The features can be either continuous or categorical. We will denote a set of features with \\(\\mathbf{x} \\in \\mathbb{R}^n\\) for \\(n\\) features, i.e. \\(\\mathbf{x}=\\left(x_1, \\ldots, x_n\\right)\\).</p> <p>We use the following linear model to predict the output:</p> \\[ h_{\\mathbf{w}}(\\mathbf{x})=w_0+w_1 x_1+\\cdots w_n x_n \\]"},{"location":"course_notes/cs188/lecture/ML-Perceptrons/#perceptrons","title":"Perceptrons","text":""},{"location":"course_notes/cs188/lecture/ML-Perceptrons/#linear-classifiers","title":"Linear Classifiers","text":"<p>Lets start by looking at a simple linear classifier, which we can use for binary classification, which is when the label has two possibilities, positive or negative.</p> <p>The basic idea of linear classifiers is to do classification using a linear combination of the features\u2013 a value which we call the activation. </p> \\[ \\operatorname{activation}_w(\\mathbf{x})=h_{\\mathbf{w}}(\\mathbf{x})=\\sum_i w_i f_i(\\mathbf{x})=\\mathbf{w}^T \\mathbf{f}(\\mathbf{x})=\\mathbf{w} \\cdot \\mathbf{f}(\\mathbf{x}) \\] \\[ \\operatorname{classify}(\\mathbf{x})= \\begin{cases}+ &amp; \\text { if } h_{\\mathbf{w}}(\\mathbf{x})&gt;0 \\\\ - &amp; \\text { if } h_{\\mathbf{w}}(\\mathbf{x})&lt;0\\end{cases} \\] <p>To understand this geometrically, let us reexamine the vectorized activation function.</p> <p></p> \\[ \\operatorname{classify}(\\mathbf{x})= \\begin{cases}+ &amp; \\text { if } \\theta&lt;\\frac{\\pi}{2} \\\\ - &amp; \\text { if } \\theta&gt;\\frac{\\pi}{2}\\end{cases} \\] <p></p> <p>We call this blue line the decision boundary because it is the boundary that separates the region where we classify data points as positive from the region of negatives. In higher dimensions, a hyperplane is a linear surface that is one dimension lower than the latent space, thus dividing the surface in two.</p> <p></p>"},{"location":"course_notes/cs188/lecture/ML-Perceptrons/#binary-perceptrons","title":"Binary Perceptrons","text":"<p>Note</p> <p>When building a classifier, you start with data, which are labeled with the correct class, we call this the training set. You build a classifier by evaluating it on the training set, comparing that to your training labels, and  adjusting the parameters of your classifier until you reach your goal.</p> <p>Let\u2019s explore one specific implementation of a simple linear classifier: the binary perceptron.</p> <ul> <li>The perceptron is a binary classifier - though it can be extended to work on more than two classes.</li> </ul> <p>The goal of the binary perceptron is to find a decision boundary that perfectly separates the training data.</p>"},{"location":"course_notes/cs188/lecture/ML-Perceptrons/#the-algorithm","title":"The Algorithm","text":"<ol> <li>Initialize all weights to 0: \\(\\mathbf{w}=\\mathbf{0}\\)</li> <li> <p>For each training sample, with features \\(\\mathbf{x}\\) and true class label     \\(y^* \\in\\{-1,+1\\}\\), do:</p> <ol> <li> <p>Classify the sample using the current weights</p> \\[ y=\\operatorname{classify}(x)= \\begin{cases}+1 &amp; \\text { if } h_{\\mathbf{w}}(\\mathbf{x})=\\mathbf{w}^T \\mathbf{f}(\\mathbf{x})&gt;0 \\\\ -1 &amp; \\text { if } h_{\\mathbf{w}}(\\mathbf{x})=\\mathbf{w}^T \\mathbf{f}(\\mathbf{x})&lt;0\\end{cases} \\] </li> <li> <p>Compare the predicted label \\(y\\) to the true label \\(y^*\\):</p> <ul> <li>If \\(y=y^*\\), do nothing</li> <li>Otherwise, if \\(y\\neq y^*\\), update the weights: \\(\\mathbf{w} \\leftarrow \\mathbf{w}+y^* \\mathbf{f}(\\mathbf{x})\\)</li> </ul> </li> </ol> </li> <li> <p>If you went through every training sample without having to update your weights (all samples predicted correctly), then terminate. Else, repeat step 2.</p> </li> </ol>"},{"location":"course_notes/cs188/lecture/ML-Perceptrons/#updating-weights","title":"Updating weights","text":"<p>Recall that in step 2b above, when our classifier is wrong, the weights vector is updated as follows:</p> \\[ \\mathbf{w} \\leftarrow \\mathbf{w}+y^* \\mathbf{f}(\\mathbf{x}) \\] <ol> <li>Mis-classified positive as negative: \\(\\mathbf{w} \\leftarrow \\mathbf{w}+\\mathbf{f}(\\mathbf{x})\\)</li> <li>Mis-classified negative as positive: \\(\\mathbf{w} \\leftarrow \\mathbf{w}-\\mathbf{f}(\\mathbf{x})\\)</li> </ol> <p>Question</p> <p>Why does this work? </p> <p>One way to look at this is to see it as a balancing act.</p> <p>In case 1, how we adjust \\(\\mathbf{w}\\) should strive to fix that and make the activation larger for that training sample.</p> \\[ h_{\\mathbf{w}+\\mathbf{f}(\\mathbf{x})}(\\mathbf{x})=(\\mathbf{w}+\\mathbf{f}(x))^T \\mathbf{f}(\\mathbf{x})=\\mathbf{w}^T \\mathbf{f}(\\mathbf{x})+\\mathbf{f}(\\mathbf{x})^T \\mathbf{f}(\\mathbf{x})=h_{\\mathbf{w}}(\\mathbf{x})+\\mathbf{f}(\\mathbf{x})^T \\mathbf{f}(\\mathbf{x}) \\] <p>Question</p> <p>While this makes it clear why we are adding and subtracting something, why would we want to add and subtract our sample point\u2019s features?</p> <p></p>"},{"location":"course_notes/cs188/lecture/ML-Perceptrons/#bias","title":"Bias","text":"<p>The problem is, even among problems where there is a linear decision boundary that separates the positive and negative classes in the data, that boundary may not go through the origin, and we want to be able to draw those lines.</p> <p></p>"},{"location":"course_notes/cs188/lecture/ML-Perceptrons/#example","title":"Example","text":"<p>\\(w_0\\) is the weight of our bias feature, which is always 1.</p>"},{"location":"course_notes/cs188/lecture/ML-Perceptrons/#multiclass-perceptrons","title":"Multiclass Perceptrons","text":"<p>For the multi-class case, we will have one weight vector for each class. In order to classify a sample, we compute a score for each class by taking the dot product of the feature vector with each of the weight vectors. Whichever class yields the highest score is the one we choose as our prediction.</p> <p>An important thing to note is that in actual implementation, we do not keep track of the weights as separate structures, we usually stack them on top of each other to create a weight matrix.</p> <p>Along with the structure of our weights, our weight update also changes when we move to a multi-class case. If we correctly classify our data point, then do nothing just like in the binary case. If we chose incorrectly, say we chose class \\(y \\neq y^*\\), then:</p> <ul> <li>Add the feature vector to the weight vector for the true class to \\(y^*\\).</li> <li>Subtract the feature vector from the weight vector corresponding to the predicted class \\(y\\).</li> </ul> <p>Note</p> <p>'Rewarding' the correct weight vector, 'punishing' the misleading, incorrect weight vector, and leaving alone an other weight vectors.</p>"},{"location":"course_notes/cs188/lecture/ML-Perceptrons/#summary","title":"Summary","text":"<p>In this note, we introduced several fundamental principles of machine learning, including:</p> <ul> <li>Splitting our data into training data, validation data, and test data.</li> <li>The difference between supervised learning, which learns from labeled data, and unsupervised learning, which doesn't have labeled data and so attempts to infer inherent structure from it.</li> </ul> <p>We then proceeded to discuss an number of supervised learning algorithms such as Naive Bayes, Linear Regression, and the Perceptron Algorithm.</p> <ul> <li>We covered the Naive Bayes algorithm and derived the maximum likelihood estimates of the unknown model parameters. We extended this idea to discuss the problem of overfitting in the context of Naive Bayes' and how this issue can be mitigated with Laplace smoothing.</li> <li>We talked about Linear Regression, a simple model where we predict real-valued outputs as linear combinations of our input features. </li> <li>Finally, we talked about linear decision boundaries and the perceptron algorithm - a method for classification that repeatedly iterates over all our data and updates weight vectors when it classifies points incorrectly.</li> </ul>"},{"location":"course_notes/cs188/lecture/Particle%20Filtering/","title":"Particle Filtering","text":"<p>Note</p> <p>The Hidden Markov Model analog to Bayes' net sampling is called particle filtering, and involves simulating the motion of a set of particles through a state graph to approximate the probability (belief) distribution of the random variables in question. This solves the same question as the Forward Algorithm: it gives us an approximation of \\(P\\left(X_N \\mid e_{1: N}\\right)\\)</p> <p>Our belief that a particle is in any given state at any given timestep is dependent entirely on the number of particles in that state at that timestep in our simulation.</p>"},{"location":"course_notes/cs188/lecture/Particle%20Filtering/#particle-filtering-simulation","title":"Particle Filtering Simulation","text":"<ul> <li>Particle Initialization. </li> <li>Time Elapse Update: Update the value of each particle according to the transition model.</li> <li>Observation Update: During the observation update for particle filtering, we use the sensor model \\(P\\left(F_i \\mid T_i\\right)\\) to weight each particle according to the probability dictated by the observed evidence and the particle\u2019s state. Specifically, for a particle in state \\(t_i\\) with sensor reading \\(f_i\\), assign a weight of \\(P\\left(f_i \\mid t_i\\right)\\) to the particle.<ol> <li>Calculate the weights of all particles</li> <li>Calculate the total weight for each state.</li> <li>If the sum of all weights across all states is 0, reinitialize all particles.</li> <li>Else, normalize the distribution of total weights over states and resample your list of particles from this distribution.</li> </ol> </li> </ul>"},{"location":"course_notes/cs188/lecture/Particle%20Filtering/#summary","title":"Summary","text":"<ul> <li>Markov models, which encode time-dependent random variables that possess the Markov property. We can compute a belief distribution at any timestep of our choice for a Markov model using probabilistic inference with the mini-forward algorithm.</li> <li>Hidden Markov Models, which are Markov models with the additional property that new evidence which can affect our belief distribution can be observed at each timestep. To compute the belief distribution at any given timestep with Hidden Markov Models, we use the forward algorithm.</li> </ul> <p>Sometimes, running exact inference on these models can be too computationally expensive, in which case we can use particle filtering as a method of approximate inference.</p>"},{"location":"course_notes/cs188/lecture/Probability/","title":"Probability","text":""},{"location":"course_notes/cs188/lecture/Probability/#probability-rundown","title":"Probability Rundown","text":"<p>Here we provide a brief summary of probability rules we will be using.</p> <p>A random variable represents an event whose outcome is uncertain. A probability distribution is  an assignment of weights to outcomes. Probability distributions must satisfy the following properties:</p> \\[ \\begin{aligned} &amp; 0 \\leq P(\\omega) \\leq 1 \\\\ &amp; \\sum_\\omega P(\\omega)=1 \\end{aligned} \\] <p>We use the notion \\(P(A, B, C)\\) to denote the joint distribution of the variables \\(A\\), \\(B\\), \\(C\\). In joint distributions ordering does not matter i.e. \\(P(A, B, C) = P(C, B, A)\\). </p> <p>We can expand a joint distribution using the chain rule:</p> \\[ \\begin{aligned} &amp; P(A, B)=P(A \\mid B) P(B)=P(B \\mid A) P(A) \\\\ &amp; P\\left(A_1, A_2 \\ldots A_k\\right)=P\\left(A_1\\right) P\\left(A_2 \\mid A_1\\right) \\ldots P\\left(A_k \\mid A_1 \\ldots A_{k-1}\\right) \\end{aligned} \\] <p>The marginal distribution of \\(A, B\\) can be obtained by summing out all possible values that variable \\(C\\) can take as \\(P(A, B) = \\sum_c P(A, B, C = c)\\). The marginal distribution of \\(A\\) can also be obtained as \\(P(A) = \\sum_b \\sum_c P(A, B = b, C = c)\\).</p> <p>When we do operations on probability distributions, sometimes we get distributions that do not necessarily sum to 1. To fix this, we normalize: take the sum of all entries in the distribution and divide each entry by this sum.</p> <p>Conditional probabilities assign probabilities to events conditioned on some known facts.</p> \\[ P(A \\mid B)=\\frac{P(A, B)}{P(B)} \\] <p>Bayes' Rule:</p> \\[ P(A \\mid B)=\\frac{P(B \\mid A) P(A)}{P(B)} \\] <p>To write that random variables \\(A\\) and \\(B\\) are mutually independent, we write \\(A  \\newcommand{\\indep}{\\perp \\!\\!\\! \\perp} \\indep B\\), equivalent to \\(B \\indep A\\). </p>"},{"location":"course_notes/cs188/lecture/Probability/#probability-inference","title":"Probability Inference","text":"<p>For the next several weeks, we will use a new model where each possible state for the world has its own probability. More precisely, our model is a joint distribution, i.e. a table of probabilities which captures the likelihood of each possible outcome, also known as an assignment of variables.</p>"},{"location":"course_notes/cs188/lecture/Probability/#inference-by-enumeration","title":"Inference by Enumeration","text":"<p>Given a joint PDF, we can trivially compute any desired probability distribution \\(P\\left(Q_1 \\ldots Q_m \\mid e_1 \\ldots e_n\\right)\\) using inference by enumeration, for which we define three types of variables:</p> <ol> <li>Query variables \\(Q_i\\), which are unknown and appear on the left side of the conditional (\\(\\mid\\)) in the     desired probability distribution.</li> <li>Evidence variables \\(e_i\\), which are observed variables whose values are known and appear on the right side of    the conditional (\\(\\mid\\)) in the desired probability distribution.</li> <li>Hidden variables, which are values presents in the overall joint distribution but not in the desired    distribution.</li> </ol> <p>In Inference By Enumeration, we follow the following algorithm:</p> <ol> <li>Collect all the rows consistent with the observed evidence variables.</li> <li>Sum out (marginalize) all the hidden variables.</li> <li>Normalize the table so that it is a probability distribution (i.e. values sum to 1)</li> </ol>"},{"location":"course_notes/cs188/lecture/RL/","title":"Reinforcement Learning","text":"<p>Solving Markov decision processes is an example of offline planning, where the agents have  full knowledge of both the transition function and the reward function, all the information they need to precompute optimal actions in the world encoded by the MDP without ever actually taking any actions.</p> <p>Abstract</p> <p>In this note, we will discuss online planning, during which an agent has no prior knowledge of rewards or transitions in the world. In online planning, the agent must try exploration, during which it performs actions and receive feedback in the form of the successor states it arrives and the corresponding rewards it reaps.</p> <p>The agent uses this feedback to estimate an optimal policy through a process known as reinforcement learning  before using this estimated policy for exploitation or reward maximization.</p> <p></p> <ul> <li>Each \\(\\left(s, a, s^{\\prime}, r\\right)\\) tuple is known as a sample: an agent in a state \\(s\\) takes an action \\(a\\) and ends up in a successor state \\(s^{\\prime}\\), attaining a reward \\(r\\).</li> <li>Often, an agent continues to take actions and collect samples in succession until arriving at a terminal state. Such a collection of samples is known as an episode.</li> </ul> <p>There are two types of reinforcement learning: model-based learning and model-free learning.</p>"},{"location":"course_notes/cs188/lecture/RL/#model-based-learning","title":"Model-Based Learning","text":"<p>Model-based learning attempts to estimate the transition and reward functions with the samples attained during exploration before using these estimates to solve the MDP normally with value or policy iteration.</p> <ul> <li>Step 1: Learn empirical MDP model<ul> <li>Count outcomes s' for each s, a</li> <li>Normalize to give an estimate of \\(\\widehat{T}\\left(s, a, s^{\\prime}\\right)\\)</li> <li>Discover each \\(\\widehat{R}\\left(s, a, s^{\\prime}\\right)\\) when we experience \\(\\left(\\mathrm{s}, \\mathrm{a}, \\mathrm{s}^{\\prime}\\right)\\)</li> </ul> </li> <li>Step 2: Solve the learned MDP<ul> <li>For example, use value iteration, as before</li> </ul> </li> </ul> <p>By the law of large numbers, as we collect more samples by having our agent experience more episodes, our models of \\(\\hat{T}\\) and \\(\\hat{R}\\) will improve.</p>"},{"location":"course_notes/cs188/lecture/RL/#model-free-learning","title":"Model-Free Learning","text":"<p>Model-free learning attempts to estimate the values or Q-values of states directly, without ever using any memory to construct a model of the rewards or transitions in the MDP.</p> <p>Direct evaluation and temporal difference learning fall under a class of algorithms known as passive reinforcement learning, where the agent is given a policy to follow and learns the value of states under that policy as it experiences episodes. </p> <p>Q-learning falls under a second class of model-free learning algorithms known as active reinforcement learning, where the can use the feedback it receives to iteratively update its policy while learning until eventually determining the optimal policy after sufficient exploration.</p>"},{"location":"course_notes/cs188/lecture/RL/#direct-evaluation","title":"Direct Evaluation","text":"<p>Idea</p> <p>Fix some policy \\(\\pi\\) and have the agent experience several episodes while following \\(\\pi\\). </p> <p>As the agent collects samples through these episodes it maintains counts of the total utility obtained from each state and the number of times it visited each state. At any point, we can compute the estimated value of any state \\(s\\) by dividing the total utility obtained from \\(s\\) by the number of times the agent visited \\(s\\).</p> <p>Direct evaluation is often unnecessary slow to converge because it wastes information about transitions between states.</p>"},{"location":"course_notes/cs188/lecture/RL/#temporal-difference-learning","title":"Temporal Difference Learning","text":"<p>Idea</p> <p>Learning from every experience.</p> <p>In policy evaluation, we used the system of equations generated by our fixed policy and the Bellman equation to determine the values of states under that policy:</p> \\[ V^\\pi(s)=\\sum_{s^{\\prime}} T\\left(s, \\pi(s), s^{\\prime}\\right)\\left[R\\left(s, \\pi(s), s^{\\prime}\\right)+\\gamma V^\\pi\\left(s^{\\prime}\\right)\\right] \\] <p>TD learning tries to answer the question of how to compute this weighted average without the weights, cleverly doing so with an exponential moving average.</p> <p>We begin by initializing \\(\\forall s, V^\\pi(s)=0\\). At each time step, an agent takes an action \\(\\pi(s)\\) from a state \\(s\\), transitions to a state \\(s^{\\prime}\\), and receives a reward \\(R\\left(s, \\pi(s), s^{\\prime}\\right)\\). We can obtain a sample value by summing the received reward with the discounted current value of \\(s^{\\prime}\\) under \\(\\pi\\) : $$ \\text { sample }=R\\left(s, \\pi(s), s^{\\prime}\\right)+\\gamma V^\\pi\\left(s^{\\prime}\\right) $$</p> <p>This sample is a new estimate for \\(V^\\pi(s)\\). The next step is to incorporate this sampled estimate into our existing model for \\(V^\\pi(s)\\) with the exponential moving average:</p> \\[ V^\\pi(s) \\leftarrow(1-\\alpha) V^\\pi(s)+\\alpha \\cdot \\text { sample } \\] <p>Above, \\(\\alpha\\) is a parameter constrained by \\(0 \\leq \\alpha \\leq 1\\) known as the learning rate that specifies the weight we want to assign our existing model for \\(V^\\pi(s), 1-\\alpha\\), and the weight we want to assign our new sampled estimate, \\(\\alpha\\). It's typical to start out with learning rate of \\(\\alpha=1\\), accordingly assigning \\(V^\\pi(s)\\) to whatever the first sample happens to be, and slowly shrinking it towards 0 , at which point all subsequent samples will be zeroed out and stop affecting our model of \\(V^\\pi(s)\\).</p> <p>Annotating the state of our model at different points in time:</p> \\[ V_k^\\pi(s) \\leftarrow(1-\\alpha) V_{k-1}^\\pi(s)+\\alpha \\cdot \\text { sample }_k \\] <p>This recursive definition for \\(V_k^\\pi(s)\\) happens to be very interesting:</p> \\[ \\begin{aligned} V_k^\\pi(s) &amp; \\leftarrow(1-\\alpha) V_{k-1}^\\pi(s)+\\alpha \\cdot \\text { sample }_k \\\\ V_k^\\pi(s) &amp; \\leftarrow(1-\\alpha)\\left[(1-\\alpha) V_{k-2}^\\pi(s)+\\alpha \\cdot \\text { sample }_{k-1}\\right]+\\alpha \\cdot \\text { sample }_k \\\\ \\vdots &amp; \\\\ V_k^\\pi(s) &amp; \\leftarrow \\alpha \\cdot\\left[(1-\\alpha)^{k-1} \\cdot \\text { sample }_1+\\ldots+(1-\\alpha) \\cdot \\text { sample }_{k-1}+\\text { sample }_k\\right] \\end{aligned} \\] <p>This means that older samples are given exponentially less weight, exactly what we want since these older samples are computed using older (and hence worse) version of our model for \\(V^\\pi(s)\\).</p> <p>With a single straightforward update rule, we are able to</p> <ul> <li>Learn at every time step</li> <li>Give exponentially less weight to older, less accurate samples</li> <li>Converge to learning true state values much faster</li> </ul>"},{"location":"course_notes/cs188/lecture/RL/#q-learning","title":"Q-Learning","text":"<p>As a result, TD learning or direct evaluation are typically used in tandem with some model-based learning to acquire estimates of T and R in order to effectively update the policy followed by the learning agent.</p> <p>A revolutionary new idea: Q-learning</p> <p>Learning the Q-values of states directly, bypassing the need to ever know any values, transition functions, or reward functions.</p> <p>As a result, Q-learning is entirely model-free. Q-learning uses the following update rule to perform what's known as \\(\\mathbf{Q}\\)-value iteration: $$ Q_{k+1}(s, a) \\leftarrow \\sum_{s^{\\prime}} T\\left(s, a, s^{\\prime}\\right)\\left[R\\left(s, a, s^{\\prime}\\right)+\\gamma \\max _{a^{\\prime}} Q_k\\left(s^{\\prime}, a^{\\prime}\\right)\\right] $$</p> <p>With this new update rule under our belt, Q-learning is derived essentially the same way as TD learning, by acquiring \\(\\mathbf{Q}\\)-value samples:</p> \\[ \\text { sample }=R\\left(s, a, s^{\\prime}\\right)+\\gamma \\max _{a^{\\prime}} Q\\left(s^{\\prime}, a^{\\prime}\\right) \\] <p>and incorporating them into an exponential moving average.</p> \\[ Q(s, a) \\leftarrow(1-\\alpha) Q(s, a)+\\alpha \\cdot \\text { sample } \\] <p>Q-learning can learn the optimal policy directly even by taking suboptimal or random actions. This is called off-policy learning (contrary to direct evaluation and TD learning, which are examples of on-policy learning).</p>"},{"location":"course_notes/cs188/lecture/RL/#approximate-q-learning","title":"Approximate Q-Learning","text":"<p>Above, if Pacman learned that Figure 1 is unfavorable after running vanilla Q-learning, it would still have no idea that Figure 2 or even Figure 3 are unfavorable as well. Approximate Q-learning tries to account for this by learning about a few general situations and extrapolating to many similar situations. The key to generalizing learning experiences is the feature-based representation of states, which represents each state as a vector known as a feature vector. For example, a feature vector for Pacman may encode</p> <ul> <li>the distance to the closest ghost.</li> <li>the distance to the closest food pellet.</li> <li>the number of ghosts.</li> <li>is Pacman trapped? 0 or 1</li> </ul> <p>With feature vectors, we can treat values of states and Q-states as linear value functions:</p> \\[ \\begin{aligned} V(s) &amp; =w_1 \\cdot f_1(s)+w_2 \\cdot f_2(s)+\\ldots+w_n \\cdot f_n(s)=\\vec{w} \\cdot \\vec{f}(s) \\\\ Q(s, a) &amp; =w_1 \\cdot f_1(s, a)+w_2 \\cdot f_2(s, a)+\\ldots+w_n \\cdot f_n(s, a)=\\vec{w} \\cdot \\vec{f}(s, a) \\end{aligned} \\] <p>Defining difference as</p> \\[ \\text { difference }=\\left[R\\left(s, a, s^{\\prime}\\right)+\\gamma \\max _{a^{\\prime}} Q\\left(s^{\\prime}, a^{\\prime}\\right)\\right]-Q(s, a) \\] <p>approximate Q-learning works almost identically to Q-learning, using the following update rule:</p> \\[ w_i \\leftarrow w_i+\\alpha \\cdot \\text { difference } \\cdot f_i(s, a) \\] <p>Rather than storing Q-values for each and every state, with approximate Q-learning we only need to store a single weight vector and can compute Q-values on-demand as needed. As a result, this gives us not only a more generalized version of Q-learning, but a significantly more memory-efficient one as well.</p> <p>As a final note on Q-learning, we can reexpress the update rule for exact Q-learning using difference as follows:</p> \\[ Q(s, a) \\leftarrow Q(s, a)+\\alpha \\cdot \\text { difference } \\] <p>This second notation gives us a slightly different but equally valuable interpretation of the update: it's computing the difference between the sampled estimated and the current model of \\(Q(s, a)\\), and shifting the model in the direction of the estimate with the magnitude of the shift being proportional to the magnitude of the difference.</p>"},{"location":"course_notes/cs188/lecture/RL/#exploration-and-exploitation","title":"Exploration and Exploitation","text":"<p>We\u2019ll discuss two methods for distributing time between exploration and exploitation: \\(\\mathcal{\\varepsilon}\\)-greedy policies and exploration functions.</p>"},{"location":"course_notes/cs188/lecture/RL/#mathcalvarepsilon-greedy-policies","title":"\\(\\mathcal{\\varepsilon}\\)-Greedy Policies","text":"<p>Note</p> <p>Agents following \\(\\varepsilon\\)-greedy policy define probability \\(0 \\leq \\varepsilon \\leq 1\\), and act randomly and explore with probability \\(\\varepsilon\\). They follow current established policy and exploit with probability \\((1-\\varepsilon)\\).</p> <p>Difficult to handle: \\(\\varepsilon\\) must be manually tuned.</p>"},{"location":"course_notes/cs188/lecture/RL/#exploration-functions","title":"Exploration Functions","text":"<p>This issue of manually tuning \\(\\varepsilon\\) is avoided by exploration functions:</p> \\[ Q(s, a) \\leftarrow(1-\\alpha) Q(s, a)+\\alpha \\cdot\\left[R\\left(s, a, s^{\\prime}\\right)+\\gamma \\max _{a^{\\prime}} f\\left(s^{\\prime}, a^{\\prime}\\right)\\right] \\] <p>where \\(f\\) denotes an exploration function. There exists some degree of flexibility in designing an exploration function, a common choice is:</p> \\[ f(s, a)=Q(s, a)+\\frac{k}{N(s, a)} \\] <p>with \\(k\\) being some predetermined value, and \\(N(s, a)\\) denoting the number of times Q-state \\((s, a)\\) has been visited. Agents in a state \\(s\\) always select the action that has the highest \\(f(s, a)\\) from each state, and hence never have to make a probabilistic decision between exploration and exploitation. Instead, exploration is automatically encoded by the exploration function, since the term \\(\\frac{k}{N(s, a)}\\) can give enough of a \"bonus\" to some infrequently-taken action such that it is selected over actions with higher Q-values.</p>"},{"location":"course_notes/cs188/lecture/RL/#summary","title":"Summary","text":"<p>It\u2019s very important to remember</p> <p>Reinforcement learning has an underlying MDP, and the goal of reinforcement learning is to solve this MDP by deriving an optimal policy. The difference between using reinforcement learning and using methods like value iteration and policy iteration is the lack of knowledge of the transition function T and the reward function R for the underlying MDP. As a result, agents must learn the optimal policy through online trial-by-error rather than pure offline computation. There are many ways to do this:</p> <ul> <li>Model-based learning: Runs computations to estimate the values of the transition function \\(T\\) and the reward function \\(R\\) and uses MDP-solving methods like value or policy iteration with these estimates.</li> <li>Model-free learning: Avoids estimating \\(T\\) and \\(R\\)<ul> <li>On-policy learning: learn the values for a specific policy before deciding whether that policy is suboptimal and needs to be updated.<ul> <li>Direct evaluation: follows a policy \\(\\pi\\) and simply counts total rewards reaped from each state and the total number of times each state is visited - slow and wasting information about transitions.</li> <li>Temporal difference learning: follows a policy \\(\\pi\\) and uses an exponential moving average with sample values until convergence to the true state values under \\(\\pi\\).</li> </ul> </li> <li>Off-policy learning: learn an optimal policy even when taking suboptimal or random actions.<ul> <li>Q-learning: learns the optimal policy directly through trial and error with Q-value iteration updates.</li> <li>Approximate Q-learning: does the same thing as Q-learning but uses a feature-based representation for states to generalize learning.</li> </ul> </li> </ul> </li> </ul>"},{"location":"course_notes/cs188/lecture/Search-with-Other-Agents/","title":"Search with Other Agents","text":""},{"location":"course_notes/cs188/lecture/Search-with-Other-Agents/#games","title":"Games","text":"<p>Games(Adversarial Search Problems): our agents have one or more adversaries who attempt to keep them from reaching their goal(s).</p> <p>The standard game formulation consists of the following definitions:</p> <ul> <li>Initial state, \\(s_0\\)</li> <li>Players, \\(Player(s)\\) denote whose turn it is in state \\(s\\)</li> <li>Action, \\(Actions(s)\\) denote the available actions for the player</li> <li>Transition model, \\(Result(s, a)\\) denote the state that results from taking action \\(a\\) in state \\(s\\)</li> <li>Terminal test, \\(Terminal - test(s)\\)</li> <li>Terminal value, \\(Utility(s, player)\\)</li> </ul>"},{"location":"course_notes/cs188/lecture/Search-with-Other-Agents/#minimax","title":"Minimax","text":"<p>A state's value is defined as the best possible outcome (utility) an agent can achieve from that state.</p> <ul> <li>The value of a terminal state is called a terminal utility.</li> </ul> <p>Defining \\(V(s)\\) as the function defining the value of state \\(s\\):</p> \\[ \\begin{gathered} \\forall \\text { non-terminal states, } V(s)=\\max _{s^{\\prime} \\in \\operatorname{successors}(s)} V\\left(s^{\\prime}\\right) \\\\ \\forall \\text { terminal states, } \\quad V(s)=\\operatorname{known} \\end{gathered} \\]"},{"location":"course_notes/cs188/lecture/Search-with-Other-Agents/#example-pacman","title":"Example: Pacman","text":"<p>There is a ghost that wants to keep Pacman from eating the pellet.</p> <p></p> <p>The rules of the game dictate that the two agents take turns making moves, leading to a game tree where the two agents switch off on layers of the tree that they \"control\". An agent having control over a node simply means that node corresponds to a state where it is that agent\u2019s turn, and so it\u2019s their opportunity to decide upon an action and change the game state accordingly. Here\u2019s the game tree that arises from the new two-agent game board above:</p> <p></p> <p>The minimax algorithm only maximizes over the children of nodes controlled by Pacman, while minimizing over the children of nodes controlled by ghosts.</p> \\[ \\begin{aligned} &amp; \\forall \\text { agent-controlled states, } \\quad V(s)=\\max _{s^{\\prime} \\in \\operatorname{successors(s)}} V\\left(s^{\\prime}\\right) \\\\ &amp; \\forall \\text { opponent-controlled states, } \\quad V(s)=\\min _{s^{\\prime} \\in \\text { successors }(s)} V\\left(s^{\\prime}\\right) \\\\ &amp; \\forall \\text { terminal states, } V(s)=\\text { known } \\\\ &amp; \\end{aligned} \\] <p>In implementation, minimax performs a post-order traversal of the game tree.</p> <p></p>"},{"location":"course_notes/cs188/lecture/Search-with-Other-Agents/#alpha-beta-pruning","title":"Alpha-Beta Pruning","text":"<p>Issue</p> <p>The time complexity of minimax is \\(O(b^m)\\), where \\(b\\) is the branching factor and \\(m\\) is the approximate tree depth at which terminal nodes can be found.</p> <p>To help mitigate this issue, minimax has an optimization - alpha-beta pruning.</p> <p>Conceptually, alpha-beta pruning is this: if you\u2019re trying to determine the value of a node n by looking at its successors, stop looking as soon as you know that n\u2019s value can at best equal the optimal value of n\u2019s parent.</p> <p></p> <p>Implementing such pruning can reduce our runtime to as good as \\(O(b^{m/2})\\).</p> <p></p>"},{"location":"course_notes/cs188/lecture/Search-with-Other-Agents/#evaluation-functions","title":"Evaluation Functions","text":"<p>Alpha-beta pruning still isn't enough to get to the bottom of search trees for a large majority of games. As a result, we turn to evaluation functions, functions that take in a state and output an estimate of the true minimax value of that node.</p> <p>Depth-Limited Minimax</p> <p>We treat non-terminal nodes located at our maximum solvable depth as terminal nodes, giving them mock terminal utilities as determined by a carefully selected evaluation function. Because evaluation functions can only yield estimates of the values of non-terminal utilities, this removes the guarantee of optimal play when running minimax.</p> <ul> <li>The better the evaluation function is, the closer the agent will come to behaving optimally.</li> <li>Going deeper into the tree before using an evaluation function also tends to give us better results.</li> </ul> <p>The most common design for evaluation functions is a linear combination of features.</p> \\[ \\operatorname{Eval}(s)=w_1 f_1(s)+w_2 f_2(s)+\\ldots+w_n f_n(s) \\] <p>Each \\(f_i(s)\\) corresponds to a feature extracted from the input state \\(s\\), and each feature is assigned a corresponding weight \\(w_i\\).</p> <p>Features are simply some element of a game state that we can extract and assign a numerical value.</p>"},{"location":"course_notes/cs188/lecture/Search-with-Other-Agents/#expectimax","title":"Expectimax","text":"<p>Expectimax introduces chance nodes into the game tree, which instead of considering the worst case scenario as minimizer nodes do, consider the average case. Chance nodes compute the expected utility or  expected value.</p> \\[ \\begin{aligned} \\forall \\text { agent-controlled states, } \\quad V(s) &amp; =\\max _{s^{\\prime} \\in \\text { successors }(s)} V\\left(s^{\\prime}\\right) \\\\ \\forall \\text { chance states, } \\quad V(s) &amp; =\\sum_{s^{\\prime} \\in \\text { successors }(s)} p\\left(s^{\\prime} \\mid s\\right) V\\left(s^{\\prime}\\right) \\\\ \\forall \\text { terminal states, } \\quad V(s) &amp; =\\text { known } \\end{aligned} \\] <p></p> <p></p>"},{"location":"course_notes/cs188/lecture/Search-with-Other-Agents/#monte-carlo-tree-search","title":"Monte Carlo Tree Search","text":"<p>MCTS is based on two ideas:</p> <ul> <li>Evaluation by rollouts: From state \\(s\\) play many times using a policy (e.g. random) and count wins/losses.</li> <li>Selective search: explore parts of the tree, without constraints on the horizon, that will improve decision at the root.</li> </ul>"},{"location":"course_notes/cs188/lecture/Search-with-Other-Agents/#mcts-version-0","title":"MCTS Version 0","text":"<p>Allocated the same amount of simulations to each alternative action:</p> <p></p>"},{"location":"course_notes/cs188/lecture/Search-with-Other-Agents/#mcts-version-09","title":"MCTS Version 0.9","text":"<p>It might become clear after a few simulations that a certain action does not return many wins and thus we might choose to allocate this computational effort in doing more simulations for the other actions. </p> <p>Allocate rollouts to more promising nodes:</p> <p></p>"},{"location":"course_notes/cs188/lecture/Search-with-Other-Agents/#mcts-version-10","title":"MCTS Version 1.0","text":"<ul> <li>Allocate rollouts to more promising nodes</li> <li>Allocate rollouts to more uncertain nodes</li> </ul>"},{"location":"course_notes/cs188/lecture/Search-with-Other-Agents/#upper-confidence-bounds-ucb-heuristics","title":"Upper Confidence Bounds (UCB) heuristics","text":"<p>The UCB algorithm captures this trade-off between \u201cpromising\" and \u201cuncertain\u2019 actions by using the following criterion at each node \\(n\\):</p> \\[ U C B 1(n)=\\frac{U(n)}{N(n)}+C \\times \\sqrt{\\frac{\\log N(\\operatorname{PARENT}(n))}{N(n)}} \\] <ul> <li>\\(N(n)=\\) number of rollouts from node \\(n\\)</li> <li>\\(U(n)=\\) total utility of rollouts (# wins) for player of \\(\\operatorname{PARENT}(n)\\)</li> </ul> <p>The first term captures how promising the node is, while the second captures how uncertain we are about that node\u2019s utility. The user-specified parameter C balances the weight we put in the two terms (\"exploration\" and \"exploitation\") and depends on the application and perhaps the stage of the task.</p> <p>The MCTS UCT algorithm uses the UCB criterion in tree search problem.</p> <ul> <li>Selection: recursively apply UCB to choose a path down to a leaf node n</li> <li>Expansion: add a new child c to n</li> <li>Simulation: run a rollout from c</li> <li>Backpropagation: update U and N counts from c back up to the root</li> </ul>"},{"location":"course_notes/cs188/lecture/Search-with-Other-Agents/#general-game","title":"General Game","text":"<p>Multi-agent utilities: rather than being a single value that alternating agents try to minimize or maximize, are represented as tuples with different values within the tuple corresponding to unique utilities for different agents.</p> <p>Each agent attempts to maximize their own utility at each node they control, ignoring the utilities of the other agents.</p> <p></p>"},{"location":"course_notes/cs188/lecture/Search-with-Other-Agents/#summary","title":"Summary","text":"<p>In this lecture, we shifted gears from standard search problems to adversarial search problems. Two primary algorithms were considered:</p> <ul> <li>Minimax: Used when our opponent(s) behaves optimally, and can be optimized using \\(\\alpha-\\beta\\) pruning.</li> <li>Expectimax: Used when we facing a suboptimal opponent(s), using a probability distribution over the moves we believe they will make to compute the expectated value of states.</li> </ul> <p>In most cases, it\u2019s too computationally expensive to run the above algorithms -&gt; evaluation functions for early termination.</p> <p>For problems with large branching factors -&gt; MCTS and UCB algorithms.</p> <p>Finally, we considered the problem of general games, where the rules are not necessarily zero-sum.</p>"},{"location":"course_notes/cs188/lecture/Uninformed-Search/","title":"Uninformed Search","text":""},{"location":"course_notes/cs188/lecture/Uninformed-Search/#state-spaces-and-search-problems","title":"State Spaces and Search Problems","text":"<p>search problem</p> <p>Given our agent's current state, how can we arrive a new state that satisfies its goal in the best possible way?</p> <p>A search problem consists of:</p> <ul> <li>A state space: The set of all possible states that are possible in your given world.</li> <li>A set of actions available in each state.</li> <li>A transition model: Outputs the next state when   a specific action is taken in a specific state.</li> <li>An action cost</li> <li>A start state</li> <li>A goal test: A function that takes a state as input,   and determines whether it is a goal state.</li> </ul>"},{"location":"course_notes/cs188/lecture/Uninformed-Search/#state-space-size","title":"State Space Size","text":"<p>fundamental counting principle</p> <p>If there are \\(n\\) variable objects in a given world which can take on \\(x_1\\), \\(x_2\\), ..., \\(x_n\\) values respectively, then the total number of states is \\(x_1 \\times x_2 \\times \\cdots \\times x_n\\).</p>"},{"location":"course_notes/cs188/lecture/Uninformed-Search/#state-space-graph-and-search-tree","title":"State Space Graph and Search Tree","text":"<ul> <li>A state space graph is constructed with states representing nodes, with directed edges existing from a state to its children. These edges represent actions, and any associated weights represent the cost of performing the corresponding action.</li> <li>Search trees: entire path(or plan) from the start state to the given state in the state space graph.</li> </ul>"},{"location":"course_notes/cs188/lecture/Uninformed-Search/#uninformed-search_1","title":"Uninformed-Search","text":"<p>Note</p> <p>When we have no knowledge of the location of goal states in our search tree, we are forced to select our strategy for tree search from one of the techniques that falls under the umbrella of uninformed search.</p> <p>Some rudimentary properties:</p> <ul> <li>completeness: if there exists a solution, is the strategy guaranteed to find it   given infinite computational resources?</li> <li>optimality: is the strategy guaranteed to find the   lowest-cost path to a goal state?</li> <li>branching factor \\(b\\): The increase in the number of   nodes on the frontier, each time a frontier node is expanded is \\(O(b)\\).</li> <li>At depth \\(k\\) in the search tree, there are \\(O(b^k)\\) nodes.</li> <li>The maximum depth \\(m\\).</li> <li>The depth of the shallowest solution \\(s\\).</li> </ul>"},{"location":"course_notes/cs188/lecture/Uninformed-Search/#depth-first-search","title":"Depth-First Search","text":"<ul> <li>Description: Always selects the deepest frontier node   from the start node for expansion.</li> <li>Frontier Representation: To implement DFS, we require   a structure that always gives the most recently added   objects highest priority. A (LIFO) stack does this.</li> <li>Completeness: DFS is not complete.</li> <li>Optimality: DFS is not optimal.</li> <li>Time Complexity: \\(O(b^m)\\)</li> <li>Space Complexity: \\(O(bm)\\)</li> </ul>"},{"location":"course_notes/cs188/lecture/Uninformed-Search/#breadth-first-search","title":"Breadth-First Search","text":"<ul> <li>Description: Always selects the shallowest frontier   node from the start node for expansion.</li> <li>Frontier Representation: We desire a structure that   outputs the oldest enqueued object to represent our frontier - a FIFO queue.</li> <li>Completeness: BFS is complete.</li> <li>Optimality: BFS is generally not optimal.</li> <li>Time Complexity: \\(O(b^s)\\)</li> <li>Space Complexity: \\(O(b^s)\\)</li> </ul>"},{"location":"course_notes/cs188/lecture/Uninformed-Search/#uniform-cost-search","title":"Uniform-Cost Search","text":"<ul> <li>Description: Always selects the lowest-cost frontier   node from the start node for expansion.</li> <li>Frontier Representation: A heap-based priority queue.</li> <li>Completeness: UCS is complete.</li> <li>Optimality: UCS is optimal if all costs are non-negative.</li> <li>Time Complexity: \\(O(b^{C^*/\\epsilon})\\)<ul> <li>\\(C^*\\) is the cost of the optimal solution.</li> <li>\\(\\epsilon\\) is the minimum cost of any action.</li> </ul> </li> <li>Space Complexity: \\(O(b^{C^*/\\epsilon})\\)</li> </ul>"},{"location":"course_notes/cs188/project/Reinforcement%20Learning/","title":"Project 3: Reinforcement Learning","text":""},{"location":"course_notes/cs188/project/Reinforcement%20Learning/#question-1-5-points-value-iteration","title":"Question 1 (5 points): Value Iteration","text":"<p>\u6839\u636eThe Bellman Equation\uff0c\u5b9e\u73b0Value Iteration\u7b97\u6cd5\u3002</p> <pre><code>    def runValueIteration(self):\n        for _ in range(self.iterations):\n            new_values = util.Counter()\n            for state in self.mdp.getStates():\n                if self.mdp.isTerminal(state):\n                    new_values[state] = 0\n                else:\n                    max_q_value = float('-inf')\n                    for action in self.mdp.getPossibleActions(state):\n                        q_value = self.computeQValueFromValues(state, action)\n                        max_q_value = max(max_q_value, q_value)\n                    new_values[state] = max_q_value\n            self.values = new_values\n</code></pre> <p><code>util.Counter()</code>\u662fdictionary\uff0c\u5b58\u50a8state-value pairs\u3002</p> <p>\u8bf4\u597d\u7684iterate\u76f4\u5230\u6536\u655b\u5462\uff1f\u8fd9\u91cc\u6307\u5b9a\u4e86iteration\u7684\u6b21\u6570\uff0c\u5927\u6982\u6709 \u51e0\u4e2a\u539f\u56e0\uff1a\u8282\u7701\u8ba1\u7b97\u8d44\u6e90\u3001\u65f6\u95f4\u3002</p> <pre><code>    def computeQValueFromValues(self, state, action):\n        \"\"\"\n          Compute the Q-value of action in state from the\n          value function stored in self.values.\n        \"\"\"\n        q_value = 0\n        for next_state, prob in self.mdp.getTransitionStatesAndProbs(state, action):\n            reward = self.mdp.getReward(state, action, next_state)\n            q_value += prob * (reward + self.discount * self.values[next_state])\n        return q_value\n\n    def computeActionFromValues(self, state):\n        \"\"\"\n          The policy is the best action in the given state\n          according to the values currently stored in self.values.\n\n          You may break ties any way you see fit.  Note that if\n          there are no legal actions, which is the case at the\n          terminal state, you should return None.\n        \"\"\"\n        if self.mdp.isTerminal(state):\n            return None\n        best_action = None\n        best_q_value = float('-inf')\n        for action in self.mdp.getPossibleActions(state):\n            q_value = self.computeQValueFromValues(state, action)\n            if q_value &gt; best_q_value:\n                best_q_value = q_value\n                best_action = action\n        return best_action\n</code></pre> <p>\u4ece\u6570\u5b66\u516c\u5f0f\u5c31\u53ef\u4ee5\u7406\u89e3<code>computeQValueFromValues</code>\u548c<code>computeActionFromValues</code>\u7684\u5b9e\u73b0\u3002</p>"},{"location":"course_notes/cs188/project/Reinforcement%20Learning/#question-2-1-point-bridge-crossing-analysis","title":"Question 2 (1 point): Bridge Crossing Analysis","text":""},{"location":"course_notes/cs188/project/Reinforcement%20Learning/#question-3-6-points-policies","title":"Question 3 (6 points): Policies","text":""},{"location":"course_notes/cs188/project/Reinforcement%20Learning/#question-4-1-points-prioritized-sweeping-value-iterationextra-credit","title":"Question 4 (1 points): Prioritized Sweeping Value Iteration[Extra Credit]","text":""},{"location":"course_notes/cs188/project/Reinforcement%20Learning/#question-5-5-points-q-learning","title":"Question 5 (5 points): Q-Learning","text":""},{"location":"course_notes/cs188/project/Reinforcement%20Learning/#question-6-2-points-epsilon-greedy","title":"Question 6 (2 points): Epsilon Greedy","text":""},{"location":"course_notes/cs188/project/Reinforcement%20Learning/#question-7-1-point-bridge-crossing-revisited","title":"Question 7 (1 point): Bridge Crossing Revisited","text":""},{"location":"course_notes/cs188/project/Reinforcement%20Learning/#question-8-1-point-q-learning-and-pacman","title":"Question 8 (1 point): Q-Learning and Pacman","text":""},{"location":"course_notes/cs188/project/Reinforcement%20Learning/#question-9-4-points-approximate-q-learning","title":"Question 9 (4 points): Approximate Q-Learning","text":""},{"location":"course_notes/deep-learning/cnn/","title":"Convolutional Neural Networks","text":""},{"location":"course_notes/deep-learning/cnn/#convolution-operation","title":"Convolution Operation","text":""},{"location":"course_notes/deep-learning/cnn/#padding","title":"Padding","text":"<p>As padding size increases, convolution gets more information, especially at the edges.</p> <p>The attributes of padding:</p> <ol> <li>Enable the acquisition of more information.</li> <li>Increase the resolution of the output.</li> </ol>"},{"location":"course_notes/deep-learning/cnn/#stride","title":"Stride","text":"<p>Stride refers to the number of steps the filter moves each time.</p> <p>The attributes of stride:</p> <ol> <li>Allow the compression of partial information.</li> <li>Reduce the resolution of the output.</li> </ol>"},{"location":"course_notes/deep-learning/cnn/#pooling","title":"Pooling","text":"<p>Pooling is a widely used technique that mainly focuses on reducing the resolution of the output.</p> <p></p>"},{"location":"course_notes/deep-learning/cnn/#the-calculation-of-output-size","title":"The Calculation of Output Size","text":"<p>Define: * image size: \\((H, W)\\) * filter size: \\((FH, FW)\\) * padding: \\(P\\) * stride: \\(S\\)</p> <p>Then the output size: \\((OH, OW)\\)</p> \\[ \\begin{align*}     OH = \\frac{H + 2P - FH}{S} + 1 \\\\     OW = \\frac{W + 2P - FW}{S} + 1 \\end{align*} \\]"},{"location":"course_notes/deep-learning/lrnn/","title":"Logistic Regression as a Neural Network","text":""},{"location":"course_notes/deep-learning/lrnn/#binary-classification","title":"Binary Classification","text":"<p>\u5728\u4e8c\u5206\u7c7b\u95ee\u9898\u4e2d\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u9884\u6d4b\u51fa\u4e00\u4e2a\u4e8c\u503c\u7684\u8f93\u51fa\uff0c\u5373 \\(y\\) \u7684\u503c\u4e3a 0 \u6216 1\u3002\u4f8b\u5982\uff0c\u6211\u4eec\u8981\u9884\u6d4b\u4e00\u5f20\u56fe\u7247\u4e2d\u662f\u5426\u5305\u542b\u732b\uff0c\u90a3\u4e48 \\(y\\) \u7684\u503c\u5c31\u662f 0 \u6216 1\u3002</p> <p></p> <p>\u5728\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e2d\uff0c\u9996\u5148\u8981\u5c06\u56fe\u7247\u8f93\u5165x\uff08\u7ef4\u5ea6\u662f\\((64, 64, 3)\\)\uff09\u8f6c\u5316\u4e3a\u4e00\u7ef4\u7684\u7279\u5f81\u5411\u91cf\uff08feature vector\uff09\u3002\u65b9\u6cd5\u662f\u6bcf\u4e2a\u901a\u9053\u4e00\u884c\u4e00\u884c\u53d6\uff0c\u518d\u8fde\u63a5\u8d77\u6765\u3002\u5219\u8f6c\u5316\u540e\u7684\u8f93\u5165\u7279\u5f81\u5411\u91cf\u7ef4\u5ea6\u4e3a\\((12288, 1)\\)\u3002\u6b64\u7279\u5f81\u5411\u91cf\\(x\\)\u662f\u5217\u5411\u91cf\uff0c\u7ef4\u5ea6\u4e00\u822c\u8bb0\u4e3a\\(n_x\\)</p> <p>\u5982\u679c\u8bad\u7ec3\u6837\u672c\u5171\u6709\\(m\\)\u5f20\u56fe\u7247\uff0c\u90a3\u4e48\u6574\u4e2a\u8bad\u7ec3\u6837\u672c\\(X\\)\u7ec4\u6210\u4e86\u77e9\u9635\uff0c\u7ef4\u5ea6\u4e3a\\((n_x,m)\\)\u3002\\(Y\\)\u662f\u4e00\u4e2a\u884c\u5411\u91cf\uff0c\u7ef4\u5ea6\u4e3a\\((1,m)\\)\u3002</p>"},{"location":"course_notes/deep-learning/lrnn/#logistic-regression","title":"Logistic Regression","text":"<p>\u903b\u8f91\u56de\u5f52\u6a21\u578b\u4e00\u822c\u7528\u6765\u89e3\u51b3\u4e8c\u5206\u7c7b\uff08Binary Classification\uff09\u95ee\u9898\u3002</p> \\[ \\text{Given} \\ x, \\quad \\text{predict} \\quad \\hat{y} = P(y=1|x) \\quad \\text{where} \\quad 0 \\leq \\hat{y} \\leq 1 \\] <ul> <li> <p>The input features vector: \\(x \\in \\mathbb{R}^{n_x}\\), where \\(n_x\\) is the number of features.</p> </li> <li> <p>The training label: \\(y \\in \\{0, 1\\}\\).</p> </li> <li>The weights: \\(w \\in \\mathbb{R}^{n_x}\\), where \\(n_x\\) is the number of features.</li> <li>The bias: \\(b \\in \\mathbb{R}\\).</li> </ul> <p>The output of the logistic regression is: $$ \\hat{y} = \\sigma(w^T x + b) $$ where \\(\\sigma\\) is the sigmoid function:</p> \\[ \\sigma(z) = \\frac{1}{1 + e^{-z}} \\] <p></p> <p>Sigmoid function\u5c06 \\(\\hat{y}\\) \u7684\u503c\u9650\u5236\u57280\u52301\u4e4b\u95f4\u3002</p>"},{"location":"course_notes/deep-learning/lrnn/#logistic-regression-cost-function","title":"Logistic Regression Cost Function","text":"<p>Lost Function (aka. Error Function) \u7528\u6765\u8861\u91cf\u6a21\u578b\u7684\u597d\u574f\u3002\u635f\u5931\u51fd\u6570\u9700\u8981\u662f\u51f8\u51fd\u6570\uff0c\u7528\u4e8e\u540e\u7eed\u4f18\u5316\uff0c\u56e0\u6b64\u50cf\\(L(\\hat{y}, y) =  \\frac{1}{2}(\\hat{y} - y)^2\\)\u8fd9\u6837\u7684\u51fd\u6570\u5c31\u4e0d\u9002\u5408\u3002\u8fd9\u91cc\u7ed9\u51fa\u903b\u8f91\u56de\u5f52\u7684\u635f\u5931\u51fd\u6570\uff1a</p> \\[ L(\\hat{y}, y) = -(y \\log \\hat{y} + (1-y) \\log (1-\\hat{y})) \\] <p>Cost Function \u662f\u6240\u6709\u8bad\u7ec3\u6837\u672c\u7684\u5e73\u5747\u635f\u5931\u51fd\u6570\uff0c\u5373\u5bf9\u7ed9\u5b9a\u7684\\(w\\)\u548c\\(b\\)\uff0c\\(\\hat{y}^{(1)}, \\hat{y}^{(2)}, \\cdots, \\hat{y}^{(m)}\\)\u4e0e \\(y^{(1)}, y^{(2)}, \\cdots, y^{(m)}\\)\u8bef\u5dee\u7684\u5e73\u5747\u503c\uff1a</p> \\[ J(w, b) = \\frac{1}{m} \\sum_{i=1}^m L(\\hat{y}^{(i)}, y^{(i)}) \\] <p>Loss Function\u9488\u5bf9\u5355\u4e2a\u6837\u672c\uff0cCost Function\u9488\u5bf9\u6574\u4e2a\u8bad\u7ec3\u96c6\u3002</p>"},{"location":"course_notes/deep-learning/lrnn/#gradient-descent","title":"Gradient Descent","text":"<p>Gradient Descent\u7684\u76ee\u6807\u662f\u627e\u5230\u4f7f\u5f97Cost Function\u6700\u5c0f\u7684\\(w\\)\u548c\\(b\\)\u3002\u4e3a\u4e86\u627e\u5230\u6700\u5c0f\u503c\uff0c\u6211\u4eec\u53ef\u4ee5\u5bf9\\(w\\)\u548c\\(b\\)\u521d\u59cb\u5316\u4efb\u610f\u503c\uff0c\u7136\u540e\u4e0d\u65ad\u8fed\u4ee3\uff0c\u6bcf\u6b21\u8fed\u4ee3\u90fd\u671d\u7740\u68af\u5ea6\u7684\u53cd\u65b9\u5411\u79fb\u52a8\u4e00\u5c0f\u6b65\uff0c\u76f4\u5230\u8fbe\u5230\u6700\u5c0f\u503c\u3002</p> \\[ \\begin{align*} w := w - \\alpha \\frac{\\partial J(w, b)}{\\partial w}\\\\ b := b - \\alpha \\frac{\\partial J(w, b)}{\\partial b} \\end{align*} \\] <p>\u5176\u4e2d\\(\\alpha\\)\u662f\u5b66\u4e60\u7387\uff08learning rate\uff09\uff0c\u7528\u6765\u63a7\u5236\u6bcf\u6b21\u8fed\u4ee3\u7684\u6b65\u957f\u3002</p>"},{"location":"course_notes/deep-learning/lrnn/#logistic-regression-gradient-descent","title":"Logistic Regression Gradient Descent","text":"<p>\u5047\u8bbe\u8f93\u5165\u6570\u636ex\u7684\u7ef4\u5ea6n=2\uff0c\u5219\u8ba1\u7b97\u6d41\u7a0b\u5982\u4e0b: </p> <p>\u8fd9\u91cc\u7684 \\(a\\) \u662f \\(\\hat{y}\\) \u3002</p> <p>\u6574\u4e2a\u4ece\u53c2\u6570\\(w\\), \\(b\\)\u63a8\u5bfc\u5230\u635f\u5931\u51fd\u6570\u503c\u7684\u8fc7\u7a0b\u88ab\u79f0\u4e3a\u6b63\u5411\u4f20\u64ad\uff08forward propagation\uff09\u3002\u800c\u4ece\u635f\u5931\u51fd\u6570\u503c\u53cd\u5411\u63a8\u5bfc\u5230\u53c2\u6570\\(w\\), \\(b\\)\u7684\u8fc7\u7a0b\u88ab\u79f0\u4e3a\u53cd\u5411\u4f20\u64ad\uff08backward propagation\uff09\u3002</p>"},{"location":"course_notes/deep-learning/lrnn/#gradient-descent-on-m-examples","title":"Gradient Descent on m Examples","text":"<p>\u8fed\u4ee3\u7b97\u6cd5be like\uff1a</p> <pre><code>J = 0, dw_1 = 0, dw_2 = 0, ..., dw_n = 0, db = 0\n# iterate over all training examples\nfor i in range(m):\n    # forward propagation\n    z_i = w^T x_i + b\n    a_i = sigmoid(z_i)\n    J += -[y_i log(a_i) + (1-y_i) log(1-a_i)]\n    # backward propagation\n    dz_i = a_i - y_i\n    for j in range(n):\n        dw_j += x_j dz_i\n    db += dz_i\n# update parameters\nJ /= m\ndw_1 /= m, dw_2 /= m, ..., dw_n /= m, db /= m\n</code></pre> <p>\u73b0\u5728\u53ef\u4ee5\u7ed3\u5408\u5b66\u4e60\u7387\\(\\alpha\\)\u6765\u66f4\u65b0\u53c2\u6570\u4e86\uff1a</p> <pre><code>for j in range(n):\n    w_j = w_j - alpha dw_j\n\nb = b - alpha db\n</code></pre> <p>\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u5f80\u5f80\u662f\u975e\u5e38\u5927\u7684\uff0c\u56e0\u6b64for loop\u5f88\u6162\u3002\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5411\u91cf\u5316\uff08vectorization\uff09\u6765\u52a0\u901f\u8ba1\u7b97\u3002</p>"},{"location":"course_notes/ic/mos-physic/","title":"MOS\u5668\u4ef6\u7269\u7406","text":""},{"location":"course_notes/ic/mos-physic/#general-considerations","title":"General considerations","text":""},{"location":"course_notes/ic/mos-physic/#mos-symbols","title":"MOS symbols","text":""},{"location":"course_notes/ic/mos-physic/#mos-as-a-switch","title":"MOS as a switch","text":"<p>Need to know the threshold voltage \\(V_{th}\\).</p>"},{"location":"course_notes/ic/mos-physic/#mos-as-a-vccs","title":"MOS as a VCCS","text":"<p>MOS is also a voltage-controlled current source (VCCS).</p> <p>Need to derive the I/V characteristics.</p>"},{"location":"course_notes/ic/mos-physic/#mos-iv-characteristics","title":"MOS I/V characteristics","text":""},{"location":"course_notes/ic/mos-physic/#threshold-voltage","title":"Threshold voltage","text":"<p>When \\(V_{G} = 0\\), the MOS is off.</p> <p>When \\(V_{G}\\) increases to a sufficiently positive value, the surface is \"inverted\" and the MOS is on.</p> <p>The value of \\(V_{G}\\) at this point is called the threshold voltage \\(V_{th}\\). If \\(V_{G}\\) rises further, the charge density in channel continues to increase (\\(I_D\\) increases).</p> \\[ V_{TH} = \\Phi_{MS} + 2\\Phi_{F} + \\frac{Q_{dep}}{C_{ox}} \\\\  \\] \\[ C_{ox} = \\frac{\\epsilon_{ox}}{t_{ox}} \\] <ul> <li>\\(\\Phi_{MS}\\): work function difference between the gate and substrate</li> <li>\\(\\Phi_{F}\\): Fermi level</li> <li>\\(Q_{dep}\\): Charge density in the depletion region</li> <li>\\(C_{ox}\\): Gate oxide capacitance per unit area</li> </ul>"},{"location":"course_notes/ic/mos-physic/#important-note-about-v_th","title":"Important note about \\(V_{th}\\)","text":"<p>For NMOS</p> <p>the higher the substrate doping, the higher the \\(V_{th}\\).</p> <p>For a given process under given temperature, \\(V_{th}\\) is almost constant.</p>"},{"location":"course_notes/ic/mos-physic/#iv-characteristics-in-triode-region","title":"I/V characteristics in triode region","text":"\\[ I_D = \\mu_n C_{ox} \\frac{W}{L} \\left[ (V_{GS} - V_{TH})V_{DS} - \\frac{V_{DS}^2}{2} \\right], \\quad V_{DS} \\le V_{GS} - V_{TH} \\] <p>Observations: \\(I_D \\propto W\\)</p> <p>In triode region</p> <p>\\(I_D\\) is influenced by both \\(V_{GS}\\) and \\(V_{DS}\\).</p> <p>\\(W/L\\) is called \"aspect ratio\", a design parameter.</p> <p>\\(\\mu_n C_{ox}\\) is a process parameter.</p> <p></p>"},{"location":"course_notes/ic/mos-physic/#application-of-nmos-in-triode-region","title":"Application of NMOS in Triode region","text":"\\[ I_D \\approx \\mu_n C_{ox} \\frac{W}{L} (V_{GS} - V_{TH})V_{DS}, \\quad if \\ \\ \\frac{V_{DS}}{2} \\ll V_{GS} - V_{TH} \\] \\[ R_{on} = \\frac{V_{DS}}{I_D} = \\frac{1}{\\mu_n C_{ox} \\frac{W}{L} (V_{GS} - V_{TH})} \\] <p>\u6df1\u7ebf\u6027\u533a\uff08\\(V_{DS}\\)\u975e\u5e38\u5c0f\uff09\uff0cMOS\u7ba1\u53ef\u4ee5\u5f53\u6210\u538b\u63a7\u7535\u963b\uff0c\u6ce8\u610fMOS\u7ba1\u662f\u538b\u63a7\u5668\u4ef6</p>"},{"location":"course_notes/ic/mos-physic/#iv-characteristics-in-saturation-region","title":"I/V characteristics in saturation region","text":"\\[ I_D = \\frac{1}{2} \\mu_n C_{ox} \\frac{W}{L} (V_{GS} - V_{TH})^2 = \\frac{1}{2} \\mu_n C_{ox} \\frac{W}{L} (V_{OD})^2, \\quad V_{DS} \\ge V_{GS} - V_{TH}  \\] <p>\\(V_{OD} = V_{GS} - V_{TH}\\) is called \"overdrive voltage\".</p> \\[ V_{OD} = \\sqrt{\\frac{2I_D}{\\mu_n C_{ox} \\frac{W}{L}}} \\] <p></p>"},{"location":"course_notes/ic/mos-physic/#application-of-mos-in-saturation-region","title":"Application of MOS in saturation region","text":"<p>\u5f53\\(V_{DS}\\)\u8f83\u5927\u65f6\uff0c\\(I_D\\)\u4e0d\u4f1a\u968f\\(V_{DS}\\)\u53d8\u5316\uff08\u975e\u5e38\u7f13\u6162\uff09\uff0c\u56e0\u6b64\uff1a</p> <p>Saturated MOS can be used as voltage-controlled current source (VCCS).</p>"},{"location":"course_notes/ic/mos-physic/#nmos-iv-characteristics-summary","title":"NMOS I/V characteristics summary","text":""},{"location":"course_notes/ic/mos-small-signal/","title":"MOS\u5c0f\u4fe1\u53f7\u6a21\u578b","text":""},{"location":"course_notes/ic/mos-small-signal/#notations","title":"Notations","text":""},{"location":"course_notes/ic/mos-small-signal/#understanding-the-concept-of-small-signal-model","title":"Understanding the concept of small signal model","text":""},{"location":"course_notes/ic/mos-small-signal/#transient-response","title":"Transient response","text":"\\[ v_{in} = v_a \\sin \\omega t \\] \\[ v_{out} = -i_dR_d = -\\mu_n C_{ox} \\frac{W}{L} (V_{GS} - V_{TH})v_a \\sin \\omega t \\] <p>Amplification factor: \\(\\mu_n C_{ox} \\frac{W}{L} (V_{GS} - V_{TH})\\)</p>"},{"location":"course_notes/ic/mos-small-signal/#transconductance-of-mos","title":"Transconductance of MOS","text":"\\[ g_m = \\mu_n C_{ox} \\frac{W}{L} (V_{GS} - V_{TH}) = \\sqrt{2 \\mu_n C_{ox} \\frac{W}{L} I_D} = \\frac{2I_D}{V_{GS}- V_{TH}} \\]"},{"location":"course_notes/ic/mos-small-signal/#three-elements-for-calculating-g_m","title":"Three elements for calculating \\(g_m\\)","text":"<ul> <li>There are only two independent elements in \\(W/L\\), \\(I_D\\) and \\(V_{GS} - V_{TH}\\).</li> <li>Any one can be derived from the other two.</li> </ul>"},{"location":"course_notes/ic/mos-small-signal/#ideal-small-signal-model-of-mos","title":"Ideal small signal model of MOS","text":""},{"location":"course_notes/ic/mos-small-signal/#second-order-effect","title":"Second order effect","text":""},{"location":"course_notes/ic/mos-small-signal/#body-effect","title":"Body effect","text":"<p>\u5f53\\(V_{S}\\)\u5927\u4e8e\\(V_{B}\\)\u65f6\uff0c\u4f1a\u53d1\u751f\u4ec0\u4e48\uff1f\\(V_{TH}\\)\u53d8\u5927\u3002</p> \\[ V_{T H}=V_{T H 0}+\\gamma\\left(\\sqrt{\\left|2 \\Phi_F+V_{S B}\\right|}-\\sqrt{\\left|2 \\Phi_F\\right|}\\right) \\] <p>\\(\\gamma\\) is called the body effect coefficient, \u8ddf\u5de5\u827a\u6709\u5173\u3002</p>"},{"location":"course_notes/ic/mos-small-signal/#influence-of-body-effect-on-small-signal-model-g_mb","title":"Influence of body effect on small signal model: \\(g_{mb}\\)","text":"<p>\\(g_{mb}\\): body transconductance</p> \\[ g_{mb} = \\frac{\\partial I_D}{\\partial V_{SB}}  \\]"},{"location":"course_notes/ic/mos-small-signal/#channel-length-modulation-effect","title":"Channel length modulation effect","text":"<ul> <li>A NMOS operating in the saturation region with large enough length</li> <li>\\(I_D\\) is independent of \\(V_{DS}\\).</li> <li>MOS\u6210\u4e3a\u7406\u60f3\u7535\u6d41\u6e90\u3002</li> <li>Infinite output resistance(\u56e0\u4e3a\u4e24\u7aef\u7535\u538b\u53d8\u5316\u540e\uff0c\u7535\u6d41\u5374\u4e0d\u53d8)</li> </ul> <p>The influence of \\(V_{DS}\\) on \\(I_D\\) is called channel length modulation effect.</p> \\[ I_D \\approx \\frac{1}{2} \\mu_n C_{o x} \\frac{W}{L}\\left(V_{G S}-V_{T H}\\right)^2\\left(1+\\lambda V_{D S}\\right) \\] <p>\u6c9f\u9053\u957f\u5ea6\u8c03\u5236\u6548\u5e94\u7684\u7ed3\u679c\u662f\uff0c\u5373\u4f7f\u5728\u9971\u548c\u533a\uff0c\u6f0f\u6781\u7535\u6d41\uff08\\(I_D\\)\uff09\u4f9d\u7136\u8f7b\u5fae\u4f9d\u8d56\u4e8e\u6f0f\u6781\u7535\u538b\uff08\\(V_{DS}\\)\uff09</p> <p>\u5b9a\u4e49finite output resistance \\(r_o\\), \\(g_{ds} = \\frac{1}{r_o}\\)</p> \\[ r_{o}=\\frac{1}{\\lambda I_{D}} \\] <p>Important note:</p> <p>MOS small-signal output resistance can be changed by adjusting bias current \\(I_D\\), or transistor length \\(L\\).</p>"},{"location":"course_notes/ic/single-stage-amp/","title":"Single Stage Amplifiers","text":""},{"location":"course_notes/ic/single-stage-amp/#ideal-amplifier","title":"Ideal amplifier","text":"<ul> <li>Linear</li> <li>Infinite input resistance</li> <li>Infinite driving capability</li> </ul> \\[ v_{out}= A_v v_{in} \\]"},{"location":"course_notes/ic/single-stage-amp/#common-source-amplifier","title":"Common source amplifier","text":"<p>A single-transistor amplifier with the source connected to AC ground.</p>"},{"location":"course_notes/ic/single-stage-amp/#cs-stage-with-resistive-load","title":"CS-stage with resistive load","text":"<ul> <li> <p>\\(V_{in} &lt; V_{TH}\\): cut off, \\(V_{out} = V_{DD}\\)</p> </li> <li> <p>\\(V_{in} - V_{TH} &lt; V_{out}\\), saturation, \\(V_{out} = V_{DD} - I_dR_D\\)</p> </li> <li> <p>\\(V_{in} - V_{TH} &gt; V_{out}\\), linear</p> </li> </ul> \\[ A_v=\\frac{\\partial V_{o u t}}{\\partial V_{i n}}=-\\mu_n C_{o x} \\frac{W}{L}\\left(V_{i n}-V_{T H}\\right) \\cdot R_D \\] \\[ A_v = -g_m R_D \\]"},{"location":"course_notes/mitos/lab/cow/","title":"Copy-on-Write Fork","text":"<p>The Problem</p> <p>The <code>fork()</code> system call in xv6 copies all of the parent process's user-space memory into the child. If the parent is large, copying can take a long time. Worse, the work is often largely wasted; for example, a <code>fork()</code> followed by <code>exec()</code> in the child will cause the child to discard the copied memory, probably without ever using most of it. On the other hand, if both parent and child use a page, and one or both writes it, a copy is truly needed.</p> <p>The solution</p> <p>The goal of copy-on-write (COW) <code>fork()</code> is to defer allocating and copying physical memory pages for the child until the copies are actually needed, if ever.</p> <p>COW <code>fork()</code> creates just a pagetable for the child, with PTEs for user memory pointing to the parent's physical pages. COW <code>fork()</code> marks all the user PTEs in both parent and child as not writable. When either process tries to write one of these COW pages, the CPU will force a page fault. The kernel page-fault handler detects this case, allocates a page of physical memory for the faulting process, copies the original page into the new page, and modifies the relevant PTE in the faulting process to refer to the new page, this time with the PTE marked writeable. When the page fault handler returns, the user process will be able to write its copy of the page.</p> <p>COW <code>fork()</code> makes freeing of the physical pages that implement user memory a little trickier. A given physical page may be referred to by multiple processes' page tables, and should be freed only when the last reference disappears.</p>"},{"location":"course_notes/mitos/lab/cow/#implement-copy-on-write","title":"Implement copy-on write","text":"<p>Modify <code>uvmcopy()</code> to map the parent's physical pages into the child, instead of allocating new pages. Clear <code>PTE_W</code> in the PTEs of both child and parent.</p> vm.c, uvmcopy()<pre><code>int\nuvmcopy(pagetable_t old, pagetable_t new, uint64 sz)\n{\n  pte_t *pte;\n  uint64 pa, i;\n  uint flags;\n\n  for(i = 0; i &lt; sz; i += PGSIZE){\n    if((pte = walk(old, i, 0)) == 0)\n      panic(\"uvmcopy: pte should exist\");\n    if((*pte &amp; PTE_V) == 0)\n      panic(\"uvmcopy: page not present\");\n    // clear PTE_W in both old and new page tables\n    *pte = *pte &amp; ~PTE_W;\n    pa = PTE2PA(*pte);\n    flags = PTE_FLAGS(*pte);\n    // map the parent's physical pages into the child\n    if(mappages(new, i, PGSIZE, pa, flags) != 0)\n      goto err;\n  }\n  return 0;\n\n err:\n  uvmunmap(new, 0, i / PGSIZE, 1);\n  return -1;\n}\n</code></pre> <p>Modify <code>usertrap()</code> to recognize page faults. When a page-fault occurs on a COW page, allocate a new page with <code>kalloc()</code>, copy the old page to the new page, and install the new page in the PTE with <code>PTE_W</code> set.</p> <p>\u53c2\u8003 lazy\uff0c\u7528\u51fd\u6570<code>needcow</code>\u5224\u65ad\u662f\u5426\u7b26\u5408 COW \u7684\u6761\u4ef6\u3002</p> vm.c, needcow<pre><code>int\nneedcow(struct proc *p, uint64 va)\n{\n  // Kill a process if it page-faults on a virtual memory address\n  // higher than any allocated with sbrk().\n  if (va &gt;= p-&gt;sz)\n    return 0;\n\n  // Handle faults on the invalid page below the user stack.\n  // should not touch guard page below stack\n  if (PGROUNDDOWN(va) &lt;= p-&gt;trapframe-&gt;sp)\n    return 0;\n\n  pte_t *pte = walk(p-&gt;pagetable, va, 0);\n  if (pte == 0 || (*pte &amp; PTE_V) == 0)\n    return 0;\n\n  return (*pte &amp; PTE_COW) != 0;\n}\n</code></pre> <p>Ensure that each physical page is freed when the last PTE reference to it goes away -- but not before. A good way to do this is to keep, for each physical page, a \"reference count\" of the number of user page tables that refer to that page. Set a page's reference count to one when <code>kalloc()</code> allocates it. Increment a page's reference count when <code>fork</code> causes a child to share the page, and decrement a page's count each time any process drops the page from its page table. <code>kfree()</code> should only place a page back on the free list if its reference count is zero. It's OK to to keep these counts in a fixed-size array of integers. You'll have to work out a scheme for how to index the array and how to choose its size. For example, you could index the array with the page's physical address divided by 4096, and give the array a number of elements equal to highest physical address of any page placed on the free list by <code>kinit()</code> in <code>kalloc.c</code>.</p>"},{"location":"course_notes/mitos/lab/fs/","title":"file system","text":"<p>In this lab you will add large files and symbolic links to the xv6 file system.</p>"},{"location":"course_notes/mitos/lab/fs/#large-files","title":"Large files","text":"<p>Task</p> <ul> <li>Modify <code>bmap()</code> so that it implements a doubly-indirect block, in addition to direct blocks and a singly-indirect block.</li> <li>The first 11 elements of <code>ip-&gt;addrs[]</code> should be direct blocks; the 12th should be a singly-indirect block, the 13th should be your new doubly-indirect block.</li> </ul> <p>\u9996\u5148\uff0c\u4fee\u6539<code>inode</code>(disk and memory)\u7684<code>addrs</code>\uff0c\u52a0\u5165double indirect block\u3002</p> fs.h<pre><code>#define NDIRECT 11\n#define NINDIRECT (BSIZE / sizeof(uint))\n#define NDOUBLEINDIRECT (NINDIRECT * NINDIRECT)\n#define INDEX_DOUBLEINDIRECT (NDIRECT + 1)\n#define MAXFILE (NDIRECT + NINDIRECT + NDOUBLEINDIRECT)\n\n// On-disk inode structure\nstruct dinode {\n  /* other stuff... */\n\n  uint addrs[NDIRECT+2];   // Data block addresses\n};\n</code></pre> file.h<pre><code>// in-memory copy of an inode\nstruct inode {\n  /* other stuff... */\n\n  uint addrs[NDIRECT+2];\n};\n</code></pre> <p>\u63a5\u4e0b\u6765\u4fee\u6539<code>bmap()</code>\uff0c\u5b9e\u73b0double indirect block\u7684\u5206\u914d\u548c\u8bfb\u53d6\u3002</p> fs.c<pre><code>static uint\nbmap(struct inode *ip, uint bn)\n{\n  uint addr, *a;\n  struct buf *bp;\n\n  if(bn &lt; NDIRECT){\n    // Load direct block, allocating if necessary...\n  }\n  bn -= NDIRECT;\n\n  if(bn &lt; NINDIRECT){\n    // Load indirect block, allocating if necessary...\n  }\n  bn -= NINDIRECT;\n\n  if (bn &lt; NDOUBLEINDIRECT) {\n    // Load double indirect block, allocating if necessary.\n    if((addr = ip-&gt;addrs[INDEX_DOUBLEINDIRECT]) == 0)\n      ip-&gt;addrs[INDEX_DOUBLEINDIRECT] = addr = balloc(ip-&gt;dev);\n    bp = bread(ip-&gt;dev, addr);\n    a = (uint*)bp-&gt;data;\n    uint double_indirect_block = a[bn / NINDIRECT];\n    if (double_indirect_block == 0) {\n      a[bn / NINDIRECT] = double_indirect_block = balloc(ip-&gt;dev);\n      log_write(bp);\n    }\n    brelse(bp);\n\n    // Load indirect block, allocating if necessary.\n    bp = bread(ip-&gt;dev, double_indirect_block);\n    a = (uint*)bp-&gt;data;\n    if((addr = a[bn % NINDIRECT]) == 0){\n      a[bn % NINDIRECT] = addr = balloc(ip-&gt;dev);\n      log_write(bp);\n    }\n    brelse(bp);\n    return addr;\n  }\n\n  panic(\"bmap: out of range\");\n}\n</code></pre> <p>\u63a5\u7740\u4fee\u6539<code>itrunc()</code>\uff0c\u5b9e\u73b0double indirect block\u7684\u91ca\u653e\u3002</p> fs.c<pre><code>void\nitrunc(struct inode *ip)\n{\n  int i, j;\n  struct buf *bp;\n  uint *a;\n\n  for(i = 0; i &lt; NDIRECT; i++){\n    // Free direct blocks...\n  }\n\n  if(ip-&gt;addrs[NDIRECT]){\n    // Free indirect blocks...\n  }\n\n  // Free double indirect blocks\n  if (ip-&gt;addrs[INDEX_DOUBLEINDIRECT]) {\n    bp = bread(ip-&gt;dev, ip-&gt;addrs[INDEX_DOUBLEINDIRECT]);\n    a = (uint*)bp-&gt;data;\n    for (i = 0; i &lt; NINDIRECT; i++) {\n      if (a[i]) {\n        struct buf *bp2 = bread(ip-&gt;dev, a[i]);\n        uint *a2 = (uint*)bp2-&gt;data;\n        for (j = 0; j &lt; NINDIRECT; j++) {\n          if (a2[j]) {\n            bfree(ip-&gt;dev, a2[j]);\n          }\n        }\n        brelse(bp2);\n        bfree(ip-&gt;dev, a[i]);\n      }\n    }\n    brelse(bp);\n    bfree(ip-&gt;dev, ip-&gt;addrs[INDEX_DOUBLEINDIRECT]);\n    ip-&gt;addrs[INDEX_DOUBLEINDIRECT] = 0;\n  }\n\n  ip-&gt;size = 0;\n  iupdate(ip);\n}\n</code></pre> <p>\u6d4b\u8bd5\uff1a\u5728xv6\u4e2d\u8fd0\u884c<code>bigfile</code>\u548c<code>usertests</code>\uff0c\u901a\u8fc7\u2705\u3002</p>"},{"location":"course_notes/mitos/lab/fs/#symbolic-links","title":"Symbolic links","text":"<p>Info</p> <p>Symbolic links (or soft links) refer to a linked file by pathname; when a symbolic link is opened, the kernel follows the link to the referred file. Symbolic links resembles hard links, but hard links are restricted to pointing to file on the same disk, while symbolic links can cross disk devices.</p> <p>Task</p> <ul> <li>Implement the <code>symlink(char *target, char *path)</code> system call, which creates a new symbolic link at path that refers to file named by target.</li> </ul> <p>\u9996\u5148\uff0c\u66f4\u65b0<code>user/usys.pl, user/user.h, kernel/sysfile.c, Makefile</code>\u7b49\u6587\u4ef6\uff0c\u8ba9xv6\u80fd\u591f\u7f16\u8bd1\u5e76\u8fd0\u884c<code>symlinktest</code>\uff0c\u8fd9\u91cc\u4e0d\u8d58\u8ff0\u3002 \u53ea\u9700\u8981\u6ce8\u610f<code>O_NOFOLLOW</code>\u4e0d\u8981\u4e0e\u5176\u5b83flag\u51b2\u7a81\u5373\u53ef\u3002</p> <p>\u63a5\u4e0b\u6765\uff0cimplement the <code>symlink(target, path)</code> system call to create a new symbolic link at path that refers to target. \u7ec6\u8282\uff1a</p> <ul> <li>Note that target does not need to exist for the system call to succeed.</li> <li><code>symlink</code> should return an integer representing success (0) or failure (-1).</li> </ul> sysfile.c<pre><code>uint64\nsys_symlink(void)\n{\n  char target[MAXPATH], path[MAXPATH];\n  if (argstr(0, target, MAXPATH) &lt; 0 || argstr(1, path, MAXPATH) &lt; 0)\n    return -1;\n\n  begin_op();\n  struct inode *ip = create(path, T_SYMLINK, 0, 0);\n  if (ip == 0) {\n    end_op();\n    return -1;\n  }\n\n  if (writei(ip, 0, (uint64)target, 0, strlen(target)) != strlen(target)) {\n    end_op();\n    return -1;\n  }\n  iunlockput(ip);\n  end_op();\n\n  return 0;\n}\n</code></pre> <p>\u5728path\u521b\u5efa\u4e00\u4e2ainode\uff0c\u7136\u540e\u5c06target\u5199\u5165inode\u7684data block\u4e2d\u5373\u53ef\u3002</p> <p>\u56e0\u4e3a\u4e0a\u6587\u63d0\u5230\u7684\u7ec6\u8282\uff0c\u6240\u4ee5\u4e0d\u9700\u8981\u68c0\u67e5target\u662f\u5426\u5b58\u5728\u3002</p> <p>\u6700\u540e\uff0cmodify the <code>open</code> system call to handle the case where the path refers to a symbolic link.</p> sysfile.c<pre><code>uint64\nsys_open(void)\n{\n  char path[MAXPATH];\n  int fd, omode;\n  struct file *f;\n  struct inode *ip;\n  int n;\n\n  if((n = argstr(0, path, MAXPATH)) &lt; 0 || argint(1, &amp;omode) &lt; 0)\n    return -1;\n\n  begin_op();\n\n  if(omode &amp; O_CREATE){\n    ip = create(path, T_FILE, 0, 0);\n    if(ip == 0){\n      end_op();\n      return -1;\n    }\n  } else {\n    if((ip = namei(path)) == 0){\n      end_op();\n      return -1;\n    }\n    ilock(ip);\n    if(ip-&gt;type == T_DIR &amp;&amp; omode != O_RDONLY){\n      iunlockput(ip);\n      end_op();\n      return -1;\n    }\n    if (ip-&gt;type == T_SYMLINK &amp;&amp; !(omode &amp; O_NOFOLLOW)) {\n      if ((ip = followsymlink(ip, 0)) == 0) {\n        end_op();\n        return -1;\n      }\n    }\n  }\n\n  // other stuff...\n}\n</code></pre> <p>\u53ea\u9700\u8981\u4fee\u6539<code>omode &amp; O_CREATE</code>\u4e3a<code>false</code>\uff0c\u4e5f\u5c31\u662f\u8981\u641c\u7d22path\u5bf9\u5e94\u7684inode\u7684\u60c5\u51b5\u5373\u53ef\u3002\u6700\u7ec8\u76ee\u7684\u662f\u628asymbol link\u7684inode\u66ff\u6362\u4e3atarget\u5bf9\u5e94\u7684inode\u3002\u8fd9\u6837\u540e\u7eed\u7684\u4ee3\u7801\u5c31\u53ef\u4ee5\u6b63\u5e38\u8fd0\u884c\uff0c\u4e0d\u9700\u8981\u5173\u5fc3symbol link\u7684\u5b58\u5728\u3002</p> <p><code>if</code>\u4e2d\u7684\u5224\u65ad\u6761\u4ef6\uff1a</p> <ul> <li><code>ip-&gt;type == T_SYMLINK</code>: path\u5bf9\u5e94\u7684inode\u662fsymbol link</li> <li><code>!(omode &amp; O_NOFOLLOW)</code>: open\u7684\u8c03\u7528\u8005\u5e0c\u671bfollow symbol link\uff0c\u5f97\u5230target\u5bf9\u5e94\u7684inode</li> </ul> <p>\u6ee1\u8db3\u6761\u4ef6\u540e\uff0c\u8c03\u7528<code>followsymlink()</code>\uff1a</p> sysfile.c<pre><code>struct inode*\nfollowsymlink(struct inode *ip, uint depth)\n{\n  // prevent symlink circle\n  if (depth &gt; 10) {\n    iunlockput(ip);\n    return 0;\n  }\n\n  char buf[MAXPATH];\n  int n;\n  struct inode *next;\n\n  if (ip-&gt;type != T_SYMLINK)\n    return ip;\n\n  if ((n = readi(ip, 0, (uint64)buf, 0, MAXPATH)) &lt; 0) {\n    iunlockput(ip);\n    return 0;\n  }\n\n  if ((next = namei(buf)) == 0) {\n    iunlockput(ip);\n    return 0;\n  }\n\n  iunlockput(ip);\n  ilock(next);\n\n  return followsymlink(next, depth + 1);\n}\n</code></pre> <p>\u8be5\u51fd\u6570\u9012\u5f52\u76f4\u5230\u627e\u5230target\u5bf9\u5e94\u7684inode\uff08\u56e0\u4e3a\u6709\u53ef\u80fd\u51fa\u73b0symbol link chain\uff09\uff0c\u7136\u540e\u8fd4\u56de\u3002\u4e3a\u4e86\u9632\u6b62symbol link circle\uff0c\u8bbe\u7f6e\u4e86\u4e00\u4e2a<code>depth</code>\u7684\u4e0a\u9650\u3002</p> <p>\u6b64\u5916\uff0c\u9700\u8981\u975e\u5e38\u6ce8\u610flock\u3002\u4e3a\u4e86\u5951\u5408<code>sys_open</code>\uff0c<code>followsymlink</code>\u8fd4\u56de\u7684inode\u662flocked\u7684\uff0c\u4f46\u662f\u904d\u5386symbol link chain\u7684\u8fc7\u7a0b\u4e2d\u9700\u8981\u5c06\u4e2d\u95f4\u7684inode unlock\uff0c\u5426\u5219\u6709\u6b7b\u9501\u7684\u98ce\u9669\u3002\u5404\u79cdedge case\u63d0\u524d<code>return</code>\u7684\u65f6\u5019\uff0c\u5230\u5e95\u5e94\u8be5\u600e\u4e48\u5904\u7406\u9501\uff0c\u9700\u8981\u4ed4\u7ec6\u8003\u8651\u3002\u6211\u4e2a\u4eba\u82b1\u8d39\u4e86\u5f88\u591a\u65f6\u95f4\u5728\u9501\u7684\u95ee\u9898\u4e0a\u3002</p> <p>\u6700\u540e\uff0c\u8fd0\u884c<code>make grade</code>\uff0c\u5706\u6ee1\u7ed3\u675f\u3002</p> <pre><code>== Test running bigfile == \n$ make qemu-gdb\nrunning bigfile: OK (108.0s) \n== Test running symlinktest == \n$ make qemu-gdb\n(0.8s) \n== Test   symlinktest: symlinks == \n  symlinktest: symlinks: OK \n== Test   symlinktest: concurrent symlinks == \n  symlinktest: concurrent symlinks: OK \n== Test usertests == \n$ make qemu-gdb\nusertests: OK (176.4s) \n== Test time == \ntime: OK\nScore: 100/100\n</code></pre>"},{"location":"course_notes/mitos/lab/lazy/","title":"xv6 lazy page allocation","text":""},{"location":"course_notes/mitos/lab/lazy/#background","title":"Background","text":"<ul> <li><code>sbrk()</code> allocates physical memory and maps it into the process's virtual address space</li> <li>It can take a long time for a kernel to allocate and map memory for a large request.</li> <li>some programs allocate more memory than they actually use</li> </ul> <p>Solution</p> <p>Allocate user memory lazily</p> <p><code>sbrk()</code> doesn't allocate physical memory, but just remembers which user addresses are allocated and marks those addresses as invalid in the user page table. When the process first tries to use any given page of lazily-allocated memory, the CPU generates a page fault, which the kernel handles by allocating physical memory, zeroing it, and mapping it.</p>"},{"location":"course_notes/mitos/lab/lazy/#eliminate-allocation-from-sbrk","title":"Eliminate allocation from sbrk()","text":"<p>Task</p> <ul> <li>delete page allocation from the sbrk(n) system call implementation</li> <li>new <code>sbrk(n)</code> should just increment the process's size <code>(myproc()-&gt;sz)</code> by <code>n</code> and return the old size</li> <li>should not allocate memory</li> </ul> sysproc.c<pre><code>uint64\nsys_sbrk(void)\n{\n  int addr;\n  int n;\n  struct proc* p = myproc();\n\n  if(argint(0, &amp;n) &lt; 0)\n    return -1;\n  addr = p-&gt;sz;\n  if (n &gt;= 0) {\n    p-&gt;sz += n;\n  } else {\n    p-&gt;sz = uvmdealloc(p-&gt;pagetable, p-&gt;sz, p-&gt;sz + n);\n  }\n\n  return addr;\n}\n</code></pre> <p>\u5f53<code>n &lt; 0</code>\u65f6\uff0c\u9700\u8981\u8c03\u7528<code>uvmdealloc</code>\u91ca\u653e\u4e0d\u518d\u9700\u8981\u7684\u5185\u5b58\u9875\uff0c\u8ba9\u8fdb\u7a0b\u7684\u865a\u62df\u5185\u5b58\u7a7a\u95f4\u53d8\u5c0f\u3002</p> <p>\u4fee\u6539\u540e\uff0c\u5c1d\u8bd5\u5728 xv6 \u4e2d\u8fd0\u884c<code>echo hi</code>\uff0c\u4f1a\u62a5\u9519\uff1a</p> xv6<pre><code>init: starting sh\n$ echo hi\nusertrap(): unexpected scause 0x000000000000000f pid=3\n            sepc=0x0000000000001258 stval=0x0000000000004008\npanic: uvmunmap: not mapped\n</code></pre> <p></p> <ul> <li><code>scause=0x000000000000000f</code>\u8868\u793a Page fault</li> </ul> <p><code>sbrk</code>\u6ca1\u6709 allocate memory\uff0c\u8fd0\u884c<code>echo hi</code>\u7684\u65f6\u5019\u8bbf\u95ee\u4e86 invalid \u7684 virtual address\uff0c\u56e0\u6b64\u53d1\u751f\u4e86 page fault\u3002</p>"},{"location":"course_notes/mitos/lab/lazy/#lazy-allocation","title":"Lazy allocation","text":"<p>Task</p> <ul> <li>respond to a page fault from user space<ul> <li>mapping a newly-allocated page of physical memory at the faulting address</li> <li>returning back to user space to let the process continue executing</li> </ul> </li> <li>Modify whatever other xv6 kernel code you need to in order to get <code>echo hi</code> to work.</li> </ul> <p>check whether a fault is a page fault by seeing if <code>r_scause()</code> is 13 or 15 in <code>usertrap()</code></p> trap.c, usertrap<pre><code>  if(scause == 8){\n    // ...\n  } else if((which_dev = devintr()) != 0){\n    // ok\n  } else if ((scause == 13 || scause == 15) &amp;&amp; uvmshouldtouch(p, r_stval())) {\n    // page fault\n    uvmlazytouch(p, r_stval());\n  } else {\n    // ...\n  }\n</code></pre> <p>\u6211\u8fd9\u91cc\u7528\u51fd\u6570\u5c01\u88c5\u903b\u8f91\uff0c\u66f4\u7b80\u6d01\u3002</p> vm.c, uvmshouldtouch<pre><code>int\nuvmshouldtouch(struct proc *p, uint64 va)\n{\n  // Kill a process if it page-faults on a virtual memory address\n  // higher than any allocated with sbrk().\n  if (va &gt;= p-&gt;sz)\n    return 0;\n\n  // Handle faults on the invalid page below the user stack.\n  // should not touch guard page below stack\n  if (PGROUNDDOWN(va) &lt;= p-&gt;trapframe-&gt;sp)\n    return 0;\n\n  pte_t *pte = walk(p-&gt;pagetable, va, 0);\n  return pte == 0 || (*pte &amp; PTE_V) == 0;\n}\n</code></pre> <p>\u6839\u636e lazy\uff0c\u53ea\u6709\u5f53<code>va</code>\u5bf9\u5e94\u7684\u9875\u8868\u9879\u4e0d\u5b58\u5728\u6216\u8005\u5bf9\u5e94\u7684\u7269\u7406\u9875\u4e0d\u5b58\u5728\u65f6\uff0c\u624d\u9700\u8981\u5206\u914d\u7269\u7406\u9875\u3002</p> <ul> <li>Handle out-of-memory correctly: if <code>kalloc()</code> fails in the page fault handler, kill the current process.</li> </ul> vm.c, uvmlazytouch<pre><code>void\nuvmlazytouch(struct proc *p, uint64 va)\n{\n  va = PGROUNDDOWN(va);\n  char *mem = kalloc();\n  if (mem == 0) {\n    printf(\"lazy: out of memory\\n\");\n    p-&gt;killed = 1;\n  } else {\n    memset(mem, 0, PGSIZE);\n    // \u521b\u5efaPTE\uff0c\u5c06\u7269\u7406\u9875\u6620\u5c04\u5230\u865a\u62df\u5730\u5740\n    if (mappages(p-&gt;pagetable, va, PGSIZE, (uint64)mem, PTE_W | PTE_X | PTE_R | PTE_U) != 0) {\n      printf(\"lazy: failed to map page\\n\");\n      kfree(mem);\n      p-&gt;killed = 1;\n    }\n  }\n}\n</code></pre> <p>\u6b64\u65f6<code>echo hi</code>\u53ef\u4ee5\u6b63\u5e38\u8fd0\u884c\uff0c\u4f46\u662f\u5df2\u7ecf\u7834\u574f\u4e86 kernel\uff0c\u9700\u8981\u4fee\u590d\u3002</p> <p>Handle the parent-to-child memory copy in fork() correctly.</p> vm.c, uvmcopy<pre><code>int\nuvmcopy(pagetable_t old, pagetable_t new, uint64 sz)\n{\n  // ...\n  for(i = 0; i &lt; sz; i += PGSIZE){\n    if((pte = walk(old, i, 0)) == 0)\n      continue;\n    if((*pte &amp; PTE_V) == 0)\n      continue;\n    // ...\n  }\n  // ...\n}\n</code></pre> <p>\u5728 lazy \u7b56\u7565\u4e0b\uff0cpage table \u4e2d\u80af\u5b9a\u5b58\u5728 PTE \u4e0d\u5b58\u5728\u6216 invalid \u7684\u60c5\u51b5\uff0c\u5982\u679c\u68c0\u6d4b\u5230\u4e86 continue \u5373\u53ef\uff08\u4e4b\u524d\u662f\u76f4\u63a5 panic\uff09</p> <p>Handle the case in which a process passes a valid address from sbrk() to a system call such as read or write, but the memory for that address has not yet been allocated. \u9605\u8bfb\u4ee3\u7801\u540e\u53d1\u73b0\uff0c<code>read</code>\u4e0e<code>write</code>\u4f7f\u7528\u4e24\u4e2a\u51fd\u6570\u6765\u5904\u7406\u7528\u6237\u548c\u5185\u6838\u4e4b\u95f4\u7684\u6570\u636e\u4f20\u8f93\uff1a</p> <ul> <li><code>copyout</code>: Copy from kernel to user.</li> <li><code>copyin</code>: Copy from user to kernel.</li> </ul> <p>\u4e8e\u662f\u53ea\u9700\u8981\u5728\u8fd9\u4e24\u4e2a\u51fd\u6570\u4e2d\u5904\u7406 lazy page allocation \u5373\u53ef\u3002</p> vm.c, copyout<pre><code>int\ncopyout(pagetable_t pagetable, uint64 dstva, char *src, uint64 len)\n{\n  uint64 n, va0, pa0;\n  struct proc *p = myproc();\n\n  while(len &gt; 0){\n    va0 = PGROUNDDOWN(dstva);\n    // MY CODE BEGIN\n    if (uvmshouldtouch(p, va0)) {\n      uvmlazytouch(p, va0);\n    }\n    // END\n    pa0 = walkaddr(pagetable, va0);\n    if(pa0 == 0)\n      return -1;\n    n = PGSIZE - (dstva - va0);\n    if(n &gt; len)\n      n = len;\n    memmove((void *)(pa0 + (dstva - va0)), src, n);\n\n    len -= n;\n    src += n;\n    dstva = va0 + PGSIZE;\n  }\n  return 0;\n}\n</code></pre> vm.c, copyin<pre><code>int\ncopyin(pagetable_t pagetable, char *dst, uint64 srcva, uint64 len)\n{\n  uint64 n, va0, pa0;\n  struct proc *p = myproc();\n\n  while(len &gt; 0){\n    va0 = PGROUNDDOWN(srcva);\n    // MY CODE BEGIN\n      if (uvmshouldtouch(p, va0)) {\n      uvmlazytouch(p, va0);\n    }\n    // END\n    pa0 = walkaddr(pagetable, va0);\n    if(pa0 == 0)\n      return -1;\n    n = PGSIZE - (srcva - va0);\n    if(n &gt; len)\n      n = len;\n    memmove(dst, (void *)(pa0 + (srcva - va0)), n);\n\n    len -= n;\n    dst += n;\n    srcva = va0 + PGSIZE;\n  }\n  return 0;\n}\n</code></pre> <p>\u903b\u8f91\u90fd\u5dee\u4e0d\u591a\uff1a\u68c0\u67e5 user \u4f20\u5165\u7684 virtual address\uff0c\u5224\u65ad\u662f\u5426\u9700\u8981\u5206\u914d\u7269\u7406\u9875\u3002</p> <p>\u6700\u540e\u8fd0\u884c lazytests &amp; usertests\uff0chappy ending\uff01</p>"},{"location":"course_notes/mitos/lab/locks/","title":"locks","text":"<p>Lock contention</p> <p>A common symptom of poor parallelism on multi-core machines is high lock contention. Improving parallelism often involves changing both data structures and locking strategies in order to reduce contention.</p> <p>\u6765\u81ea\u7f51\u4e0a\u8001\u54e5\u7684\u535a\u5ba2\uff0c\u5199\u5f97\u5f88\u597d\uff1a</p> <p>\u9501\u7ade\u4e89\u4f18\u5316\u4e00\u822c\u6709\u51e0\u4e2a\u601d\u8def\uff1a</p> <ul> <li>\u53ea\u5728\u5fc5\u987b\u5171\u4eab\u7684\u65f6\u5019\u5171\u4eab\uff08\u5bf9\u5e94\u4e3a\u5c06\u8d44\u6e90\u4ece CPU \u5171\u4eab\u62c6\u5206\u4e3a\u6bcf\u4e2a CPU \u72ec\u7acb\uff09</li> <li>\u5fc5\u987b\u5171\u4eab\u65f6\uff0c\u5c3d\u91cf\u51cf\u5c11\u5728\u5173\u952e\u533a\u4e2d\u505c\u7559\u7684\u65f6\u95f4\uff08\u5bf9\u5e94\u201c\u5927\u9501\u5316\u5c0f\u9501\u201d\uff0c\u964d\u4f4e\u9501\u7684\u7c92\u5ea6\uff09</li> </ul>"},{"location":"course_notes/mitos/lab/locks/#memory-allocator","title":"Memory allocator","text":"<p>The root cause of lock contention in kalloctest is that <code>kalloc()</code> has a single free list, protected by a single lock.</p> <p>Task</p> <ul> <li>Your job is to implement per-CPU freelists, and stealing when a CPU's free list is empty.</li> <li>You must give all of your locks names that start with \"kmem\".</li> </ul> <p>\u9996\u5148\u4fee\u6539<code>kmem</code>\uff0c\u8ba9\u6bcf\u4e00\u4e2aCPU\u90fd\u6709\u81ea\u5df1\u7684free list. \u521d\u59cb\u5316\u65f6\u5206\u914d\u540d\u5b57\u3002</p> <pre><code>struct kmem {\n  struct spinlock lock;\n  struct run *freelist;\n  char lockname[8];\n};\n\nstruct kmem kmems[NCPU];\n\nvoid\nkinit()\n{\n  for (int i = 0; i &lt; NCPU; i++) {\n    snprintf(kmems[i].lockname, sizeof(kmems[i].lockname), \"kmem_%d\", i);\n    initlock(&amp;kmems[i].lock, kmems[i].lockname);\n  }\n  freerange(end, (void*)PHYSTOP);\n}\n</code></pre> <p>\u4ee5\u4e0b\u64cd\u4f5c\u9700\u8981\u6ce8\u610f\uff1a\u5728\u83b7\u53d6CPU ID\u65f6\u5fc5\u987b\u5148\u5173\u4e2d\u65ad\u3002</p> <pre><code>void\nkfree(void *pa)\n{\n  struct run *r;\n\n  if(((uint64)pa % PGSIZE) != 0 || (char*)pa &lt; end || (uint64)pa &gt;= PHYSTOP)\n    panic(\"kfree\");\n\n  // Fill with junk to catch dangling refs.\n  memset(pa, 1, PGSIZE);\n\n  r = (struct run*)pa;\n\n  push_off();\n  struct kmem *kmem_ptr = &amp;kmems[cpuid()];\n  pop_off();\n  acquire(&amp;kmem_ptr-&gt;lock);\n  r-&gt;next = kmem_ptr-&gt;freelist;\n  kmem_ptr-&gt;freelist = r;\n  release(&amp;kmem_ptr-&gt;lock);\n}\n</code></pre> <p><code>freerange</code>\u4f7f\u7528\u4e86\u4e0a\u9762\u7684<code>kfree</code>\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u4e00\u5f00\u59cb\u6240\u6709\u7684free memory\u90fd\u5728\u4e00\u4e2aCPU\uff08\u5373\u8c03\u7528<code>freerange</code>\u7684CPU\uff09\u7684freelist\u4e2d\u3002</p> <pre><code>void *\nkalloc(void)\n{\n  struct run *r;\n\n  push_off();\n  int id = cpuid();\n  struct kmem *kmem_ptr = &amp;kmems[id];\n  pop_off();\n  acquire(&amp;kmem_ptr-&gt;lock);\n\n  // no free memory in current CPU's free list\n  // steal 64 pages from other CPUs\n  if (kmem_ptr-&gt;freelist == 0) {\n    int steal_left = 64;\n    for (int i = 0; i &lt; NCPU; i++) {\n      if (i == id) continue;\n      acquire(&amp;kmems[i].lock);\n      while (kmems[i].freelist &amp;&amp; steal_left &gt; 0) {\n        r = kmems[i].freelist;\n        kmems[i].freelist = r-&gt;next;\n        r-&gt;next = kmem_ptr-&gt;freelist;\n        kmem_ptr-&gt;freelist = r;\n        steal_left--;\n      }\n      release(&amp;kmems[i].lock);\n      if (steal_left == 0) break;\n    }\n  }\n\n  r = kmem_ptr-&gt;freelist;\n  if(r)\n    kmem_ptr-&gt;freelist = r-&gt;next;\n  release(&amp;kmem_ptr-&gt;lock);\n\n  if(r)\n    memset((char*)r, 5, PGSIZE); // fill with junk\n  return (void*)r;\n}\n</code></pre> <p>\u5982\u679c\u5f53\u524dCPU\u7684freelist\u4e3a\u7a7a\uff0c\u5c31\u4ece\u5176\u4ed6CPU\u7684freelist\u4e2d\u507764\u4e2apages\u3002\u5077pages\u7684\u5408\u7406\u6570\u91cf\u5e94\u8be5\u7531\u5b9e\u9a8c\u5f97\u51fa\uff0c\u5982\u679c\u592a\u5c0f\uff0c\u5077pages\u7684\u6b21\u6570\u4f1a\u5f88\u591a\uff0c\u5bfc\u81f4\u9891\u7e41\u7684\u9501\u64cd\u4f5c\u3002</p>"},{"location":"course_notes/mitos/lab/locks/#buffer-cache","title":"Buffer cache","text":"<p>If multiple processes use the file system intensively, they will likely contend for <code>bcache.lock</code>, which protects the disk block cache in <code>kernel/bio.c</code>.</p> <p>Task</p> <ul> <li>Modify the block cache to reduce contention.</li> <li>Modify <code>bget</code> and <code>brelse</code> so that concurrent lookups and releases for different blocks that are in the bcache are unlikely to conflict on locks.</li> <li>You must maintain the invariant that at most one copy of each block is cached.</li> </ul> <p>\u65e7\u7248\u672c\u7684<code>bcache</code></p> <pre><code>struct {\n  struct spinlock lock;\n  struct buf buf[NBUF];\n\n  // Linked list of all buffers, through prev/next.\n  // Sorted by how recently the buffer was used.\n  // head.next is most recent, head.prev is least.\n  struct buf head;\n} bcache;\n</code></pre> <p>\u4f7f\u7528\u4e00\u4e2a<code>spinlock</code>\u4fdd\u62a4\u6574\u4e2a<code>bcache</code>\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u5927\u91cf\u7684\u9501\u7ade\u4e89\u3002buffer\u4ee5\u53cc\u5411\u94fe\u8868\u7684\u5f62\u5f0f\u7ec4\u7ec7\uff0c\u6700\u8fd1\u4f7f\u7528\u7684buffer\u5728\u94fe\u8868\u5934\u90e8\uff0c\u6700\u4e45\u672a\u4f7f\u7528\u7684\u5728\u94fe\u8868\u5c3e\u90e8\uff0c\u5b9e\u73b0\u4e86LRU\u3002</p> <p>\u4e3a\u4e86\u51cf\u5c11\u9501\u7ade\u4e89\uff0c\u6211\u4eec\u53ef\u4ee5\u5f15\u5165\u4e00\u4e2ahash table\uff0c\u7528<code>dev</code>\u548c<code>blockno</code>\u8ba1\u7b97\u51fahash\u503c\u3002</p> <pre><code>#define NBUCKET 13\n#define HASH(dev, blockno) (((uint)dev ^ (uint)blockno) % NBUCKET)\n\nstruct {\n  struct buf buf[NBUF];\n\n  struct buf bufmap[NBUCKET];\n  struct spinlock bufmap_lock[NBUCKET];\n  struct spinlock evict_lock;\n} bcache;\n\nvoid\nbinit(void)\n{\n  struct buf *b;\n\n  initlock(&amp;bcache.evict_lock, \"evict_lock\");\n\n  for (int i = 0; i &lt; NBUCKET; i++) {\n    bcache.bufmap[i].next = 0; \n    initlock(&amp;bcache.bufmap_lock[i], \"bufmap_lock\");\n  }\n\n  // Put all buffers into bufmap's 0th bucket.\n  for (b = bcache.buf; b &lt; bcache.buf + NBUF; b++) {\n    initsleeplock(&amp;b-&gt;lock, \"buffer\");\n    b-&gt;last_use = 0;\n    b-&gt;next = bcache.bufmap[0].next;\n    bcache.bufmap[0].next = b;\n  }\n}\n</code></pre> <p><code>bufmap</code>\u7684\u6bcf\u4e00\u4e2abucket\u90fd\u6307\u5411\u4e00\u4e2a\u5355\u5411\u7684buffer linked list\uff0c\u6bcf\u4e00\u4e2abucket\u90fd\u6709\u4e00\u4e2a<code>spinlock</code>\u4fdd\u62a4\u3002\u8fd9\u76f8\u5f53\u4e8e\u628a\u539f\u672c\u7684\u6574\u4e2alinked list\u62c6\u5206\u4e3a\u4e86\u591a\u4e2a\u5c0f\u7684linked list\uff0c\u6bcf\u4e2a\u5c0f\u7684linked list\u5b58\u50a8\u5728\u4e00\u4e2abucket\u4e2d\uff0c\u6bcf\u4e2abucket\u90fd\u6709\u4e00\u4e2a<code>spinlock</code>\u4fdd\u62a4\uff0c\u8fd9\u6837\u5c31\u51cf\u5c11\u4e86\u9501\u7ade\u4e89\u3002</p> <p>\u5728<code>buf</code>\u4e2d\u589e\u52a0\u4e86<code>last_use</code>\u5b57\u6bb5\uff0c\u4e0e<code>trap.c</code>\u4e2d\u7684<code>ticks</code>\u7ed3\u5408\u4f7f\u7528\uff0c\u7528\u6765\u8bb0\u5f55\u6700\u8fd1\u4e00\u6b21\u4f7f\u7528\u7684\u65f6\u95f4\u3002\u5220\u53bb<code>prev</code>\uff0c\u4e0d\u518d\u4f7f\u7528\u53cc\u5411\u94fe\u8868\u3002</p> <pre><code>struct buf {\n  int valid;   // has data been read from disk?\n  int disk;    // does disk \"own\" buf?\n  uint dev;\n  uint blockno;\n  struct sleeplock lock;\n  uint refcnt;\n  uint last_use;\n  struct buf *next;\n  uchar data[BSIZE];\n};\n</code></pre> <p>\u672c\u6b21Lab\u7684\u91cd\u5934\u620f\u6765\u4e86\uff1a<code>bget</code></p> <p>\u6709\u51e0\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u7ec6\u8282\uff0c\u9700\u8981\u8c28\u614e\u5730\u7528lock\u6765\u5904\u7406</p> <ul> <li>maintain the invariant that at most one copy of each block is cached</li> <li>\u6b7b\u9501\u95ee\u9898\uff08\u8fdb\u7a0bA\u6301\u6709bucket1\u7684\u9501\uff0cevict\u904d\u5386\u65f6acquire bucket2\u7684\u9501\uff0c\u8fdb\u7a0bB\u6301\u6709bucket2\u5230\u9501\uff0cevict\u904d\u5386\u65f6acquire bucket1\u7684\u9501\uff09</li> <li>\u5f97\u5230lru_buffer\u540e\u4e5f\u9700\u8981\u7ee7\u7eed\u6301\u6709\u5bf9\u5e94bucket\u7684\u9501\uff0c\u5426\u5219\u5176\u4ed6\u8fdb\u7a0b\u53ef\u80fd\u4f1a\u589e\u52a0\u5bf9\u6b64buffer\u7684\u5f15\u7528\u8ba1\u6570\uff0c\u5bfc\u81f4\u539f\u672c\u53ef\u9a71\u9010\u7684lru_buffer\u65e0\u6cd5\u88ab\u9a71\u9010</li> </ul> <p>\u5982\u679c\u60f3\u770b\u66f4\u8be6\u7ec6\u7684\u5206\u6790\uff0c\u53ef\u4ee5\u770b\u8fd9\u91cc\uff0c\u6211\u7684\u5b9e\u73b0\u4e5f\u662f\u53c2\u8003\u8fd9\u7bc7\u535a\u5ba2\u7684\uff0c\u8fd9\u4e2a\u54e5\u4eec\u5199\u5f97\u771f\u7684\u5f88\u597d\u3002</p> <p>\u8fd9\u91cc\u76f4\u63a5\u8d34\u6211\u7684\u5b9e\u73b0\uff1a</p> <pre><code>static struct buf*\nbget(uint dev, uint blockno)\n{\n  struct buf *b;\n\n  int key = HASH(dev, blockno);\n  acquire(&amp;bcache.bufmap_lock[key]);\n\n  // Is the block already cached?\n  for (b = bcache.bufmap[key].next; b != 0; b = b-&gt;next) {\n    if (b-&gt;dev == dev &amp;&amp; b-&gt;blockno == blockno) {\n      b-&gt;refcnt++;\n      release(&amp;bcache.bufmap_lock[key]);\n      acquiresleep(&amp;b-&gt;lock);\n      return b;\n    }\n  }\n\n  // Avoid deadlock\n  release(&amp;bcache.bufmap_lock[key]);\n\n  acquire(&amp;bcache.evict_lock);\n\n  // Check Again:\n  // Is the block already cached?\n  for (b = bcache.bufmap[key].next; b != 0; b = b-&gt;next) {\n    if (b-&gt;dev == dev &amp;&amp; b-&gt;blockno == blockno) {\n      b-&gt;refcnt++;\n      release(&amp;bcache.evict_lock);\n      acquiresleep(&amp;b-&gt;lock);\n      return b;\n    }\n  }\n\n  // Not cached.\n  // Find a buffer to evict.\n  // Trick: record the pointer before lru_buf for easier deletion.\n  struct buf *before_lru_buf = 0;\n  int holding_bucket = -1;\n  for (int i = 0; i &lt; NBUCKET; i++) {\n    int new_found = 0;\n    acquire(&amp;bcache.bufmap_lock[i]);\n    for (b = &amp;bcache.bufmap[i]; b-&gt;next != 0; b = b-&gt;next) {\n      if (b-&gt;next-&gt;refcnt == 0) {\n        if (before_lru_buf == 0 || b-&gt;next-&gt;last_use &lt; before_lru_buf-&gt;last_use) {\n          before_lru_buf = b;\n          new_found = 1;\n        }\n      }\n    }\n    if (new_found) {\n      if (holding_bucket != -1)\n        release(&amp;bcache.bufmap_lock[holding_bucket]);\n      // keep holding the lock where new lru_buf is found.\n      holding_bucket = i;\n    } else {\n      release(&amp;bcache.bufmap_lock[i]);\n    }\n  }\n\n  if (before_lru_buf == 0) {\n    panic(\"bget: no buffers\");\n  }\n\n  b = before_lru_buf-&gt;next;\n  if (holding_bucket != key) {\n    // Remove lru_buf from original bucket\n    before_lru_buf-&gt;next = b-&gt;next;\n    // Add lru_buf to the head of the bucket\n    acquire(&amp;bcache.bufmap_lock[key]);\n    b-&gt;next = bcache.bufmap[key].next;\n    bcache.bufmap[key].next = b;\n    release(&amp;bcache.bufmap_lock[key]);\n  }\n  // Update buf info\n  b-&gt;blockno = blockno;\n  b-&gt;dev = dev;\n  b-&gt;valid = 0;\n  b-&gt;refcnt = 1;\n  b-&gt;last_use = ticks;\n  acquiresleep(&amp;b-&gt;lock);\n  release(&amp;bcache.bufmap_lock[holding_bucket]);\n  release(&amp;bcache.evict_lock);\n\n  return b;\n}\n</code></pre> <ol> <li>\u9996\u5148\uff0c\u5982\u679cblock\u5df2\u7ecf\u5728cache\u4e2d\uff0c\u76f4\u63a5\u8fd4\u56de\u3002</li> <li>\u5982\u679cblock\u4e0d\u5728cache\u4e2d\uff0c\u9700\u8981\u4ece\u5176\u4ed6bucket\u4e2d\u9a71\u9010\u4e00\u4e2ablock\uff0c\u8fd9\u91cc\u9700\u8981\u904d\u5386\u6240\u6709\u7684bucket\uff0c\u627e\u5230\u4e00\u4e2a\u53ef\u9a71\u9010\u7684block\u3002<ol> <li>\u904d\u5386\u524d\uff0c<code>release(&amp;bcache.bufmap_lock[key])</code>\uff0c\u907f\u514d\u521a\u624d\u63d0\u5230\u7684\u6b7b\u9501\u3002</li> <li><code>acquire(&amp;bcache.evict_lock)</code>\uff0c\u4fdd\u8bc1\u53ea\u6709\u4e00\u4e2a\u8fdb\u7a0b\u5728\u6267\u884cevict\u64cd\u4f5c\uff0c\u53ea\u6709\u4e00\u4e2a\u8fdb\u7a0b\u80fd\u591f\u4fee\u6539buf\u5728bucket\u4e2d\u7684\u4f4d\u7f6e\u3002</li> <li>Check Again\uff1a\u4ece<code>release(&amp;bcache.bufmap_lock[key])</code>\u5230<code>acquire(&amp;bcache.evict_lock)</code>\u671f\u95f4\uff0c\u53ef\u80fd\u6709\u5176\u4ed6\u8fdb\u7a0b\u5b8c\u6210\u4e86evict\u64cd\u4f5c\uff0c\u6240\u4ee5\u9700\u8981\u518d\u6b21\u68c0\u67e5block\u662f\u5426\u5df2\u7ecf\u5728cache\u4e2d\u3002</li> </ol> </li> <li>\u904d\u5386\u8fc7\u7a0b\u4e2d\uff0c\u7528<code>holding_bucket</code>\u8bb0\u5f55\u5f53\u524d\u627e\u5230\u7684lru_buffer\u6240\u5728\u7684bucket\uff0c\u4fdd\u8bc1\u5728\u904d\u5386\u8fc7\u7a0b\u4e2d\u4e0d\u4f1a\u91ca\u653e\u8fd9\u4e2abucket\u7684\u9501\u3002\u5982\u679c\u627e\u5230\u4e86\u4e00\u4e2a\u65b0\u7684lru_buffer\uff0c\u5c31\u91ca\u653e\u4e4b\u524d\u7684bucket\u7684\u9501\uff0c\u4fdd\u6301\u5bf9\u65b0\u7684bucket\u7684\u9501\u3002\u8fd9\u786e\u4fdd\u4e86\u5c06\u8981\u88abevict\u7684lru_buffer\u4e0d\u4f1a\u88ab\u5176\u4ed6\u8fdb\u7a0b\u589e\u52a0\u5f15\u7528\u8ba1\u6570\u3002</li> <li>\u7ec8\u4e8e\u904d\u5386\u5b8c\u4e86\uff0c\u627e\u5230\u4e86\u4e00\u4e2alru_buffer\uff0c\u5c06\u5176\u4ece\u539f\u6765\u7684bucket\u4e2d\u5220\u9664\uff0c\u52a0\u5165\u5230\u65b0\u7684bucket\u7684\u5934\u90e8\uff0c\u66f4\u65b0buf\u7684\u4fe1\u606f\uff0c\u5904\u7406\u76f8\u5173\u7684\u9501\uff0c\u8fd4\u56de\u3002</li> </ol>"},{"location":"course_notes/mitos/lab/locks/#_1","title":"\u603b\u7ed3","text":"<p>\u8fd9\u6b21Lab\u7684\u91cd\u70b9\u662f\u9501\u7ade\u4e89\u4f18\u5316\uff0c\u6d89\u53ca\u5230\u9501\u7684\u7c92\u5ea6\u3001\u9501\u7684\u6570\u91cf\u3001\u9501\u7684\u83b7\u53d6\u987a\u5e8f\u7b49\u7b49\u3002\u8fd9\u4e9b\u90fd\u662f\u975e\u5e38\u7ec6\u8282\u7684\u4e1c\u897f\uff0c\u9700\u8981\u4ed4\u7ec6\u5730\u5206\u6790\uff0c\u624d\u80fd\u5199\u51fa\u6b63\u786e\u7684\u4ee3\u7801\u3002</p> <p>\u6458\u6284\u4e00\u6bb5\uff1a</p> <p>don't share if you don't have to</p> <p>start with a few coarse-grained locks</p> <p>instrument your code -- which locks are preventing parallelism?</p> <p>use fine-grained locks only as needed for parallel performance</p> <p>use an automated race detector</p>"},{"location":"course_notes/mitos/lab/mmap/","title":"mmap","text":"<p>Info</p> <pre><code>The `mmap` and `munmap` system calls allow UNIX programs to exert detailed control over their address spaces. They can be used to share memory among processes, to map files into process address spaces, and as part of user-level page fault schemes such as the garbage-collection algorithms discussed in lecture. In this lab you'll add `mmap` and `munmap` to xv6, focusing on memory-mapped files.\n</code></pre>"},{"location":"course_notes/mitos/lab/thread/","title":"Multithreading","text":"<p>\u6700\u7b80\u5355\u7684lab\u6ca1\u6709\u4e4b\u4e00\u3002</p>"},{"location":"course_notes/mitos/lab/thread/#uthread-switching-between-threads","title":"Uthread: switching between threads","text":"<p>Uthread\u662f\u4e00\u4e2a\u7528\u6237\u6001\u7684\u7ebf\u7a0b\u5e93\uff0c\u7531\u6211\u4eec\u81ea\u5df1\u5b9e\u73b0\u3002</p> <p>Task</p> <p>Your job is to come up with a plan to create threads and save/restore registers to switch between threads, and implement that plan.</p>"},{"location":"course_notes/mitos/lab/thread/#thread-creation","title":"Thread creation","text":"<pre><code>void \nthread_create(void (*func)())\n{\n  struct thread *t;\n\n  for (t = all_thread; t &lt; all_thread + MAX_THREAD; t++) {\n    if (t-&gt;state == FREE) break;\n  }\n  t-&gt;state = RUNNABLE;\n  t-&gt;context.ra = (uint64)func;\n  t-&gt;context.sp = (uint64)(t-&gt;stack + STACK_SIZE);\n}\n</code></pre> <p>when <code>thread_schedule()</code> runs a given thread, the thread executes the function passed to <code>thread_create()</code>, on its own stack.</p> <p>\u8981\u505a\u5230\u8fd9\u4e00\u70b9\uff0c\u9700\u8981\u628a<code>ra</code>\u8bbe\u7f6e\u4e3a<code>func</code>\u7684\u5730\u5740\uff0c\u8fd9\u6837\u5f53scheduler\u5207\u6362\u5230\u8fd9\u4e2a\u7ebf\u7a0b\u7684\u65f6\u5019\uff0c\u5c31\u4f1a\u4ece<code>func</code>\u5f00\u59cb\u6267\u884c\u3002</p> <p>\u522b\u5fd8\u4e86\u8bbe\u7f6e<code>sp</code>\uff0c\u8ba9<code>sp</code>\u6307\u5411\u7ebf\u7a0b\u6808\u7684\u9876\u90e8\u3002</p>"},{"location":"course_notes/mitos/lab/thread/#thread-switching","title":"Thread switching","text":"<p>\u4eff\u7167\u5185\u6838\u7ebf\u7a0b\u7684\u5207\u6362\uff0c\u6211\u4eec\u4e5f\u9700\u8981\u4fdd\u5b58\u548c\u6062\u590dcontext\u3002</p> <pre><code>struct context {\n  uint64 ra;\n  uint64 sp;\n\n  // callee-saved\n  uint64 s0;\n  uint64 s1;\n  uint64 s2;\n  uint64 s3;\n  uint64 s4;\n  uint64 s5;\n  uint64 s6;\n  uint64 s7;\n  uint64 s8;\n  uint64 s9;\n  uint64 s10;\n  uint64 s11;\n};\n\nstruct thread {\n  char       stack[STACK_SIZE]; /* the thread's stack */\n  int        state;             /* FREE, RUNNING, RUNNABLE */\n  struct context context;       /* register context */\n};\n</code></pre> <p>\u7ed9<code>struct thread</code>\u6dfb\u52a0<code>context</code>\uff0c\u7528\u6765\u4fdd\u5b58\u5bc4\u5b58\u5668\u7684\u503c\u3002</p> <p>\u8fd9\u91cc\u53ef\u4ee5\u89c2\u5bdf\u5230\u7ebf\u7a0b\u7684stack\u53ea\u662f<code>thread</code>\u7ed3\u6784\u4f53\u4e2d\u7684\u4e00\u4e2a<code>char</code>\u6570\u7ec4\uff0c\u5927\u5c0f\u56fa\u5b9a\u4e3a<code>STACK_SIZE=8192 bytes</code>\u3002</p> <pre><code>extern void thread_switch(struct context*, struct context*);\n</code></pre> <p><code>uthread_switch.S</code>\u53ef\u4ee5\u76f4\u63a5\u7167\u642c\u5185\u6838\u7684<code>swtch.S</code>\uff0c\u90fd\u662f\u4e00\u6837\u7684\u3002</p>"},{"location":"course_notes/mitos/lab/thread/#thread-scheduling","title":"Thread scheduling","text":"<pre><code>void \nthread_schedule(void)\n{\n  struct thread *t, *next_thread;\n\n  /* Find another runnable thread. */\n  /* ... */\n\n  if (current_thread != next_thread) {         /* switch threads?  */\n    next_thread-&gt;state = RUNNING;\n    t = current_thread;\n    current_thread = next_thread;\n    thread_switch(&amp;t-&gt;context, &amp;next_thread-&gt;context);\n  } else\n    next_thread = 0;\n}\n</code></pre>"},{"location":"course_notes/mitos/lab/thread/#using-threads","title":"Using threads","text":"<p>Explore parallel programming with UNIX <code>pthread</code> threading library using a hash table.</p> <p><code>notxv6/ph.c</code> contains a simple hash table that is correct if used from a single thread, but incorrect when used from multiple threads.</p> <p>\u4e3a\u4ec0\u4e48\u591a\u7ebf\u7a0b\u7684\u60c5\u51b5\u4e0b\u4f1a\u6709missing keys?</p> <p>\u5176\u5b9exv6 book\u4e2dLock\u7684\u7ae0\u8282\u5df2\u7ecf\u7ed9\u51fa\u7c7b\u4f3c\u7684\u4f8b\u5b50\uff0c\u4e24\u4e2a\u7ebf\u7a0b\u540c\u65f6\u5bf9\u4e00\u4e2a\u94fe\u8868\u6267\u884c\u63d2\u5165node\u64cd\u4f5c\uff0c\u4f1a\u5bfc\u81f4\u5176\u4e2d\u4e00\u4e2anode\u4e22\u5931\u3002\u7ed3\u5408\u4ee3\u7801\u753b\u753b\u56fe\u5c31\u80fd\u770b\u51fa\u6765\u3002</p> <p>Task</p> <p>Insert lock and unlock statements in <code>put</code> and <code>get</code> in <code>notxv6/ph.c</code> so that the number of keys missing is always 0 with multiple threads.</p> <p>\u5982\u679c\u76f4\u63a5\u7ed9put, get\u64cd\u4f5c\u52a0\u9501\uff0c\u80fd\u5f97\u5230\u6b63\u786e\u7684\u7ed3\u679c\uff0c\u4f46\u662f\u6027\u80fd\u53cd\u800c\u6bd4\u5355\u7ebf\u7a0b\u7684\u60c5\u51b5\u4e0b\u5dee\u3002\u56e0\u4e3a\u6bcf\u6b21\u53ea\u80fd\u6709\u4e00\u4e2a\u7ebf\u7a0b\u8bbf\u95eehash table\uff0c\u5176\u4ed6\u7ebf\u7a0b\u90fd\u5728\u7b49\u5f85\u3002\u8fd9\u5c31\u8ddf\u5355\u7ebf\u7a0b\u7684\u60c5\u51b5\u4e0b\u6ca1\u6709\u533a\u522b\u4e86\u3002\u6b64\u5916\uff0c\u9501\u64cd\u4f5c\uff08\u52a0\u9501\u3001\u89e3\u9501\u3001\u9501\u7ade\u4e89\uff09\u4e5f\u4f1a\u5e26\u6765\u989d\u5916\u7684\u5f00\u9500\uff0c\u8fdb\u4e00\u6b65\u964d\u4f4e\u6027\u80fd\u3002</p> <p>\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u51cf\u5c11\u9501\u7684\u7c92\u5ea6\u2014\u2014\u6bcf\u4e2abucket\u4e00\u4e2a\u9501\uff0c\u8ba9\u591a\u4e2a\u7ebf\u7a0b\u53ef\u4ee5\u540c\u65f6\u8bbf\u95eehash table\u3002</p> <pre><code>pthread_mutex_t locks[NBUCKET];\n</code></pre> <pre><code>static \nvoid put(int key, int value)\n{\n  int i = key % NBUCKET;\n  pthread_mutex_lock(&amp;locks[i]);\n\n  // is the key already present?\n  struct entry *e = 0;\n  for (e = table[i]; e != 0; e = e-&gt;next) {\n    if (e-&gt;key == key)\n      break;\n  }\n  if(e){\n    // update the existing key.\n    e-&gt;value = value;\n  } else {\n    // the new is new.\n    insert(key, value, &amp;table[i], table[i]);\n  }\n  pthread_mutex_unlock(&amp;locks[i]);\n}\n\nstatic struct entry*\nget(int key)\n{\n  int i = key % NBUCKET;\n  pthread_mutex_lock(&amp;locks[i]);\n\n  struct entry *e = 0;\n  for (e = table[i]; e != 0; e = e-&gt;next) {\n    if (e-&gt;key == key) break;\n  }\n  pthread_mutex_unlock(&amp;locks[i]);\n\n  return e;\n}\n</code></pre> <p>\u522b\u5fd8\u4e86\u5728<code>main</code>\u51fd\u6570\u4e2d\u521d\u59cb\u5316\u9501\u3002 <pre><code>int\nmain(int argc, char *argv[])\n{\n  pthread_t *tha;\n  void *value;\n  double t1, t0;\n\n  for(int i = 0; i &lt; NBUCKET; i++) {\n    assert(pthread_mutex_init(&amp;locks[i], NULL) == 0);\n  }\n  // ...\n}\n</code></pre></p>"},{"location":"course_notes/mitos/lab/thread/#barrier","title":"Barrier","text":"<p>Task</p> <p>Implement a barrier: a point in an application at which all participating threads must wait until all other participating threads reach that point too.</p> <p>\u5148\u7406\u89e3\u4ee3\u7801\uff1a</p> <pre><code>static int nthread = 1;\nstatic int round = 0;\n\nstruct barrier {\n  pthread_mutex_t barrier_mutex;\n  pthread_cond_t barrier_cond;\n  int nthread;      // Number of threads that have reached this round of the barrier\n  int round;     // Barrier round\n} bstate;\n</code></pre> <p>\u53ef\u4ee5\u770b\u5230\u5168\u5c40\u53d8\u91cf<code>nthread</code>\u8bb0\u5f55\u4e86\u7a0b\u5e8f\u6709\u591a\u5c11\u4e2a\u7ebf\u7a0b\u3002<code>bstate</code>\u8bb0\u5f55\u4e86\u9501\u3001\u6761\u4ef6\u53d8\u91cf\u3001\u5f53\u524d\u8fbe\u5230barrier\u7684\u7ebf\u7a0b\u6570\u3001\u5f53\u524dbarrier\u7684round\u3002</p> <pre><code>static void *\nthread(void *xa)\n{\n  long n = (long) xa;\n  long delay;\n  int i;\n\n  for (i = 0; i &lt; 20000; i++) {\n    int t = bstate.round;\n    assert (i == t);\n    barrier();\n    usleep(random() % 100);\n  }\n\n  return 0;\n}\n</code></pre> <p>\u6bcf\u4e2a\u7ebf\u7a0b\u90fd\u4f1a\u6267\u884c20000\u6b21\u5faa\u73af\uff0c\u6bcf\u6b21\u5faa\u73af\u4e2d\uff0c\u68c0\u67e5\u5f53\u524d\u7684round\u662f\u5426\u7b49\u4e8e\u5faa\u73af\u6b21\u6570\u3002\u540e\u8c03\u7528<code>barrier()</code>\uff0c\u7b49\u5f85\u5176\u4ed6\u7ebf\u7a0b\u5230\u8fbebarrier\u3002\u5982\u679cbarrier\u6b63\u786e\u540c\u6b65\u4e86\u6240\u6709\u7ebf\u7a0b\uff0c\u5c31\u4e0d\u4f1a\u51fa\u73b0<code>i != t</code>\u7684\u60c5\u51b5\u3002</p> <p>\u4e0b\u9762\u770b\u770b<code>barrier()</code>\u7684\u5b9e\u73b0\uff1a</p> <pre><code>static void \nbarrier()\n{\n  // Block until all threads have called barrier() and\n  // then increment bstate.round.\n  pthread_mutex_lock(&amp;bstate.barrier_mutex);\n  bstate.nthread++;\n  if (bstate.nthread == nthread) {\n    bstate.round++;\n    bstate.nthread = 0;\n    pthread_cond_broadcast(&amp;bstate.barrier_cond);\n  } else {\n    pthread_cond_wait(&amp;bstate.barrier_cond, &amp;bstate.barrier_mutex);\n  }\n  pthread_mutex_unlock(&amp;bstate.barrier_mutex);\n}\n</code></pre> <p>\u9996\u5148\uff0c\u6bcf\u4e2a\u7ebf\u7a0b\u90fd\u4f1a\u8c03\u7528<code>barrier()</code>\uff0c\u56e0\u6b64\u9700\u8981\u52a0\u9501\u3002\u7136\u540e\uff0c\u6bcf\u4e2a\u7ebf\u7a0b\u90fd\u4f1a\u628a<code>bstate.nthread</code>\u52a01\uff0c\u8868\u793a\u5f53\u524d\u7ebf\u7a0b\u5df2\u7ecf\u5230\u8fbebarrier\u3002\u5982\u679c\u5f53\u524d\u7ebf\u7a0b\u662f\u6700\u540e\u4e00\u4e2a\u5230\u8fbebarrier\u7684\u7ebf\u7a0b\uff0c\u90a3\u4e48\u5c31\u9700\u8981\u66f4\u65b0<code>bstate.round</code>\uff0c\u5e76\u4e14\u5524\u9192\u5176\u4ed6\u7ebf\u7a0b\u3002\u5426\u5219\uff0c\u5f53\u524d\u7ebf\u7a0b\u5c31\u9700\u8981\u7b49\u5f85\u5176\u4ed6\u7ebf\u7a0b\u5230\u8fbebarrier\u3002</p>"},{"location":"course_notes/mitos/lab/trap/","title":"traps","text":""},{"location":"course_notes/mitos/lab/trap/#backtrace","title":"Backtrace","text":"\ud83d\udccc Backtrace: a list of the function calls on the stack above the point at which the error occurred.   <p>\u76ee\u7684\uff1a\u5f53 kernel panic \u7684\u65f6\u5019\u80fd\u591f\u6253\u5370\u51fa backtrace \u4fe1\u606f</p> <p>\u5bf9\u4e8e\u8fd9\u4e2a\u5b9e\u9a8c\uff0c\u5f88\u91cd\u8981\u7684\u4e00\u5f20\u56fe\uff1a</p> <p></p> <p>stack \u4ece\u9ad8\u5730\u5740\u5411\u4f4e\u5730\u5740\u589e\u957f\uff0c\u5728\u8fd9\u4e2a\u56fe\u662f\u4ece\u4e0a\u5230\u4e0b\u7684\u65b9\u5411\u3002\u800c backtrace \u662f\u4ece\u6700 low \u7684 stack \u5f00\u59cb\uff08\u4e5f\u5c31\u662f\u6700\u65b0\u7684\u3001\u521a\u521a\u53d1\u751f panic \u7684 stack\uff09\uff0c\u4ece\u4e0b\u5f80\u4e0a\u904d\u5386\u6240\u6709 stack\u3002</p> <p>\u5927\u6982\u601d\u8def\u6709\u4e86\u4e4b\u540e\uff0c\u8ddf\u7740 hints \u628a\u5b9e\u9a8c\u505a\u5b8c</p> <p>Add the prototype for backtrace to kernel/defs.h so that you can invoke backtrace.</p> <pre><code>// defs.h\nvoid            backtrace(void);\n</code></pre> <p>\u8fd9\u91cc\u544a\u8bc9\u6211\u4eec\u600e\u4e48\u63d0\u53d6<code>ra</code>(return address)\u548c\u4e0a\u4e00\u4e2a\u6808\u7684<code>fp</code></p> <p>Xv6 allocates one page for each stack in the xv6 kernel at PAGE-aligned address. You can compute the top and bottom address of the stack page by using PGROUNDDOWN(fp) and PGROUNDUP(fp). These number are helpful for backtrace to terminate its loop.</p> <p>\u8fd9\u91cc\u544a\u8bc9\u6211\u4eec\u5e94\u8be5\u5728\u4ec0\u4e48\u65f6\u5019\u505c\u6b62\u904d\u5386\u3002</p> <pre><code>void\nbacktrace(void)\n{\n    // \u83b7\u53d6\u5f53\u524d\u7684fp\n  uint64 fp = r_fp();\n    // \u56e0\u4e3a\u6808\u662f\u4ece\u9ad8\u5730\u5740\u5411\u4f4e\u5730\u5740\u589e\u957f\u7684\uff0c\u6240\u4ee5\u5e94\u8be5\u4f7f\u7528PGROUNDUP\u83b7\u5f97stack\u5e95\u90e8\n  uint64 stack_page_bottom = PGROUNDUP(fp);\n  while (fp != stack_page_bottom) {\n        // return address\n    printf(\"%p\\n\", *(uint64 *)(fp - 8));\n        // previous frame pointer\n    fp = *(uint64 *)(fp - 16);\n  }\n}\n</code></pre>"},{"location":"course_notes/mitos/lab/trap/#alarm","title":"Alarm","text":"\ud83d\udccc periodically alerts a process as it uses CPU time   <ul> <li>add a new\u00a0<code>sigalarm(interval, handler)</code>\u00a0system call</li> <li>If an application calls\u00a0<code>sigalarm(n, fn)</code>, then after every\u00a0<code>n</code>\u00a0\"ticks\" of CPU time that the program consumes, the kernel should cause application function\u00a0<code>fn</code>\u00a0to be called.</li> <li>When\u00a0<code>fn</code>\u00a0returns, the application should resume where it left off.</li> </ul>"},{"location":"course_notes/mitos/lab/trap/#invoke-handler","title":"invoke handler","text":"<p>Update <code>user/usys.pl</code> (which generates <code>user/usys.S</code>), <code>kernel/syscall.h</code>, and <code>kernel/syscall.c</code> to allow\u00a0<code>alarmtest</code>\u00a0to invoke the <code>sigalarm</code> and <code>sigreturn</code> system calls</p> <ul> <li><code>user/usys.pl</code>\uff1aGenerate usys.S, the stubs for syscalls. \u6709\u4e86 stub\uff0c\u7528\u6237\u7a0b\u5e8f\u7684\u7cfb\u7edf\u8c03\u7528\u8bf7\u6c42\u5c31\u53ef\u4ee5\u88ab\u8f6c\u53d1\u5230\u5185\u6838\uff0c\u4ee5\u4fbf\u6267\u884c\u76f8\u5e94\u7684\u7cfb\u7edf\u8c03\u7528</li> </ul> <p>Your\u00a0<code>sys_sigalarm()</code>\u00a0should store the alarm interval and the pointer to the handler function in new fields in the\u00a0<code>proc</code>\u00a0structure.</p> <p>You'll need to keep track of how many ticks have passed since the last call to a process's alarm handler; you'll need a new field in\u00a0struct\u00a0<code>proc</code>\u00a0for this too. You can initialize\u00a0<code>proc</code>\u00a0fields in\u00a0<code>allocproc()</code>\u00a0in\u00a0<code>proc.c</code>.</p> <p>When a trap on the RISC-V returns to user space, what determines the instruction address at which user-space code resumes execution?</p> <p>\u8fd9\u662f\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u63d0\u793a\uff1a\u5f53 RISC-V \u4ece kernel \u8fd4\u56de user space \u7684\u65f6\u5019</p> <p>\u2026returning to user space\u2026setting <code>sepc</code> to the previously saved user program counter</p> <p><code>usertrap</code>\u4e2d\u7684\u4ee3\u7801\uff1a</p> <pre><code>// save user program counter.\np-&gt;trapframe-&gt;epc = r_sepc();\n</code></pre> <p><code>usertrapret</code>\u4e2d\u7684\u4ee3\u7801\uff1a</p> <pre><code>// set S Exception Program Counter to the saved user pc.\nw_sepc(p-&gt;trapframe-&gt;epc);\n</code></pre> <p>\u6240\u4ee5\u53ea\u8981\u4fee\u6539<code>p-&gt;trapframe-&gt;epc</code>\uff0c\u5c31\u53ef\u4ee5\u63a7\u5236 user space \u4ee3\u7801 resumes execution \u7684\u4f4d\u7f6e</p> <p>\u6700\u5f00\u59cb\u7684\u65f6\u5019\uff0c\u7528\u6237\u9700\u8981\u8c03\u7528 system call -- sigalarm \u6765\u914d\u7f6e\u3001\u6fc0\u6d3b alarm\u3002</p> <p>\u6ca1\u5565\u597d\u8bf4\u7684\uff0c\u914d\u7f6e<code>struct proc</code>\u4e2d alarm \u76f8\u5173\u7684 fields \u5373\u53ef</p> sysproc.c, sys_sigalarm<pre><code>uint64\nsys_sigalarm(void)\n{\n  // get ticks in a0 and handler function in a1\n  int ticks;\n  if (argint(0, &amp;ticks) &lt; 0)\n    return -1;\n\n  uint64 handler;\n  if (argaddr(1, &amp;handler) &lt; 0)\n    return -1;\n\n  struct proc *p = myproc();\n\n  if (ticks == 0 &amp;&amp; handler == 0) {\n    // disable alarm\n    p-&gt;alarm_interval = p-&gt;alarm_handler_addr = p-&gt;alarm_ticks_left = 0;\n    return 0;\n  }\n\n  p-&gt;alarm_interval = ticks;\n  p-&gt;alarm_handler_addr = handler;\n  p-&gt;alarm_ticks_left = ticks;\n  return 0;\n}\n</code></pre> <p>\u5f53\u7528\u6237\u8c03\u7528<code>sigalarm</code>\u4e4b\u540e\uff0c\u4ec5\u4ec5\u662f\u628a<code>struct proc</code>\u91cc\u9762\u7684\u51e0\u4e2a fields \u7684\u503c\u6539\u4e86\u4e00\u4e0b\u800c\u5df2\u3002</p> <p>\u6240\u4ee5\u8fd8\u9700\u8981\u5728\u53d1\u751f timer interrupt \u7684\u65f6\u5019\uff0c\u6839\u636e\u8be5\u8fdb\u7a0b\u7684 alarm \u4fe1\u606f\uff0c\u8fdb\u884c\u5bf9\u5e94\u7684\u5904\u7406</p> kernel/trap.c, usertrap<pre><code>  // give up the CPU if this is a timer interrupt.\n  if(which_dev == 2) {\n    if (p-&gt;alarm_interval &gt; 0) {\n      if (--p-&gt;alarm_ticks_left == 0 &amp;&amp; p-&gt;handler_executing == 0) {\n        p-&gt;alarm_ticks_left = p-&gt;alarm_interval;\n\n        // save original trapframe\n        memmove(p-&gt;alarm_trapframe, p-&gt;trapframe, sizeof(struct trapframe));\n\n        p-&gt;trapframe-&gt;epc = p-&gt;alarm_handler_addr;\n        p-&gt;handler_executing = 1;\n      }\n    }\n    yield();\n  }\n</code></pre> <ul> <li>\u68c0\u67e5\u662f\u5426\u5f00\u542f alarm<code>if (p-&gt;alarm_interval &gt; 0)</code>\u5982\u679c\u5f00\u542f\uff1a</li> <li><code>--p-&gt;alarm_ticks_left</code>\u51cf\u5c0f\u5012\u8ba1\u65f6\u3002\u5982\u679c\u5012\u8ba1\u65f6\u53d8\u6210 0\uff1a<ul> <li>reset \u5012\u8ba1\u65f6</li> <li><code>p-&gt;trapframe-&gt;epc = p-&gt;alarm_handler_addr;</code>\u8fd9\u6837\u8fd4\u56de\u5230 user space \u65f6\u7684 PC \u5c31\u662f alarm handler \u7684\u5730\u5740</li> </ul> </li> </ul> <p>\u540e\u7eed\u8981\u6c42\uff1awhen the alarm handler is done, control returns to the instruction at which the user program was originally interrupted by the timer interrupt</p> <p>\u63a5\u4e0b\u6765\u6839\u636e hints \u7ee7\u7eed\u89e3\u91ca\u4ee3\u7801</p> <p>user alarm handlers are required to call the <code>sigreturn</code> system call when they have finished.</p> <p>Have <code>usertrap</code> save enough state in <code>struct proc</code> when the timer goes off that <code>sigreturn</code> can correctly return to the interrupted user code.</p> <p>\u4e0a\u9762<code>usertrap</code>\u7684\u4ee3\u7801\u4e2d\uff0c<code>memmove(p-&gt;alarm_trapframe, p-&gt;trapframe, sizeof(struct trapframe));</code>\u628a\u8fdb\u7a0b\u539f\u672c\u7684<code>p-&gt;trapframe</code>\u4fdd\u5b58\u5230\u4e00\u4e2a\u65b0\u589e\u7684 field\uff1a<code>alarm_trapframe</code>\uff0c\u5907\u4efd\u4e86\u4e4b\u540e\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u5bf9<code>p-&gt;trapframe</code>\u4e3a\u6240\u6b32\u4e3a\u4e86\uff0c\u56e0\u4e3a<code>p-&gt;trapframe</code>\u4f1a\u5728<code>sigreturn</code>\u4e2d\u88ab\u6062\u590d\u3002</p> <p>Prevent re-entrant calls to the handler----if a handler hasn't returned yet, the kernel shouldn't call it again.</p> <p>\u52a0\u4e00\u4e2a\u6807\u5fd7\u4f4d\u5373\u53ef\uff1a\u5982\u679c\u8fdb\u5165\u4e86 alarm handler\uff0c\u5c31\u628a<code>p-&gt;handler_executing</code>\u8bbe\u4e3a 1\uff0chandler \u6267\u884c\u7ed3\u675f\u540e\u8c03\u7528\u7684<code>sigreturn</code>\u4f1a\u628a<code>p-&gt;handler_executing</code>\u91cd\u65b0\u7f6e 0\u3002\u6bcf\u6b21 timer interrupt \u90fd\u9700\u8981\u68c0\u67e5<code>p-&gt;handler_executing</code>\u662f\u5426\u4e3a 0.</p>"},{"location":"course_notes/mitos/lab/trap/#resume-interrupted-code","title":"resume interrupted code","text":"<pre><code>uint64\nsys_sigreturn(void)\n{\n  struct proc *p = myproc();\n  memmove(p-&gt;trapframe, p-&gt;alarm_trapframe, sizeof(struct trapframe));\n  p-&gt;handler_executing = 0;\n  return 0;\n}\n</code></pre> <p>\u6062\u590d\u4e4b\u524d\u5907\u4efd\u597d\u7684\u6570\u636e\uff0c\u628a\u6807\u5fd7\u4f4d\u8bbe\u4e3a 0 \u5373\u53ef\u3002</p> <p></p>"},{"location":"paper_read/Performance_Study_10GbE_NICs_SR-IOV/","title":"Evaluating Standard-Based Self-Virtualizing Devices: A Performance Study of 10GbE NICs with SR-IOV Support","text":""},{"location":"paper_read/Performance_Study_10GbE_NICs_SR-IOV/#summary","title":"Summary","text":"<p>The hardware-based SR-IOV approach provides superior performance to the software-based approach in KVM.</p> <p>SR-IOV approach has great potential to achieve high performance I/O in a virtualized environment.</p>"},{"location":"paper_read/Performance_Study_10GbE_NICs_SR-IOV/#outline","title":"Outline","text":""},{"location":"paper_read/Performance_Study_10GbE_NICs_SR-IOV/#introduction","title":"Introduction","text":"<p>We can divide the current I/O virtualization models into two categories:</p> <ul> <li>Software-based  <ul> <li>Several software components (e.g., hypervisor, guest VMs...) work together to provide access points to VMs without special hardware support.</li> <li>Major concern: may suffer significant performance degradation - overheads such as context/control switches and memory copies.</li> </ul> </li> <li>Hardware-based<ul> <li>Potentially achieve higher performance by allowing direct hardware access from within a guest VM.</li> </ul> </li> </ul>"},{"location":"paper_read/Performance_Study_10GbE_NICs_SR-IOV/#background","title":"Background","text":""},{"location":"paper_read/Performance_Study_10GbE_NICs_SR-IOV/#sr-iov","title":"SR-IOV","text":""},{"location":"paper_read/Performance_Study_10GbE_NICs_SR-IOV/#io-virtualization-in-kvm","title":"I/O Virtualization in KVM","text":"<p>Network Virtualization in KVM:</p> <p></p> <p>When code within a virtual machine attempts to send network packets through a virtual network interface card, the virtual machine needs to perform a VM exit. This transfers control to the I/O emulation code in the user space of the host machine to handle these packets. The process includes:</p> <ol> <li>The program within the virtual machine tries to perform a network operation, such as sending a packet.</li> <li>As the virtual machine cannot directly communicate with physical hardware, this operation triggers a VM exit, causing the execution of the virtual machine to be suspended.</li> <li>Control is passed to the host's KVM module, which identifies the operation as a network action.</li> <li>KVM transfers control to the I/O emulation code running in user mode.</li> <li>The I/O emulation code sends or receives network packets through the user space TAP device.</li> <li>Once the network I/O operation is complete, control is returned to the virtual machine, which then continues to execute the remaining instructions.</li> </ol>"},{"location":"paper_read/Performance_Study_10GbE_NICs_SR-IOV/#evaluation-methodology","title":"Evaluation Methodology","text":""},{"location":"paper_read/Performance_Study_10GbE_NICs_SR-IOV/#experimental-testbed","title":"Experimental Testbed","text":"<ul> <li>Two servers with a back-to-back 10 GbE connection.</li> <li>Primary server</li> <li>Secondary server</li> <li>NIC with support for SR-IOV</li> </ul>"},{"location":"paper_read/Performance_Study_10GbE_NICs_SR-IOV/#test-cases-benchmarks-and-data-collection","title":"Test Cases, Benchmarks and Data Collection","text":"<ul> <li>SR-IOV: This is the case where PCI passthrough is used to dedicated a virtual function of the X3100 10 GbE NIC to each VM.</li> <li>VIRTIO: The software approach is used in this case where a TAP interface is used to emulate a NIC in each VM and bridged to the X3100 NIC in the host. The emulated NICs in the guest VMs are based on virtio.</li> <li>Native: The tests are done in native Linux without using any VMs.</li> </ul>"},{"location":"paper_read/Performance_Study_10GbE_NICs_SR-IOV/#evaluation","title":"Evaluation","text":"<p>Latency, Bandwidth, Inter-VM Communication etc.</p>"},{"location":"paper_read/Performance_Study_10GbE_NICs_SR-IOV/#limitation-or-extension","title":"Limitation or Extension","text":""},{"location":"paper_read/Performance_Study_10GbE_NICs_SR-IOV/#my-opinion","title":"My Opinion","text":""},{"location":"paper_read/SR-IOV-KVM-Performance-Impact/","title":"Measuring the impact of SR-IOV and virtualization on packet round-trip time","text":""},{"location":"paper_read/SR-IOV-KVM-Performance-Impact/#summary","title":"Summary","text":"<p>This paper presents a comprehensive study on the impact of Single Root I/O Virtualization (SR-IOV) and various virtualization technologies on packet round-trip time (RTT) in cloud computing environments. The authors, through a series of controlled experiments, aim to understand the performance implications of SR-IOV when paired with different configurations and loads in a Kernel-based Virtual Machine (KVM) setting.</p>"},{"location":"paper_read/SR-IOV-KVM-Performance-Impact/#main-points","title":"Main Points","text":""},{"location":"paper_read/SR-IOV-KVM-Performance-Impact/#introduction","title":"Introduction","text":"<ul> <li>Resource virtualization has proven beneficial across data centers,   telecommunications systems, and computer networks.</li> <li>Several performance issues need consideration when using virtualization.</li> <li>Several networking solutions have emerged<ul> <li>Virtual Machine Device Queues (VMDQ): offloads packet sorting from the virtual machine manager (VMM), the hypervisor, to the network controller to accelerate network I/O throughput.</li> <li>PCI Passthrough and SR-IOV: Both aimed at lowering packet processing latency by acting at the hardware level.</li> </ul> </li> </ul> <p>Choosing the appropriate technology for a given use case is still an open research issue.</p>"},{"location":"paper_read/SR-IOV-KVM-Performance-Impact/#virtualization-and-single-root-io-virtualization","title":"Virtualization and single root I/O virtualization","text":"<p>Single Root I/O Virtualization (SR-IOV) is a hardware-based networking technology that allows a single physical input/output (I/O) device to present itself as multiple virtual devices with similar capabilities.</p> <p>As SR-IOV is \"simply\" a physical technology, there are multiple ways to logically connect it to user space processes. However, regarding interfacing virtual machines and SR-IOV, two main designs dominate the scene.</p>"},{"location":"paper_read/SR-IOV-KVM-Performance-Impact/#pci-passthrough","title":"PCI passthrough","text":"<p>PCI Passthrough</p> <p>PCI Passthrough is a mechanism that assigns host PCI devices directly to the virtual machines (VMs) and acts as a hypervisor bypass.</p> <ul> <li>allows a VM to directly connect to the PCI device, behaving as if it was physically connected</li> <li>the hypervisor no longer has to perform packet translation between the VM and the host.</li> </ul> <p>Removing the hypervisor from the data plane is a clear step to mitigate performance degradation caused by context switches and memory copies between VMs and the hypervisor during packet processing.</p> <p>SR-IOV is typically utilized in combination with PCI Passthrough.</p>"},{"location":"paper_read/SR-IOV-KVM-Performance-Impact/#standard-virtual-drivers","title":"Standard virtual drivers","text":""},{"location":"paper_read/SR-IOV-KVM-Performance-Impact/#limitations-or-extensions","title":"Limitations or Extensions","text":""},{"location":"paper_read/SR-IOV_Notes/","title":"High performance network virtualization with SR-IOV","text":"<p>Background:  https://learn.microsoft.com/en-us/windows-hardware/drivers/network/overview-of-single-root-i-o-virtualization--sr-iov-</p> <ul> <li>SR-IOV allows a device, such as a network adapter, to separate access to its resources among various PCIe hardware functions. These functions consist of the following types:<ul> <li>A PCIe Physical Function (PF)</li> <li>One or more PCIe Virtual Functions (VFs): A VF shares one or more physical resources of the device, such as a memory and a network port, with the PF and other VFs on the device.</li> </ul> </li> </ul>"},{"location":"paper_read/SR-IOV_Notes/#summary","title":"Summary","text":"<p>A generic virtualization architecture that can be implemented on various Virtual Machine Monitors (VMMs), including optimizations to reduce virtualization overhead and a dynamic network interface switching scheme to facilitate virtual machine migration without sacrificing performance.</p>"},{"location":"paper_read/SR-IOV_Notes/#outline","title":"Outline","text":""},{"location":"paper_read/SR-IOV_Notes/#sr-iov-virtualization-architecture","title":"SR-IOV virtualization architecture","text":""},{"location":"paper_read/SR-IOV_Notes/#my-opinion","title":"My opinion","text":""},{"location":"paper_read/SR-IOV_Notes/#sr-iov-vs-virtio","title":"SR-IOV vs VirtIO","text":"<ul> <li>Hardware Requirements<ul> <li>SR-IOV: Requires specific hardware support for virtual functions (VFs) on network cards.</li> <li>VirtIO: Hardware-agnostic, does not require specific hardware support and can work with standard network devices.</li> </ul> </li> <li>Data Path<ul> <li>SR-IOV: Allows direct data path between VMs and physical hardware, bypassing the hypervisor for I/O transactions.</li> <li>VirtIO: The data path goes through the hypervisor, which handles I/O operations between VMs and hardware.</li> </ul> </li> <li>Performance<ul> <li>SR-IOV: Allows direct data path between VMs and physical hardware, bypassing the hypervisor for I/O transactions.</li> <li>VirtIO: The data path goes through the hypervisor, which handles I/O operations between VMs and hardware.</li> </ul> </li> <li>Portability and Compatibility<ul> <li>SR-IOV: Less portable due to hardware dependencies</li> <li>VirtIO: High portability and compatibility across different hypervisors and hardware platforms.</li> </ul> </li> </ul>"},{"location":"paper_read/virtio-linux-overview/","title":"virtio: Towards a De-Facto Standard For Virtual I/O Devices","text":""},{"location":"paper_read/virtio-linux-overview/#summary","title":"Summary","text":"<p>VirtIO is an abstraction layer over virtual I/O devices in Linux, providing a common and efficient interface to handle block, network, and other I/O operations with guests in a standardized way.</p>"},{"location":"paper_read/virtio-linux-overview/#outline","title":"Outline","text":"<ul> <li> <p>VirtIO: The Three Goal</p> <ul> <li>Driver Unification</li> <li>Common ABI for general publication and use of buffers</li> <li>Complete ABI implementations</li> </ul> </li> <li> <p>The configuration operations</p> <ul> <li>Reading and writing feature bits</li> <li>Accessing the configuration space</li> <li>Managing the status bits</li> <li>Performing a device reset</li> </ul> </li> <li> <p>Host-Guest Transport</p> <ul> <li>Abstraction: Virtqueues</li> <li>Implementation: VirtIO_Ring</li> </ul> </li> <li> <p>Current VirtIO Drivers</p> <ul> <li>VirtIO-Block</li> <li>VirtIO-Net</li> </ul> </li> </ul>"},{"location":"paper_read/virtio-linux-overview/#my-understanding","title":"My understanding","text":""}]}